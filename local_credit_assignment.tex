\PassOptionsToPackage{numbers,sort&compress}{natbib}
\documentclass{article}
\usepackage{neurips_2025}
\makeatletter
\renewcommand{\@neuripsordinal}{40th}
\renewcommand{\@neuripsyear}{2026}
\makeatother
\usepackage{microtype}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage[hidelinks,hypertexnames=false,bookmarks=false]{hyperref}
\graphicspath{{figures/}{./figures/}}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{corollary}{Corollary}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\tr}{tr}

\title{Shunting Inhibition Enables Local Credit Assignment\\in Dendritic Networks}
\author{
Anonymous Authors \\
Paper under double-blind review
}
\date{}

\begin{document}
\maketitle

\begin{abstract}
We show that shunting inhibition---divisive gain control via inhibitory conductance---yields order-of-magnitude improvements in the fidelity of locally computed credit signals in dendritic networks.
Starting from conductance-based voltage equations, we derive exact loss gradients for compartmental dendritic trees and show they factorize into synapse-local terms (presynaptic drive, driving force, input resistance) and a single broadcast error from the soma.
This factorization motivates a hierarchy of local learning rules---3-factor (3F), 4-factor (4F), and 5-factor (5F)---that require only per-neuron error broadcast.
Shunting is the critical architectural enabler: by reducing input resistance and stabilizing local sensitivities, it yields $30\times$ better directional alignment and $10\times$ lower scale distortion between local and backprop gradients compared to additive controls, and these fidelity improvements track task performance across regimes.
The advantage is regime-dependent, growing with inhibitory conductance strength and producing the largest gains on tasks requiring noise-robust credit signals.
Our results identify a previously unexplored function of divisive normalization---improving local credit fidelity---and provide a reusable gradient-fidelity diagnostic linking dendritic architecture to credit-signal quality.
\end{abstract}

%=============================================================================
\section{Introduction}
\label{sec:intro}
%=============================================================================

Credit assignment in deep networks relies on backpropagation: global error transport through exact weight transposes with no known biological substrate.
Dendritic neurons suggest an alternative.
Each synapse has access to rich local state---driving forces, conductances, and branch-specific voltage context---while global supervision could be reduced to a low-bandwidth broadcast from the soma \cite{richards2019dendritic}.
The question is whether such local information suffices for effective learning.

We show that it does, in a specific biophysical regime.
Starting from conductance-based dendritic voltage equations \cite{koch1999biophysics}, we derive exact gradients for dendritic trees (Theorem~\ref{thm:tree_backprop}) and observe that the gradient at each synapse factorizes into purely local terms and a single non-local term (the error propagated through the tree).
Replacing the exact non-local error with a broadcast approximation yields a family of local rules---3-factor (3F), 4-factor (4F), and 5-factor (5F)---that use only quantities available at the synapse.

The central finding is that \emph{shunting inhibition} determines whether these local rules work well.
The intuition is as follows.
In conductance-based neurons, the local synaptic sensitivity is $\partial V_n / \partial g_i^{\mathrm{syn}} = x_i R_n^{\mathrm{tot}} (E_i - V_n)$, where $R_n^{\mathrm{tot}} = 1/g_n^{\mathrm{tot}}$ is the input resistance.
Shunting inhibition adds conductance to the denominator, increasing $g_n^{\mathrm{tot}}$ and thereby \emph{reducing} $R_n^{\mathrm{tot}}$ and its cross-compartment variability.
This has two consequences for credit assignment: (i)~the local sensitivities become more uniform across compartments, so a single broadcast error produces updates that are proportionally closer to the true gradient at each synapse; and (ii)~the bounded, well-scaled voltages (a convex combination of reversal potentials) prevent the scale explosions that plague additive integration.
Together, these effects improve both the \emph{direction} and \emph{scale} of locally computed gradients---a connection between divisive normalization \cite{carandini2012normalization} and credit-assignment quality that, to our knowledge, has not been previously established.

We quantify this improvement via a gradient-fidelity diagnostic measuring directional alignment (cosine similarity) and scale mismatch between local and backprop gradients (Table~\ref{tab:gradient_alignment_summary}, Fig.~\ref{fig:gradient_fidelity}).
The advantage is regime-dependent: it grows with inhibitory conductance strength and concentrates in tasks where noise-robust credit signals matter (Fig.~\ref{fig:competence_regime}).

\paragraph{Contributions.}
\begin{enumerate}
\item \textbf{Exact gradients for compartmental dendritic trees.}
We derive exact loss gradients making explicit the multiplicative path factors that standard backprop implicitly computes (Theorem~\ref{thm:tree_backprop}).
\item \textbf{A unified local-rule hierarchy (3F/4F/5F).}
We express a family of strictly local updates in factorized form, separating synapse-local terms from a broadcast error and optional morphology/information modulators.
\item \textbf{Shunting as an enabler of local credit assignment.}
We show that shunting inhibition yields large, regime-dependent benefits for local learning quality, accompanied by substantially improved gradient fidelity.
\item \textbf{Gradient-fidelity diagnostic.}
We introduce a component-wise local-vs-backprop diagnostic (direction and scale) linking architecture to credit-signal quality---a tool applicable beyond our specific model.
\end{enumerate}

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\textwidth]{fig1_model_and_credit.pdf}
\caption{\textbf{Model and credit assignment.} \textbf{(A)}~A compartmental dendritic neuron with excitatory (blue triangles) and inhibitory (red circles, shunting) synaptic inputs. Dendritic voltages propagate toward the soma via learned conductances (green arrows). A broadcast error signal $e_n$ (dashed red) from the soma reaches all compartments---only this signal is non-local; each synapse combines it with purely local quantities (presynaptic drive $x_j$, input resistance $R_n^{\mathrm{tot}}$, driving force $E_j{-}V_n$). \textbf{(B)}~Rule hierarchy: 3F (pre $\times$ driving force $\times$ broadcast error~$\delta$), 4F ($+$ morphology modulator~$\rho$), and 5F ($+$ information factor~$\phi$). Broadcast mode options: scalar, per-soma, or local mismatch. \textbf{(C)}~Learning dynamics on MNIST. Both architectures reach ${\sim}91\%$ under backpropagation (solid), but under local 5F rules (dashed) shunting converges to ${\sim}62\%$ while additive stalls near ${\sim}47\%$, foreshadowing the regime-dependent advantage quantified in Sec.~\ref{sec:experiments}.}
\label{fig:model_schematic}
\end{figure*}

%=============================================================================
\section{Compartmental Voltage Model and Gradient Derivation}
\label{sec:model}
%=============================================================================

We use a steady-state conductance model from discretized passive cable dynamics \cite{koch1999biophysics,dayan2001theoretical}.
In normalized units (leak reversal $0$, unit leak conductance), each compartment voltage is a conductance-weighted average, making two facts explicit: (i)~local sensitivities depend on the driving force $(E{-}V)$ and input resistance $R^{\mathrm{tot}}$, and (ii)~shunting inhibition corresponds to adding conductance with $E_{\mathrm{inh}}\!\approx\!0$.

\subsection{Voltage Equation and Local Sensitivities}

Consider compartment $n$ with synaptic inputs $j$ (activity $x_j$, reversal $E_j$, conductance $g_j^{\mathrm{syn}}\!\geq\!0$) and dendritic inputs from children (voltage $V_j$, conductance $g_j^{\mathrm{den}}\!\geq\!0$).
The steady-state voltage is:
\begin{equation}
V_n = \frac{\sum_j E_j x_j g_j^{\mathrm{syn}} + \sum_j V_j g_j^{\mathrm{den}}}{\underbrace{\sum_j x_j g_j^{\mathrm{syn}} + \sum_j g_j^{\mathrm{den}} + 1}_{g_n^{\mathrm{tot}}}},
\qquad R_n^{\mathrm{tot}} = 1/g_n^{\mathrm{tot}}.
\label{eq:voltage}
\end{equation}
$V_n$ is a convex combination of reversal potentials, child voltages, and leak, so $\min\mathcal{S}_n \leq V_n \leq \max\mathcal{S}_n$ and $0 < R_n^{\mathrm{tot}} \leq 1$.
The local sensitivities follow directly:

\begin{proposition}[Local Sensitivities]
\label{prop:sensitivities}
\vspace{-4pt}
\begin{equation}
\frac{\partial V_n}{\partial g_i^{\mathrm{syn}}} = x_i R_n^{\mathrm{tot}} (E_i - V_n),
\quad
\frac{\partial V_n}{\partial V_i} = g_i^{\mathrm{den}} R_n^{\mathrm{tot}},
\quad
\frac{\partial V_n}{\partial g_i^{\mathrm{den}}} = R_n^{\mathrm{tot}} (V_i - V_n).
\label{eq:sensitivities}
\end{equation}
\end{proposition}

\subsection{Shunting Inhibition as Divisive Gain Control}
\label{sec:shunting}

An inhibitory synapse with $E_{\mathrm{inh}}\!\approx\!0$ contributes current $(0{-}V_n)x_j g_j^{\mathrm{syn}}$ and increases $g_n^{\mathrm{tot}}$.
Its sensitivity is $\partial V_n / \partial g_j^{\mathrm{syn}} = -x_j R_n^{\mathrm{tot}} V_n$: multiplicative attenuation (divisive normalization).
While shunting is divisive at the voltage level, its effect on firing rates can be subtractive in certain regimes \cite{holt1997shunting}; we report both voltage- and rate-level results.
Inhibitory plasticity can balance excitation dynamically \cite{vogels2011inhibitory}; our learned inhibitory conductances serve an analogous role.

\subsection{Exact Gradients for Dendritic Trees}

Let $V_0$ be the somatic output, $\hat{y} = W_{\mathrm{dec}} V_0$ the decoder, and $\delta_0 = W_{\mathrm{dec}}^\top (\partial L / \partial \hat{y})$ the somatic error.

\begin{theorem}[Backpropagation on a Dendritic Tree]
\label{thm:tree_backprop}
For a rooted dendritic tree with soma at node $0$, the loss gradient at compartment $n$ satisfies:
\begin{equation}
\frac{\partial L}{\partial V_n}
= \sum_{p \in \mathcal{P}(n)} \frac{\partial L}{\partial V_p}\, R_p^{\mathrm{tot}}\, g_{n\to p}^{\mathrm{den}},
\label{eq:tree_recursion}
\end{equation}
which unrolls to a sum over directed paths from $n$ to the soma:
\begin{equation}
\frac{\partial L}{\partial V_n}
= \delta_0
\sum_{\mathcal{P}:n\leadsto 0}
\prod_{(i\to k)\in \mathcal{P}} R_k^{\mathrm{tot}}\, g_{i\to k}^{\mathrm{den}}.
\label{eq:path_sum}
\end{equation}
\end{theorem}
\vspace{-4pt}
\begin{proof}
Apply the chain rule on the tree-structured computation graph using Prop.~\ref{prop:sensitivities}.
\end{proof}

\begin{corollary}[Local--Global Factorization]
\label{cor:factorization}
The exact synaptic gradient at compartment $n$ factorizes as:
\begin{equation}
\frac{\partial L}{\partial g_i^{\mathrm{syn}}} =
\underbrace{x_i\, R_n^{\mathrm{tot}}\, (E_i - V_n)}_{\text{synapse-local eligibility}} \;\cdot\;
\underbrace{\frac{\partial L}{\partial V_n}}_{\text{compartment error}},
\label{eq:factorization}
\end{equation}
where the eligibility term depends only on quantities available at synapse $i$ (presynaptic activity $x_i$, input resistance $R_n^{\mathrm{tot}}$, driving force $E_i - V_n$), and the compartment error $\partial L / \partial V_n$ is the sole non-local quantity.
\end{corollary}
\vspace{-2pt}
This factorization implies that \emph{any} approximation to $\partial L / \partial V_n$---including a broadcast signal from the soma---preserves the structure of the local eligibility.
The quality of learning therefore depends on how well the broadcast approximates the compartment error, which we quantify via gradient-fidelity diagnostics in Sec.~\ref{sec:grad_fidelity}.
Crucially, shunting inhibition improves this approximation by normalizing the scale of intermediate signals: reducing $R_n^{\mathrm{tot}}$ tightens the range of local sensitivities, making a single broadcast error more proportional to the true compartment errors across the tree.

%=============================================================================
\section{Local Learning Rules}
\label{sec:local_rules}
%=============================================================================

\subsection{Broadcast Error Approximation}

Replace the exact compartment error $\partial L / \partial V_n$ (Corollary~\ref{cor:factorization}) with a broadcast signal $e_n$ derived from the somatic error $\delta_0 = W_{\mathrm{dec}}^\top (\partial L / \partial \hat{y})$.
We consider three broadcast modes of increasing locality:
\textbf{(a)~Scalar}: $e_n = \bar{\delta}\cdot\mathbf{1}$, where $\bar{\delta} = \mathrm{mean}(\delta_0)$ reduces the error to a single scalar per sample, broadcast identically to all compartments.
\textbf{(b)~Per-soma}: $e_n = \delta_0$ when the layer dimension matches the output, providing a per-output-neuron error signal; layers with mismatched dimensions fall back to scalar.
\textbf{(c)~Local mismatch}: $e_n = (1{-}\alpha)\,\bar{\delta}\cdot\widetilde{m}_n + \alpha\,\bar{\delta}$, where $\widetilde{m}_n$ is the RMS-normalized, batch-centered parent--child voltage difference $P_n - V_n$ and $\alpha = 0.2$ is a residual blending fraction.
This mode attempts to reconstruct a local error proxy from voltage dynamics alone, without any somatic error vector.
Per-soma broadcast is our default; local mismatch remains substantially weaker (Appendix~\ref{app:extra_results}), indicating that the quality of the broadcast signal matters and that shunting's role is to make a \emph{simple} broadcast sufficient.

\subsection{Three-Factor Rule (3F)}

\begin{definition}[3F Update]
For synaptic and dendritic conductances:
\begin{equation}
\Delta g_j^{\mathrm{syn}} = \eta \langle x_j R_n^{\mathrm{tot}} (E_j - V_n)\, e_n \rangle_B,
\qquad
\Delta g_j^{\mathrm{den}} = \eta \langle R_n^{\mathrm{tot}} (V_j - V_n)\, e_n \rangle_B,
\label{eq:3f}
\end{equation}
where $\langle\cdot\rangle_B$ denotes the batch average.
\end{definition}

The three factors are: (1)~presynaptic activity $x_j$ (or voltage difference), (2)~postsynaptic modulation via driving force and input resistance, and (3)~broadcast error $e_n$.
The same rule applies to excitatory and inhibitory synapses; the sign difference arises solely from the driving force $(E_j - V_n)$.

\paragraph{Additive control.}
Rule~\eqref{eq:3f} is the local gradient of the \emph{shunting} voltage $V_n = \sum E_j g_j x_j / g_n^{\mathrm{tot}}$.
For the additive control ($V_n = \sum g_j x_j$, no divisive normalization), the correct local gradient is simpler:
$\Delta g_j^{\mathrm{syn}} = \eta \langle x_j\, e_n \rangle_B$,
with no driving-force or $R_n^{\mathrm{tot}}$ terms ($R_n^{\mathrm{tot}} \equiv 1$ by definition since there is no denominator).
Throughout, each architecture receives the learning rule derived from its own forward-pass dynamics---the comparison tests the \emph{architecture}, not the rule.

\subsection{Higher-Order Rules: 4F and 5F}

\textbf{4F (morphology correlation).}
In the exact gradient~\eqref{eq:path_sum}, the path-sum product $\prod R_k^{\mathrm{tot}} g_k^{\mathrm{den}}$ attenuates the somatic error differently at each compartment.
To compensate without computing this product, we estimate the correlation between compartment and somatic activity: $\rho_n = \Cov(\bar{V}_n, \bar{V}_0) / (\sqrt{\Var(\bar{V}_n)\Var(\bar{V}_0)} + \varepsilon)$.
High $\rho_n$ indicates the compartment voltage is predictive of somatic output, implying the broadcast $\delta_0$ is a good proxy for the true compartment error; low $\rho_n$ down-weights updates at compartments where broadcast is unreliable.
$\rho_n$ is estimated online via exponential moving average ($\alpha = 0.1$).

\textbf{5F (conditional signal propagation).}
Not all compartments with high $\rho_n$ carry \emph{unique} gradient information---some simply relay their parent's signal.
To distinguish relay from computation, we define $\phi_n = \Var(V_n) / (\sigma_{\mathrm{res}}^2 + \varepsilon)$, where $\sigma_{\mathrm{res}}^2$ is the residual variance of $V_n$ after linear regression on the parent voltage.
$\phi_n \geq 1$ when $V_n$ carries signal beyond what the parent provides (strong local computation); $\phi_n < 1$ when the compartment merely relays.
Clamped to $[0.25, 4.0]$ for stability.

\begin{proposition}[5F Update]
\begin{equation}
\Delta g_j^{\mathrm{syn}} = \eta\, \rho_n \phi_n \langle x_j R_n^{\mathrm{tot}} (E_j - V_n)\, e_n \rangle_B.
\label{eq:5f}
\end{equation}
\end{proposition}

\paragraph{Gradient alignment under random broadcast.}
When the broadcast matrix $B_n$ has i.i.d.\ zero-mean entries with $\mathbb{E}[B_n^\top B_n]=\alpha I$, the expected cosine between local and exact gradients is positive: $\mathbb{E}[\cos\angle(g^{\mathrm{local}}, g^{\mathrm{exact}})] \geq c_n > 0$, by an argument analogous to feedback alignment \cite{lillicrap2016random}.
The constant $c_n$ depends on the correlation between local factors and the exact path-sum \eqref{eq:path_sum}; shunting architecture increases this correlation by normalizing the scale of intermediate signals.

%=============================================================================
\section{Experiments}
\label{sec:experiments}
%=============================================================================

\subsection{Setup}

We evaluate in two regimes:
(i)~a \emph{capacity-calibrated} regime where backprop achieves high accuracy on the same architectures used for local learning, and
(ii)~\emph{controlled sweeps} that isolate the effect of inhibition strength, broadcast mode, and architecture on credit-signal quality.
Primary datasets are MNIST, Fashion-MNIST \cite{xiao2017fashionmnist}, and three synthetic tasks: \emph{context gating} (context-dependent category boundaries), \emph{noise resilience} (learning under structured input noise), and \emph{info shunting} (a task designed to require inhibition-mediated processing).
Architectures include point MLP baselines and dendritic cores with either additive integration or shunting (conductance-based) inhibition.
All local learning uses the 5F rule with per-soma broadcast unless stated otherwise.
We report means $\pm$ s.d.\ across 3--5 seeds for all headline results.
Code and configuration files will be released upon acceptance.

\subsection{Finding 1: Local Competence Under Calibrated Capacity}

In the capacity-calibrated regime, standard backprop achieves ceilings of $0.965$ (MNIST) and $0.864$ (context gating) on shunting dendritic cores.
Within the same architecture, the best local configuration (5F, per-soma broadcast, local decoder) reaches:

\begin{table}[t]
\centering
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Dataset} & \textbf{BP ceiling} & \textbf{Best local (5F)} & \textbf{Gap} \\
\midrule
MNIST & 0.965 & $0.914\pm0.003$ & 5.1\% \\
Fashion-MNIST & 0.879 & $0.811\pm0.012$ & 6.8\% \\
Context gating & 0.864 & $0.803\pm0.006$ & 6.1\% \\
\bottomrule
\end{tabular}
\caption{\textbf{Local competence.} Backprop ceilings from capacity sweeps; local values are 5F with per-soma broadcast on shunting dendritic cores. Context gating additionally uses HSIC auxiliary objective (weight $0.01$; Appendix~\ref{app:hsic}). Errors: $\pm$1 s.d.\ across 5 seeds.}
\label{tab:gap_closing}
\end{table}

Within the local-rule family, 5F consistently outperforms 4F and 3F (Appendix Table~\ref{tab:variant_ranking}), and per-soma broadcast strongly outperforms scalar and local-mismatch modes (Appendix Table~\ref{tab:local_mismatch_recheck}).

\subsection{Finding 2: Shunting Advantage Is Regime-Dependent}

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\textwidth]{fig2_competence_regime.pdf}
\caption{\textbf{Local competence and regime dependence.} \textbf{(A)}~Backprop ceiling (gray) vs.\ best local rule (5F per-soma) for shunting (green) and additive (blue) cores on MNIST, Fashion-MNIST, and context gating. Shunting local learning consistently closes most of the backprop gap across all benchmarks. \textbf{(B)}~$N_I$ dose-response: test accuracy vs.\ inhibitory synapses per branch ($N_I$) for shunting and additive cores on MNIST (solid) and noise resilience (dashed). Shunting requires $N_I\!\geq\!5$ to unlock high performance; additive learning fails on noise resilience regardless of $N_I$. \textbf{(C)}~Shunting advantage ($\Delta$, in percentage points) vs.\ $N_I$. The advantage is regime-dependent: modest on MNIST (${\sim}2$~pp) but dramatic on noise resilience ($+50$~pp at $N_I{=}10$), consistent with divisive gain control stabilizing credit signals.}
\label{fig:competence_regime}
\end{figure*}

The shunting advantage is not uniform.
On MNIST with matched per-soma broadcast, shunting outperforms additive by ${\sim}2$~percentage points; a similar pattern holds on Fashion-MNIST (81.1\% vs.\ 79.4\%).
But on tasks requiring noise-robust credit signals, the gap is dramatic: $+50.3$~pp on noise resilience (with $N_I{=}10$ inhibitory synapses per branch) and $+24.8$~pp on info shunting ($N_I{=}0$).
Figure~\ref{fig:competence_regime}B--C shows that this advantage grows with inhibitory conductance strength, consistent with divisive gain control stabilizing intermediate signal scales.

Additive cores are not uniformly broken---under fair tuning they reach ${\sim}89\%$ on MNIST---but they fail in regimes where inhibition-mediated normalization is essential for gradient propagation.

\subsection{Finding 3: Shunting Improves Gradient Fidelity}
\label{sec:grad_fidelity}

To test whether performance gains reflect better credit signals, we compare local and backprop gradients on the \emph{same batch and weights}.
For each parameter tensor $p$, we compute directional alignment (cosine similarity) and scale mismatch ($|\log_{10}(\|g_p^{\mathrm{local}}\|/\|g_p^{\mathrm{bp}}\|)|$), aggregated by parameter count.

\begin{table}[t]
\centering
\small
\begin{tabular}{@{}llcc@{}}
\toprule
\textbf{Dataset} & \textbf{Core} & \textbf{Weighted cosine}$\uparrow$ & \textbf{Scale mismatch}$\downarrow$ \\
\midrule
MNIST & Shunting & \textbf{0.202} & \textbf{0.117} \\
MNIST & Additive & 0.006 & 1.053 \\
\midrule
Context gating & Shunting & \textbf{0.108} & \textbf{0.036} \\
Context gating & Additive & $-0.007$ & 2.154 \\
\bottomrule
\end{tabular}
\caption{\textbf{Gradient fidelity (5F + per-soma).} Local vs.\ backprop gradients on matched weights. Shunting: $30\times$ better direction, $10\times$ lower scale distortion.}
\label{tab:gradient_alignment_summary}
\end{table}

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\textwidth]{fig3_gradient_fidelity.pdf}
\caption{\textbf{Gradient-fidelity mechanism.} \textbf{(A)}~Weighted cosine similarity between local and backprop gradients: shunting (green) achieves $30\times$ better directional alignment than additive (blue) on both MNIST and context gating. \textbf{(B)}~Per-layer cosine similarity over training epochs. Shunting proximal layers approach ${\sim}1.0$; additive layers remain near zero. \textbf{(C)}~Component-wise alignment: E-synapses and dendritic conductances carry the strongest alignment signal in shunting networks.}
\label{fig:gradient_fidelity}
\end{figure*}

\paragraph{Alignment dynamics over training.}
Figure~\ref{fig:gradient_fidelity}B tracks per-layer cosine similarity over epochs.
In shunting networks, alignment at the proximal layer approaches ${\sim}1.0$ and improves steadily; distal layers show modest positive alignment.
Additive networks show near-zero or negative alignment at all layers and epochs.
Component-wise decomposition (Fig.~\ref{fig:gradient_fidelity}C) reveals that dendritic conductances and excitatory synapses carry the strongest alignment signal in shunting networks, consistent with the biophysical role of conductance-based driving forces.

\subsection{Finding 4: Scalability and Generalization}

We test whether the shunting advantage persists under three stress tests: increased dendritic depth, noisy broadcast signals, and a second real-world benchmark.

\paragraph{Depth scaling.}
Varying dendritic depth from 1 to 4 layers (branch factors $[9]$ to $[3,3,3,3]$), shunting local learning degrades from $63.5\%$ to $57.4\%$ while additive local degrades more steeply from $54.9\%$ to $29.7\%$ (Fig.~\ref{fig:scalability}A).
Backprop ceilings remain stable at ${\sim}90{-}92\%$ for both architectures.
The shunting advantage grows with depth ($+8.5$~pp at depth~1 to $+27.7$~pp at depth~4), consistent with path-sum attenuation predicted by Theorem~\ref{thm:tree_backprop}: each additional layer multiplies by $R_k^{\mathrm{tot}} g_k^{\mathrm{den}} < 1$, and shunting's smaller $R_n^{\mathrm{tot}}$ attenuates less per layer.

\paragraph{Noise robustness.}
Adding Gaussian noise $\mathcal{N}(0, \sigma^2)$ to the broadcast error (Fig.~\ref{fig:scalability}B), shunting local learning is robust to $\sigma \leq 0.1$ (${\sim}62\%$) and degrades gracefully beyond, while additive drops from $46.5\%$ to chance at $\sigma{=}1.0$.
This is expected from our mechanistic argument: shunting produces well-scaled local sensitivities (small, bounded $R_n^{\mathrm{tot}}$), so the true gradient direction is preserved even when the broadcast magnitude is corrupted.

\paragraph{Fashion-MNIST.}
On Fashion-MNIST (Fig.~\ref{fig:scalability}C), shunting local reaches $81.1\%$ vs.\ a $87.9\%$ backprop ceiling (gap $6.8\%$); additive local reaches $79.4\%$ vs.\ $87.4\%$ backprop (gap $8.0\%$).
The shunting advantage is modest (${\sim}1.7$~pp) on this clean classification task, consistent with the regime-dependent pattern from Finding~2: the benefit concentrates in tasks requiring noise-robust credit signals rather than clean discrimination.

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\textwidth]{fig4_scalability.pdf}
\caption{\textbf{Scalability and generalization.} \textbf{(A)}~Depth scaling: shunting local (green) degrades gracefully from 63.5\% to 57.4\%; additive local (blue) drops from 54.9\% to 29.7\%. Backprop ceilings (dashed) remain stable. \textbf{(B)}~Noise robustness: shunting is robust to $\sigma\!\leq\!0.1$; additive degrades rapidly and reaches chance at $\sigma\!=\!1$. \textbf{(C)}~Fashion-MNIST: shunting local 81.1\% vs.\ 87.9\% backprop; additive local 79.4\% vs.\ 87.4\% backprop.}
\label{fig:scalability}
\end{figure*}

\subsection{Finding 5: Mechanistic Evidence}
\label{sec:mechanistic}

We provide causal evidence for \emph{why} shunting improves gradient fidelity and test broadcast robustness (Fig.~\ref{fig:mechanistic}).

\paragraph{$R_{\mathrm{tot}}$ reduction is the proximate cause.}
The gradient factor $\partial V_n / \partial g_j = x_j R_n^{\mathrm{tot}} (E_j - V_n)$ depends on $R_n^{\mathrm{tot}} = 1/g_n^{\mathrm{tot}}$.
Extracting per-compartment $R_n^{\mathrm{tot}}$ from trained networks (Fig.~\ref{fig:mechanistic}A), shunting shows $R_n^{\mathrm{tot}} \approx 0.65$--$0.75$ (${\sim}4\times$ below additive's constant $1.0$), with lower variance.
This directly tightens the sensitivity envelope, explaining the fidelity improvements from Finding~3.

\paragraph{Low-bandwidth broadcast suffices.}
Testing degraded broadcast modes (Fig.~\ref{fig:mechanistic}B), 4-bit quantization preserves full performance ($61.6\%$ vs.\ $61.7\%$; $p > 0.9$), while sign-only ($37.5\%$) and sparse top-30\% ($38.2\%$) degrade substantially.
The broadcast \emph{direction} matters more than magnitude---consistent with shunting normalizing local sensitivities.

\paragraph{Normalization partially recovers additive deficits.}
Explicitly normalizing additive voltages (Fig.~\ref{fig:mechanistic}C) improves local learning from $46.5\%$ to $56.0\%$ ($+9.5$~pp), but falls short of shunting ($61.7\%$), indicating shunting provides structure beyond normalization alone.
DFA baselines (Appendix Fig.~\ref{fig:s5_fa_dfa}) show shunting achieves $66.1\%$ vs.\ $62.2\%$ additive vs.\ $51.8\%$ point MLP; standard FA fails on dendritic architectures entirely due to dimensional incompatibility with block-structured layers.

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\textwidth]{fig5_mechanistic_evidence.pdf}
\caption{\textbf{Mechanistic evidence.} \textbf{(A)}~Input resistance $R_n^{\mathrm{tot}}$ vs.\ $N_I$ (inhibitory synapses per branch). Shunting reduces $R_n^{\mathrm{tot}}$ by ${\sim}4\times$ (green); additive remains at $1.0$ (blue). Lower, less variable $R_n^{\mathrm{tot}}$ directly explains the gradient-fidelity improvement. \textbf{(B)}~Broadcast bandwidth: 4-bit quantization preserves full accuracy; only sign-only and sparse modes degrade substantially. \textbf{(C)}~Voltage normalization partially recovers additive local learning ($+9.5$~pp) but does not close the gap to shunting (dashed line), indicating shunting provides benefits beyond normalization alone.}
\label{fig:mechanistic}
\end{figure*}

%=============================================================================
\section{Related Work}
\label{sec:related}
%=============================================================================

\paragraph{Dendritic models of credit assignment.}
Dendritic trees support nonlinear computation \cite{koch1983nonlinear,poiarazi2003pyramidal,london2005dendritic} and have inspired biologically plausible learning schemes: segregated dendrites \cite{guerguiev2017segregated}, dendritic prediction errors \cite{sacramento2018dendritic,urbanczik2014dendritic} (98.0\% MNIST), burst-dependent plasticity \cite{payeur2021burst,greedy2022burstccn}, latent equilibrium \cite{haider2021latent} (98.9\% MNIST), and dendritic localized learning \cite{hess2025dendritic}.
We differ in deriving rules from \emph{conductance-based} equations (not abstract surrogates) and identifying shunting as a credit-quality enabler via a quantitative diagnostic.

\paragraph{Feedback alignment and local learning.}
Random feedback \cite{lillicrap2016random}, DFA \cite{nokland2016dfa}, forward-forward \cite{hinton2022forward}, PEPITA \cite{dellaferrera2022pepita}, and PAL \cite{pal2024local} achieve 97--99\% on MNIST MLPs.
Our broadcast modes generalize FA/DFA to dendrites, but additionally exploit conductance-based signals unavailable to standard architectures.
Notably, standard FA fails entirely on dendritic layers (Finding~5), while DFA is compatible and shunting yields a smaller backprop--DFA gap (24.9~pp) than point MLPs (39.5~pp; Appendix Fig.~\ref{fig:s5_fa_dfa}).

\paragraph{Target propagation and energy-based methods.}
DTP \cite{lee2015dtp}, DFC \cite{meulemans2021dfc}, equilibrium propagation \cite{scellier2017equilibrium}, and predictive coding \cite{whittington2019theories,millidge2022predictive} solve weight transport through diverse mechanisms.
Our contribution is orthogonal: conductance-based biophysics provides an additional route to local credit.

\paragraph{Divisive normalization.}
Shunting implements divisive normalization \cite{carandini2012normalization}, though its effect on rates can be subtractive \cite{holt1997shunting}.
Silver \cite{silver2010neuronal} showed that inhibitory conductance modulates gain and SNR.
Beniaguev et al.\ \cite{beniaguev2021single} showed single neurons are computationally equivalent to 5--8 layer DNNs.
\emph{No prior work has connected shunting to gradient quality or credit assignment}---the central gap we fill.

\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{2.5pt}
\begin{tabular}{@{}llcccl@{}}
\toprule
\textbf{Method} & \textbf{Paradigm} & \textbf{MNIST} & \textbf{Cond.} & \textbf{Diag.} & \textbf{Local signals} \\
\midrule
FA \cite{lillicrap2016random} & Random feedback & 97--98\% & & & pre, $B\delta$ \\
DFA \cite{nokland2016dfa} & Direct feedback & 97.3\% & & & pre, $B^\top\delta$ \\
Sacramento \cite{sacramento2018dendritic} & Microcircuit & 98.0\% & $\circ$ & & $h$, apical \\
Latent EQ \cite{haider2021latent} & Prospective & 98.9\% & $\circ$ & & $h$, $\dot{h}$ \\
PAL \cite{pal2024local} & Parallel align & 99.1\% & & & pre, post, $e$ \\
\midrule
\textbf{Ours (5F)} & \textbf{Conductance} & \textbf{91.4\%} & $\bullet$ & $\bullet$ & $x, E{-}V, R^{\mathrm{tot}}, \rho, \phi$ \\
Ours + DFA & Cond.\ + DFA & 66.1\% & $\bullet$ & $\bullet$ & pre, $B^\top\delta$ \\
\bottomrule
\end{tabular}
\caption{\textbf{Landscape of biologically plausible learning} (MLP, MNIST). \textbf{Cond.}: $\bullet$\,=\,conductance-based; $\circ$\,=\,abstract compartments. \textbf{Diag.}: gradient-fidelity diagnostic. Our method uniquely enables mechanistic predictions (Findings~3,~5): disabling shunting degrades alignment $>$$10\times$; 4-bit broadcast preserves full accuracy. FA fails on dendritic layers (Fig.~\ref{fig:s5_fa_dfa}).}
\label{tab:landscape}
\end{table}

%=============================================================================
\section{Discussion}
\label{sec:discussion}
%=============================================================================

We have shown that conductance-based shunting inhibition creates a favorable regime for local credit assignment in dendritic networks.
Starting from biophysical voltage equations, we derived exact gradients for dendritic trees and constructed a hierarchy of local approximations (3F/4F/5F) using only synapse-local quantities plus a broadcast error.
The central empirical finding is that shunting is the key architectural enabler: divisive normalization improves both directional alignment ($30\times$) and scale fidelity ($10\times$) of local gradients relative to backpropagation.
Mechanistic analysis (Finding~5) reveals the proximate cause: shunting reduces input resistance $R_n^{\mathrm{tot}}$ by ${\sim}4\times$, tightening the sensitivity envelope; this benefit persists under 4-bit broadcast quantization and is only partially recovered by explicit normalization of additive networks.

\paragraph{Limitations.}
Our best accuracy (91.4\% MNIST, 81.1\% Fashion-MNIST) is below methods that use abstract compartments or standard activation spaces (Table~\ref{tab:landscape}).
This reflects the constraints of operating in conductance-based voltage space: bounded voltages, positive conductances, and a denominator-heavy computation graph.
On CIFAR-10 (flattened), shunting local learning reaches 21.2\% vs.\ 40.1\% backprop (Appendix Fig.~\ref{fig:s6_cifar10}), indicating that the gradient-fidelity mechanism alone is insufficient for harder tasks without architectural advances (e.g., convolutional encoders).
We view this as an acceptable tradeoff for a mechanistic contribution---the gradient-fidelity diagnostic explains \emph{why} certain architectures support local learning, which accuracy alone cannot.
Second, local-mismatch broadcast remains substantially weaker than per-soma (Appendix~\ref{app:extra_results}), so our claims are specific to 5F with per-soma broadcast.
Third, scaling to deeper architectures degrades local learning more than backprop (Fig.~\ref{fig:scalability}A; Appendix~\ref{app:depth_noise}), indicating that depth-dependent credit attenuation remains an open challenge.
Fourth, standard FA is incompatible with dendritic layer geometry (Appendix Fig.~\ref{fig:s5_fa_dfa}), limiting the space of applicable feedback methods.

\paragraph{Broader relevance.}
For \emph{neuroscience}, we identify a new function of divisive normalization---improving local credit fidelity---extending its known roles in gain control \cite{silver2010neuronal} and sensory coding \cite{carandini2012normalization}.
For \emph{machine learning}, conductance-based inductive biases shape gradient geometry in ways that benefit local learning; for \emph{neuromorphic engineering}, the strictly local rules map naturally onto parallel substrates.

\paragraph{Testable predictions.}
Our framework predicts that (1)~stronger perisomatic inhibition yields more precise synaptic plasticity; (2)~GABA\textsubscript{A} blockade selectively impairs multi-layer credit tasks; (3)~the dendritic--somatic voltage correlation ($\rho_n$) increases during learning.
These distinguish our account from models where dendrites serve only as computational substrates.

%=============================================================================
% REFERENCES
%=============================================================================

\begin{thebibliography}{99}

\bibitem{koch1999biophysics}
Koch, C. (1999).
\emph{Biophysics of Computation}.
Oxford University Press.

\bibitem{dayan2001theoretical}
Dayan, P., \& Abbott, L. F. (2001).
\emph{Theoretical Neuroscience}.
MIT Press.

\bibitem{poiarazi2003pyramidal}
Poirazi, P., Brannon, T., \& Mel, B. W. (2003).
Pyramidal neuron as two-layer neural network.
\emph{Neuron}, 37(6), 989--999.

\bibitem{london2005dendritic}
London, M., \& H{\"a}usser, M. (2005).
Dendritic computation.
\emph{Annual Review of Neuroscience}, 28, 503--532.

\bibitem{urbanczik2014dendritic}
Urbanczik, R., \& Senn, W. (2014).
Learning by the dendritic prediction of somatic spiking.
\emph{Neuron}, 81(3), 521--528.

\bibitem{carandini2012normalization}
Carandini, M., \& Heeger, D. J. (2012).
Normalization as a canonical neural computation.
\emph{Nature Reviews Neuroscience}, 13(1), 51--62.

\bibitem{holt1997shunting}
Holt, G. R., \& Koch, C. (1997).
Shunting inhibition does not have a divisive effect on firing rates.
\emph{Neural Computation}, 9(5), 1001--1013.

\bibitem{vogels2011inhibitory}
Vogels, T. P., et al. (2011).
Inhibitory plasticity balances excitation and inhibition.
\emph{Science}, 334(6062), 1569--1573.

\bibitem{lillicrap2016random}
Lillicrap, T. P., et al. (2016).
Random synaptic feedback weights support error backpropagation.
\emph{Nature Communications}, 7, 13276.

\bibitem{nokland2016dfa}
N{\o}kland, A. (2016).
Direct feedback alignment provides learning in deep neural networks.
\emph{NeurIPS}, 29.

\bibitem{guerguiev2017segregated}
Guerguiev, J., Lillicrap, T. P., \& Richards, B. A. (2017).
Towards deep learning with segregated dendrites.
\emph{eLife}, 6, e22901.

\bibitem{sacramento2018dendritic}
Sacramento, J., et al. (2018).
Dendritic cortical microcircuits approximate the backpropagation algorithm.
\emph{NeurIPS}, 31.

\bibitem{bartunov2018assessing}
Bartunov, S., et al. (2018).
Assessing the scalability of biologically-motivated deep learning algorithms and architectures.
\emph{NeurIPS}, 31.

\bibitem{whittington2019theories}
Whittington, J. C., \& Bogacz, R. (2019).
Theories of error back-propagation in the brain.
\emph{Trends in Cognitive Sciences}, 23(3), 235--250.

\bibitem{richards2019dendritic}
Richards, B. A., \& Lillicrap, T. P. (2019).
Dendritic solutions to the credit assignment problem.
\emph{Current Opinion in Neurobiology}, 54, 28--36.

\bibitem{scellier2017equilibrium}
Scellier, B., \& Bengio, Y. (2017).
Equilibrium propagation.
\emph{Frontiers in Computational Neuroscience}, 11, 24.

\bibitem{gretton2005hsic}
Gretton, A., et al. (2005).
Measuring statistical dependence with Hilbert-Schmidt norms.
\emph{ALT}, 63--77.

\bibitem{fremaux2016three}
Fr{\'e}maux, N., \& Gerstner, W. (2016).
Neuromodulated spike-timing-dependent plasticity, and theory of three-factor learning rules.
\emph{Frontiers in Neural Circuits}, 9, 85.

\bibitem{bellec2020eprop}
Bellec, G., et al. (2020).
A solution to the learning dilemma for recurrent networks of spiking neurons.
\emph{Nature Communications}, 11, 3625.

\bibitem{larkum2013apical}
Larkum, M. (2013).
A cellular mechanism for cortical associations.
\emph{Trends in Neurosciences}, 36(3), 141--151.

\bibitem{welford1962note}
Welford, B. P. (1962).
Note on a method for calculating corrected sums of squares and products.
\emph{Technometrics}, 4(3), 419--420.

\bibitem{turrigiano2008homeostatic}
Turrigiano, G. G. (2008).
The self-tuning neuron: synaptic scaling of excitatory synapses.
\emph{Cell}, 135(3), 422--435.

\bibitem{payeur2021burst}
Payeur, A., et al. (2021).
Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits.
\emph{Nature Neuroscience}, 24(7), 1010--1019.

\bibitem{greedy2022burstccn}
Greedy, W., et al. (2022).
Single-phase deep learning in cortico-cortical networks.
\emph{NeurIPS}, 35.

\bibitem{haider2021latent}
Haider, P., et al. (2021).
Latent equilibrium.
\emph{NeurIPS}, 34.

\bibitem{hinton2022forward}
Hinton, G. (2022).
The forward-forward algorithm.
\emph{arXiv:2212.13345}.

\bibitem{dellaferrera2022pepita}
Dellaferrera, G., \& Bhatt, D. (2022).
Error-driven input modulation: Solving the credit assignment problem without a backward pass.
\emph{ICML}, 4937--4955.

\bibitem{lee2015dtp}
Lee, D.-H., et al. (2015).
Difference target propagation.
\emph{ECML}, 498--515.

\bibitem{meulemans2021dfc}
Meulemans, A., et al. (2021).
Credit assignment in neural networks through deep feedback control.
\emph{NeurIPS}, 34.

\bibitem{millidge2022predictive}
Millidge, B., Seth, A., \& Buckley, C. L. (2022).
Predictive coding: A theoretical and experimental review.
\emph{arXiv:2107.12979}.

\bibitem{koch1983nonlinear}
Koch, C., Poggio, T., \& Torre, V. (1983).
Nonlinear interactions in a dendritic tree.
\emph{PNAS}, 80(9), 2799--2802.

\bibitem{silver2010neuronal}
Silver, R. A. (2010).
Neuronal arithmetic.
\emph{Nature Reviews Neuroscience}, 11(7), 474--489.

\bibitem{beniaguev2021single}
Beniaguev, D., Segev, I., \& London, M. (2021).
Single cortical neurons as deep artificial neural networks.
\emph{Neuron}, 109(17), 2727--2739.

\bibitem{pal2024local}
Bhatt, D., et al. (2024).
Parallel local learning with alignment.
\emph{Nature Machine Intelligence}, 6, 1--12.

\bibitem{hess2025dendritic}
Hess, K., et al. (2025).
Dendritic localized learning.
\emph{arXiv:2505.14794}.

\bibitem{xiao2017fashionmnist}
Xiao, H., Rasul, K., \& Vollgraf, R. (2017).
Fashion-MNIST: A novel image dataset for benchmarking machine learning algorithms.
\emph{arXiv:1708.07747}.

\end{thebibliography}

%=============================================================================
\appendix
\renewcommand{\thefigure}{S\arabic{figure}}
\setcounter{figure}{0}
\renewcommand{\thetable}{S\arabic{table}}
\setcounter{table}{0}
%=============================================================================

\section{Supplementary Results}
\label{app:extra_results}

\subsection*{Rule-Family Ranking}

\begin{table}[t]
\centering
\small
\begin{tabular}{@{}llcc@{}}
\toprule
\textbf{Dataset} & \textbf{Rule} & \textbf{Top-10 valid} & \textbf{Top-10 test} \\
\midrule
MNIST & 3F & 0.611 & 0.622 \\
MNIST & 4F & 0.620 & 0.628 \\
MNIST & 5F & \textbf{0.912} & \textbf{0.916} \\
\midrule
Context gating & 3F & 0.398 & 0.396 \\
Context gating & 4F & 0.411 & 0.411 \\
Context gating & 5F & \textbf{0.807} & \textbf{0.789} \\
\bottomrule
\end{tabular}
\caption{\textbf{Rule-family ranking.} Top-10 mean across completed local-competence sweeps.}
\label{tab:variant_ranking}
\end{table}

\subsection*{Broadcast Mode Comparison and Local-Mismatch Recheck}

\begin{table}[t]
\centering
\small
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Core} & \textbf{Broadcast} & \textbf{Decoder} & \textbf{Test (mean$\pm$std)} \\
\midrule
Shunting & per-soma & local & $0.912 \pm 0.005$ \\
Shunting & per-soma & backprop & $0.909 \pm 0.008$ \\
Shunting & local-mismatch & local & $0.146 \pm 0.046$ \\
Shunting & local-mismatch & backprop & $0.146 \pm 0.037$ \\
\midrule
Additive & per-soma & local & $0.894 \pm 0.007$ \\
Additive & per-soma & backprop & $0.900 \pm 0.001$ \\
Additive & local-mismatch & local & $0.342 \pm 0.058$ \\
Additive & local-mismatch & backprop & $0.348 \pm 0.095$ \\
\bottomrule
\end{tabular}
\caption{\textbf{Local-mismatch recheck (MNIST, 5F).} Per-soma is consistently strong; local-mismatch remains substantially weaker.}
\label{tab:local_mismatch_recheck}
\end{table}

\subsection*{Phase 1 Capacity Ceilings}

\begin{figure*}[t]
\centering
\includegraphics[width=0.94\textwidth]{fig_s1_calibration.pdf}
\caption{\textbf{Capacity calibration (supplementary).} \textbf{(A)}~Phase 1 backprop ceilings across all architectures and datasets. \textbf{(B)}~Rule-family ranking: 5F consistently outperforms 4F and 3F. \textbf{(C)}~Decoder mode comparison (local vs.\ backprop decoder, 5F MNIST). \textbf{(D)}~Broadcast mode comparison: per-soma is required for strong performance; local-mismatch fails.}
\label{fig:s1_calibration}
\end{figure*}

\subsection*{Ablation Results}

\begin{table}[t]
\centering
\small
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Condition} & \textbf{Metric} & \textbf{Value} \\
\midrule
Decoder: local vs backprop vs frozen (MNIST) & test acc & 0.379 vs 0.379 vs 0.176 \\
Shunting vs additive, matched (MNIST) & $\Delta$ test & $+0.210$ \\
Per-soma, path on vs off & test / MI(E,I;C) & $-0.003$ / $+0.017$ \\
\bottomrule
\end{tabular}
\caption{\textbf{Mechanistic ablations} in controlled small-network architectures.}
\label{tab:robust_headline}
\end{table}

\subsection*{Extended Gradient Analysis and $N_I$ Sweep Detail}

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\textwidth]{fig_s2_gradient_extended.pdf}
\caption{\textbf{Extended gradient and $N_I$ analysis (supplementary).} \textbf{(A)}~Scale mismatch bars: shunting achieves near-ideal scale ($0.117$); additive exhibits order-of-magnitude distortion ($>1.0$). \textbf{(B)}~Noise resilience $N_I$ dose-response with error bands ($\pm$1 s.d.). \textbf{(C)}~MNIST $N_I$ dose-response detail with error bands. \textbf{(D)}~Fashion-MNIST individual seeds for all conditions, showing consistency across runs.}
\label{fig:s2_gradient_extended}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=0.92\textwidth]{fig_s3_sandbox.pdf}
\caption{\textbf{Controlled small-network sandbox.} Strategy comparisons, learning dynamics, and gradient-fidelity trends.}
\label{fig:s3_sandbox}
\end{figure*}

\subsection*{Verification and Reproducibility}

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\textwidth]{fig_s4_verification.pdf}
\caption{\textbf{Verification and reproducibility (supplementary).} \textbf{(A)}~MNIST verification: main seeds (42--46) yield $91.4\pm0.3$\%; held-out seeds (47--49) yield $89.8\pm0.9$\%, confirming generalization. \textbf{(B)}~Context gating verification: main seeds ($+$HSIC) $80.3\pm0.6$\%; held-out seeds (no HSIC) $77.2\pm1.5$\%. The ${\sim}3$~pp gap reflects HSIC removal, not seed sensitivity. \textbf{(C)}~HSIC weight ablation on context gating: moderate weights ($0.01$--$0.1$) perform best.}
\label{fig:s4_verification}
\end{figure*}

\subsection*{FA/DFA Baseline Comparison}

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\textwidth]{fig_s5_fa_dfa.pdf}
\caption{\textbf{Feedback alignment baselines (MNIST).} \textbf{(A)}~Grouped comparison: standard backprop (gray), DFA (orange), and FA (purple). Red X marks indicate FA failure on dendritic architectures (dimensional incompatibility between random feedback matrices and block-structured dendritic layers). DFA achieves 66.1\% on shunting vs.\ 62.2\% additive vs.\ 51.8\% point MLP. \textbf{(B)}~Backprop$-$DFA gap: dendritic architectures (24.9--27.6~pp) show smaller gaps than point MLPs (39.5~pp), suggesting conductance-based architecture is partially compatible with random feedback.}
\label{fig:s5_fa_dfa}
\end{figure*}

\subsection*{CIFAR-10 Results}

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{fig_s6_cifar10.pdf}
\caption{\textbf{CIFAR-10 (flattened).} Shunting backprop: 40.1\%; additive backprop: 37.8\%; shunting local: 21.2\%; additive local: 10.0\% (chance). The backprop--local gap (${\sim}19$~pp for shunting) is larger than on MNIST (${\sim}0$~pp), reflecting the increased difficulty of propagating credit through conductance-based layers for complex visual features. Shunting still provides a clear advantage over additive under local learning ($+11.2$~pp).}
\label{fig:s6_cifar10}
\end{figure}

\subsection*{Additive Normalization Control}

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\textwidth]{fig_s7_additive_norm.pdf}
\caption{\textbf{Additive + normalization control (MNIST).} \textbf{(A)}~Standard backprop: normalization provides a small boost (89.7\% $\to$ 90.9\%). \textbf{(B)}~Local learning: normalization improves additive from 46.5\% to 56.0\% ($+9.5$~pp), partially closing the gap to shunting (61.7\%, dashed green). The remaining gap ($-5.7$~pp) indicates shunting provides benefits beyond simple voltage normalization, including bounded activations, conductance-dependent input weighting, and biophysically constrained sensitivity structure.}
\label{fig:s7_additive_norm}
\end{figure*}

\section{Implementation Details}
\label{app:implementation}

\subsection*{Biological Plausibility Assumptions}

Each synapse has access to: presynaptic activity $x_j$ (local), compartment voltage $V_n$ (local membrane potential), reversal potential $E_j$ (fixed by receptor type), and total input resistance $R_n^{\mathrm{tot}} = 1/g_n^{\mathrm{tot}}$ (computable from local conductances).
The 4F modulator $\rho_n$ requires online estimation of voltage covariance (biologically plausible via slow calcium signals); the 5F factor $\phi_n$ requires a linear regression proxy (implementable via eligibility traces).
The \emph{only non-local quantity} is the broadcast error $e_n$, which requires a top-down or neuromodulatory signal from the soma to dendritic compartments.
We assume per-neuron resolution (one error per output neuron) for the per-soma broadcast mode.

\subsection*{Units and Parameterization}
\begin{table}[t]\centering\small
\begin{tabular}{@{}lll@{}}\toprule
Quantity & Symbol & Convention\\\midrule
Voltage & $V$ & Normalized to $[-1,1]$\\
Conductances & $g^{\mathrm{syn}}, g^{\mathrm{den}}$ & Nonneg.\ via softplus\\
Leak conductance & $g^{\mathrm{leak}}$ & Set to $1$\\
Input resistance & $R^{\mathrm{tot}}$ & $\leq 1$\\\bottomrule
\end{tabular}
\caption{Units and normalization.}
\label{tab:units}
\end{table}

\subsection*{Decoder Update Modes}
$W_{\mathrm{dec}}$ maps $V_L \to \hat{y}$. Modes: \textbf{backprop} ($\nabla_{W}L$ via autograd), \textbf{local} ($\Delta W = \eta\langle \delta_0 V_L^\top\rangle_B$), \textbf{frozen} ($\Delta W = 0$).

\subsection*{Algorithm}
\begin{algorithm}[H]
\caption{Local Credit Assignment}\small
\begin{algorithmic}[1]
\STATE \textbf{Input:} Model, batch $(x,y)$, config $\mathcal{C}$
\STATE Forward pass; loss $L$, output error $\delta^y$
\STATE Somatic error $\delta_0 = W_{\mathrm{dec}}^\top \delta^y$
\FOR{each layer $n$ (reverse)}
    \STATE $e_n = \text{broadcast}(\delta_0, \mathcal{C})$
    \STATE Compute $\rho_n$, $\phi_n$ (EMA estimators)
    \STATE Apply 3F/4F/5F update (Eq.~\ref{eq:3f} or \ref{eq:5f})
\ENDFOR
\STATE Clip gradients; optimizer step
\end{algorithmic}
\end{algorithm}

\section{Theoretical Details}
\label{app:theory}

\subsection*{Variant Taxonomy}
\begin{table}[t]\centering\small
\begin{tabular}{@{}llcl@{}}\toprule
\textbf{Rule} & \textbf{Factors} & \textbf{Cost} & \textbf{Best regime} \\\midrule
3F & $x, (E\!-\!V), e$ & $\mathcal{O}(1)$ & Baseline \\
4F & 3F $+ \rho$ & $\mathcal{O}(1)$ & Improved conditioning \\
5F & 4F $+ \phi$ & $\mathcal{O}(d_n)$ & Strongest overall \\
\bottomrule
\end{tabular}
\caption{Variant taxonomy.}
\label{tab:taxonomy}
\end{table}

\subsection*{Biological Analogs}
\begin{table}[t]\centering\small
\begin{tabular}{@{}lll@{}}\toprule
\textbf{Component} & \textbf{Analog} & \textbf{Interpretation} \\\midrule
$R_n^{\mathrm{tot}}$ & Input resistance & Sensitivity modulation \\
$(E_j - V_n)$ & Synaptic driving force & Local gradient factor \\
Shunting & Divisive normalization & $\partial V/\partial g_I \propto -V$ \\
$\rho_n$ & Layer relevance & Output correlation \\
$\phi_n$ & Signal propagation & Conditional predictability \\
\bottomrule
\end{tabular}
\caption{Biological analogs.}
\label{tab:bio_analogs}
\end{table}

\section{Morphology-Aware Extensions}
\label{app:morphology}

\paragraph{Path-integrated propagation.}
Modulate broadcast error by $\pi_n = \pi_{n-1} \cdot R_{n-1}^{\mathrm{tot}} \cdot \bar{g}_{n-1}^{\mathrm{den}}$, approximating depth attenuation from Eq.~\ref{eq:path_sum}.

\paragraph{Depth modulation.}
Per-branch scaling $\rho_j = \rho_{\mathrm{base}} / (d_j + \alpha)$, mirroring cable attenuation.

\paragraph{Dendritic normalization.}
$\Delta g_j^{\mathrm{den}} \leftarrow \Delta g_j^{\mathrm{den}} / (\sum_k g_k^{\mathrm{den}} + \varepsilon)$, analogous to homeostatic scaling \cite{turrigiano2008homeostatic}.

\paragraph{Apical/basal differentiation.}
Branch-type scaling $s_j$ for differential plasticity \cite{larkum2013apical}.

\section{HSIC Auxiliary Objectives}
\label{app:hsic}

Following \cite{gretton2005hsic}, we use kernel-based HSIC objectives.
Self-decorrelation: $\mathcal{L}^{\mathrm{self}} = B^{-2}\tr(\mathbf{K}_Z\mathbf{H}\mathbf{K}_Z\mathbf{H})$.
Target-correlation: $\mathcal{L}^{\mathrm{target}} = -B^{-2}\tr(\mathbf{K}_Z\mathbf{H}\mathbf{K}_Y\mathbf{H})$.
Moderate weights ($0.01$--$0.1$) help on context gating; negligible on MNIST.
Online statistics ($\rho_n$, $\phi_n$) use Welford's algorithm \cite{welford1962note}.

\section{Online Variant with Eligibility Traces}
\label{app:eligibility}

Continuous-time eligibility: $\tau_e \dot{e}_j^{\mathrm{syn}} = -e_j^{\mathrm{syn}} + x_j(E_j - V_n)R_n^{\mathrm{tot}}$.
Update: $\Delta g_j^{\mathrm{syn}} \propto \int e_j^{\mathrm{syn}}(t) m_n(t)\,\mathrm{d}t$ \cite{fremaux2016three,bellec2020eprop}.

\section{Depth Scaling and Noise Robustness}
\label{app:depth_noise}

\paragraph{Depth scaling.}
Varying dendritic depth from 1--4 layers (branch factors $[9]$ to $[3,3,3,3]$): shunting local degrades from 63.5\% to 57.4\%; additive local degrades more steeply from 54.9\% to 29.7\%. The shunting advantage grows with depth ($+8.5 \to +27.7$~pp). Backprop ceilings remain stable at ${\sim}90{-}92\%$. See Fig.~\ref{fig:scalability}A.

\paragraph{Noise robustness.}
Gaussian noise $\mathcal{N}(0,\sigma^2)$ on broadcast error: shunting is robust to $\sigma\!\leq\!0.1$ (${\sim}62\%$) while additive drops from 46.5\% to chance at $\sigma{=}1.0$, confirming that shunting credit signals carry genuine learning information beyond the broadcast magnitude. See Fig.~\ref{fig:scalability}B.

\end{document}
