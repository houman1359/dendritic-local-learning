\PassOptionsToPackage{numbers,sort&compress}{natbib}
\documentclass{article}
\usepackage{neurips_2025}
\usepackage{microtype}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage[hidelinks,hypertexnames=false,bookmarks=false]{hyperref}
\graphicspath{{figures/}{./figures/}}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{corollary}{Corollary}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\tr}{tr}

\title{Local Credit Assignment in Compartmental Dendritic Networks}
\author{
Anonymous Authors \\
Paper under double-blind review
}
\date{}

\begin{document}
\maketitle

\begin{abstract}
How can neurons learn efficiently when plasticity is synapse-local and global supervision is low bandwidth? We study conductance-based compartmental dendritic networks and show that dendritic structure and shunting inhibition create regimes where strictly local learning becomes effective and mechanistically interpretable. Starting from the compartment voltage equation, we derive exact loss gradients for arbitrary dendritic trees and obtain a factorization that highlights synapse-local terms (presynaptic drive, driving force, input resistance) and a global modulatory term (broadcast error). This motivates a hierarchy of local rules---3-factor (3F), morphology-modulated 4-factor (4F), and information-modulated 5-factor (5F) updates---plus morphology-aware and shunting-aware extensions. Empirically, shunting inhibition is the key architectural enabler: across completed sweeps it yields large gains over additive controls and markedly improves local-vs-backprop gradient fidelity (direction and scale). Under a calibrated capacity regime, the best local configuration (5F with per-soma broadcast) reaches $0.914\pm0.003$ test accuracy on MNIST and $0.803\pm0.006$ on context gating, compared to backprop ceilings of $0.965$ and $0.864$ on the same shunting architecture.
\end{abstract}

\section{Introduction}

Credit assignment in deep networks is accurate with backpropagation but biologically implausible: it requires global error transport through the exact transpose of forward weights, and symmetric forward-backward pathways that have no known biological substrate. Dendritic neurons suggest an alternative architecture for learning. Real neurons possess spatially extended dendritic trees where each synapse has access to rich local state---driving forces, conductances, and branch-specific context---while global supervision may be reduced to a low-bandwidth modulatory broadcast from the soma.

This paper asks a concrete question: \emph{can dendritic structure and shunting inhibition create regimes where strictly local learning rules (synapse-local factors + a broadcast teaching signal) approach the credit-assignment quality of backpropagation?} We answer affirmatively. Starting from conductance-based dendritic equations, we derive exact gradients for dendritic trees and construct a hierarchy of biologically-local approximations (3-factor, 4-factor, and 5-factor rules). We then show empirically that \emph{shunting inhibition}---conductance that primarily modulates input resistance and gain---is the key architectural enabler for local learning: it stabilizes credit signals and substantially improves local-vs-backprop gradient fidelity compared to additive dendritic controls.

\paragraph{Contributions.}
\begin{enumerate}
\item \textbf{Exact gradients for compartmental dendritic trees.} We derive the exact loss gradient for arbitrary dendritic tree morphologies in a conductance-based compartment model, making explicit the multiplicative path factors that standard backprop implicitly computes.
\item \textbf{A unified local-rule template (3F/4F/5F + modifiers).} We express a family of strictly local updates in a shared factorized form, separating synapse-local terms (presynaptic drive, driving force, input resistance) from global teaching terms (broadcast errors) and optional morphology/information modulators.
\item \textbf{Shunting as an architectural enabler of local learning.} We show that shunting inhibition yields large and regime-dependent benefits for local learning and that these gains are accompanied by substantially improved local-vs-backprop gradient fidelity compared to additive dendritic controls.
\item \textbf{Mechanistic diagnostics beyond accuracy.} We introduce a component-wise gradient-fidelity diagnostic (direction and scale) that links architecture and learning-rule design to credit-signal quality.
\end{enumerate}

\paragraph{Three empirical findings (this draft).}
\begin{enumerate}
\item \textbf{Local competence:} In a calibrated capacity regime, 5F with per-soma broadcast closes much of the backprop gap on both MNIST and context gating (Section~\ref{sec:experiments}).
\item \textbf{Regime dependence:} Shunting dendritic cores outperform additive controls under the same local rule, with gaps widening under stronger inhibition/noise (Fig.~\ref{fig:claimA_shunting_heatmap}).
\item \textbf{Mechanism:} Shunting architectures yield substantially higher local-vs-backprop gradient fidelity (Table~\ref{tab:gradient_alignment_summary}, Fig.~\ref{fig:gradient_alignment_components}), supporting a mechanistic explanation for the performance gains.
\end{enumerate}

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\textwidth]{fig_model_schematic.pdf}
\caption{\textbf{Model overview.} (A) A compartmental dendritic neuron where \emph{each branch} receives both excitatory ($E_j > 0$, blue) and inhibitory ($E_j = 0$, red) synaptic inputs via separate sparse connectivity (TopK). Inhibitory conductances enter only the denominator of the voltage equation (shunting/divisive normalization). Dendritic branch voltages propagate toward the soma via learned dendritic conductances (green). The steady-state voltage at each compartment is a conductance-weighted average (Eq.~\ref{eq:voltage}). (B) Local learning rules of increasing complexity: 3-factor (pre-synaptic activity $\times$ driving force $\times$ broadcast error), 4-factor (+ variance modulator $\rho$), and 5-factor (+ information-theoretic factor $\phi$). The same rule applies to both E and I synapses; the sign difference arises from the driving force $(E_j - V_n)$. The broadcast error $\delta$ can operate in scalar, per-soma, or local mismatch mode.}
\label{fig:model_schematic}
\end{figure*}

\section{Compartmental Voltage Model}
We use a standard steady-state conductance model obtained by discretizing passive cable dynamics (e.g., \cite{koch1999biophysics,dayan2001theoretical}). In normalized units with leak reversal potential $0$ and unit leak conductance, each compartment voltage is a conductance-weighted average of synaptic reversal potentials, child voltages, and leak. This form makes two facts explicit: (i) local sensitivities depend on the driving force $(E-V)$ and input resistance $R^{\mathrm{tot}}$, and (ii) shunting inhibition corresponds to adding conductance with $E_{\mathrm{inh}}\approx 0$ (Section~\ref{sec:shunting}).

\subsection{Voltage Equation}

Consider compartment $n$ receiving synaptic inputs indexed by $j$ and dendritic inputs from child compartments. Let:
\begin{itemize}
\item $x_j \in \mathbb{R}_+$: presynaptic activity at synapse $j$
\item $E_j \in \mathbb{R}$: reversal potential of synapse $j$ (excitatory: $E_j > 0$; inhibitory: $E_j \leq 0$)
\item $g_j^{\mathrm{syn}} \geq 0$: synaptic conductance (learned parameter)
\item $V_j \in \mathbb{R}$: voltage of child compartment $j$
\item $g_j^{\mathrm{den}} \geq 0$: dendritic conductance from child $j$ (learned parameter)
\end{itemize}

\paragraph{Currents.}
Synaptic current:
\begin{equation}
I_{\mathrm{syn}} = \sum_j (E_j - V_n) x_j g_j^{\mathrm{syn}}
\end{equation}
Dendritic current:
\begin{equation}
I_{\mathrm{den}} = \sum_j (V_j - V_n) g_j^{\mathrm{den}}
\end{equation}

\paragraph{Steady-state voltage.}
With unit leak conductance to reversal potential $0$:
\begin{equation}
V_n = \frac{\sum_j E_j x_j g_j^{\mathrm{syn}} + \sum_j V_j g_j^{\mathrm{den}}}{\sum_j x_j g_j^{\mathrm{syn}} + \sum_j g_j^{\mathrm{den}} + 1}
\label{eq:voltage}
\end{equation}

\paragraph{Total conductance and resistance.}
\begin{equation}
g_n^{\mathrm{tot}} = \sum_j x_j g_j^{\mathrm{syn}} + \sum_j g_j^{\mathrm{den}} + 1, \qquad R_n^{\mathrm{tot}} = \frac{1}{g_n^{\mathrm{tot}}}
\label{eq:conductance}
\end{equation}

\begin{lemma}[Convexity and Bounds]
\label{lem:convex}
Let $\mathcal{S}_n=\{E_j\}_{\text{syn at }n}\cup \{V_j\}_{\text{children}} \cup \{0\}$. Then $V_n$ in \eqref{eq:voltage} is a convex combination of elements of $\mathcal{S}_n$, hence
\[
\min \mathcal{S}_n \ \le\ V_n \ \le\ \max \mathcal{S}_n.
\]
Moreover, $0<R_n^{\mathrm{tot}}\le 1$ and $R_n^{\mathrm{tot}} g_{i}^{\mathrm{den}}<1$ for all $i$.
\end{lemma}

\subsection{Local Sensitivities}

\begin{proposition}[Synaptic Gradient]
\label{prop:grad_gsyn}
\begin{equation}
\frac{\partial V_n}{\partial g_i^{\mathrm{syn}}} = x_i R_n^{\mathrm{tot}} (E_i - V_n)
\label{eq:grad_gsyn}
\end{equation}
\end{proposition}

\begin{proposition}[Child Voltage Gradient]
\label{prop:grad_V}
\begin{equation}
\frac{\partial V_n}{\partial V_i} = g_i^{\mathrm{den}} R_n^{\mathrm{tot}}
\label{eq:grad_V}
\end{equation}
\end{proposition}

\begin{proposition}[Dendritic Gradient]
\label{prop:grad_gden}
\begin{equation}
\frac{\partial V_n}{\partial g_i^{\mathrm{den}}} = R_n^{\mathrm{tot}} (V_i - V_n)
\label{eq:grad_gden}
\end{equation}
\end{proposition}

\subsection{Shunting Inhibition and Divisive Gain Control}
\label{sec:shunting}

A conductance-based inhibitory synapse with $E_{\mathrm{inh}}\approx E_\mathrm{leak}{=}0$ contributes current $I_{\mathrm{inh}} = (0 - V_n)\, x_j g_j^{\mathrm{syn}}$ and increases $g_n^{\mathrm{tot}}$ in \eqref{eq:conductance}. 

\begin{proposition}[Subthreshold Effect of Shunts]
For a pure shunt ($E_j=0$), the steady-state sensitivity to the inhibitory conductance is
\[
\frac{\partial V_n}{\partial g_j^{\mathrm{syn}}}
= x_j R_n^{\mathrm{tot}} (0 - V_n) = - x_j R_n^{\mathrm{tot}} V_n.
\]
Thus $V_n$ is multiplicatively attenuated (divisive normalization) by increased inhibitory conductance at fixed drives.
\end{proposition}

\begin{remark}[Divisive vs.\ Subtractive at the Firing-Rate Level]
While shunting produces divisive scaling of subthreshold voltages, its net effect on firing rates can be subtractive in many regimes \cite{holt1997shunting}, so we report both voltage- and rate-level analyses in experiments. Normalization via added conductance is consistent with canonical divisive normalization models in cortex \cite{carandini2012normalization}.
\end{remark}

\paragraph{Inhibitory/shunting synapses.}
For an inhibitory synapse with $E_j \approx 0$, the 3F update reduces to
\[
\Delta g_{j,\mathrm{inh}}^{\mathrm{syn}} 
= \eta\, \langle x_j R_n^{\mathrm{tot}} (-V_n)\, e_n \rangle_B,
\]
i.e., anti-Hebbian in $V_n$ and divisive in $g_n^{\mathrm{tot}}$. 
With 4F/5F, multiply by $\rho$ and $\phi$ (Def.~\ref{def:phi_fixed}). Note that the same multiplicative factors are applied to both excitatory and inhibitory synapses in the implementation; the sign difference arises solely from the driving force $(E_j - V_n)$.

\subsection{Loss Propagation}

Let $V_0$ denote the somatic/output compartment. The decoder produces $\hat y = W_{\mathrm{dec}} V_0$ (linear case), and $L$ is the task loss. Define the error gradients:
\begin{equation}
\delta^y := \frac{\partial L}{\partial \hat y}, \qquad
\delta_0 := \frac{\partial L}{\partial V_0} = \left(\frac{\partial \hat y}{\partial V_0}\right)^\top \delta^y
= W_{\mathrm{dec}}^\top \delta^y.
\end{equation}

\begin{theorem}[Backpropagation on a Dendritic Tree]
\label{thm:tree_backprop}
Let the dendritic morphology be a rooted tree with soma/output at node $0$. For any compartment $n$ with parent set $\mathcal{P}(n)$ (typically $|\mathcal{P}(n)|{=}1$), the loss gradient satisfies the recursion
\begin{equation}
\frac{\partial L}{\partial V_n}
=\sum_{p \in \mathcal{P}(n)} \frac{\partial L}{\partial V_p}\, \frac{\partial V_p}{\partial V_n}
=\sum_{p \in \mathcal{P}(n)} \delta_p \, R_p^{\mathrm{tot}}\, g_{n\to p}^{\mathrm{den}},
\qquad \delta_p \equiv \frac{\partial L}{\partial V_p}.
\label{eq:tree_recursion}
\end{equation}
Unrolling the recursion yields a sum over all directed paths $\mathcal{P}: n \leadsto 0$:
\begin{equation}
\frac{\partial L}{\partial V_n} 
= \frac{\partial L}{\partial V_0}
\sum_{\mathcal{P}:n\leadsto 0}
\prod_{(i\to k)\in \mathcal{P}} R_k^{\mathrm{tot}}\, g_{i\to k}^{\mathrm{den}}.
\label{eq:path_sum}
\end{equation}
\end{theorem}
\begin{proof}
Apply the multivariate chain rule on the directed acyclic computation graph defined by the tree; use Proposition~\ref{prop:grad_V}. Each path contributes a product of edge sensitivities. Summing over parents produces \eqref{eq:tree_recursion}; unrolling yields \eqref{eq:path_sum}.
\end{proof}

\section{Local Learning Approximations}

\subsection{Broadcast Error Approximation}
\label{sec:broadcast}

\begin{definition}[Local Approximation]
Replace the exact gradient $\frac{\partial L}{\partial V_n}$ with a broadcast error signal $e_n$ derived from the output error $\delta_0 = \frac{\partial L}{\partial V_0}$:
\begin{equation}
\frac{\partial L}{\partial V_n} \approx e_n, \qquad \prod_{i=0}^n R_i^{\mathrm{tot}} g_i^{\mathrm{den}} \approx 1
\label{eq:local_approx}
\end{equation}
\end{definition}

Three broadcast modes are considered:

\paragraph{(A) Scalar broadcast.}
For minibatch index $b$:
\begin{equation}
\bar{\delta}(b) = \frac{1}{d_{\mathrm{out}}} \sum_{k=1}^{d_{\mathrm{out}}} \delta_k(b), \qquad e_n(b) = \bar{\delta}(b) \mathbf{1}_{d_n}
\end{equation}

\paragraph{(B) Per-compartment mapping.}
If $d_n = d_{\mathrm{out}}$: $e_n(b) = \delta(b)$. Otherwise, \emph{fallback to scalar broadcast}. An optional DFA-style mode uses a fixed random feedback matrix $B_n \in \mathbb{R}^{d_n \times d_{\mathrm{out}}}$ sampled once at initialization: $e_n(b) = B_n \delta(b)$. This supports testing Theorem~\ref{thm:fa_alignment}.

\paragraph{(C) Local mismatch modulation.}
Let $P_n(b)$ be parent compartment drive (e.g., blocklinear output). Define centered mismatch:
\begin{equation}
\varepsilon_n(b) = \left(P_n(b) - V_n(b)\right) - \frac{1}{B} \sum_{t=1}^B \left(P_n(t) - V_n(t)\right)
\end{equation}
Then:
\begin{equation}
e_n(b) = \bar{\delta}(b) \varepsilon_n(b)
\end{equation}

\subsection{Gradient Alignment with Broadcast Errors}
\label{sec:alignment}

Define the exact synaptic gradient at layer $n$ by $g^{\mathrm{exact}} = \delta_0 \cdot \Xi_n$, where $\Xi_n$ collects local factors and the exact path-sum \eqref{eq:path_sum}. The local 3F gradient with broadcast error $e_n=B_n \delta_0$ is $g^{\mathrm{local}} = e_n \cdot \widehat{\Xi}_n$, where $\widehat{\Xi}_n$ omits the path-sum.

\begin{theorem}[Positive Expected Alignment under Random Broadcast]
\label{thm:fa_alignment}
Let $B_n\in\mathbb{R}^{d_n\times d_{\mathrm{out}}}$ have i.i.d.\ zero-mean entries with $\mathbb{E}[B_n^\top B_n]=\alpha I$. If the decoder aligns with the forward pathway (standard during training), then
\[
\mathbb{E}\big[\cos\angle(g^{\mathrm{local}},g^{\mathrm{exact}})\big] \ \ge\ c_n>0,
\]
where $c_n$ depends on $\alpha$ and the average correlation between $\widehat{\Xi}_n$ and $\Xi_n$. Thus $g^{\mathrm{local}}$ provides a descent direction in expectation.
\end{theorem}
\begin{proof}[Sketch]
Adapt the feedback-alignment argument \cite{lillicrap2016random,nokland2016dfa}: fixed random feedback suffices for alignment as forward weights adapt. Here, $\widehat{\Xi}_n$ is proportional to $\Xi_n$ up to the missing path factor; Jensen bounds on \eqref{eq:path_sum} yield $c_n>0$.
\end{proof}

\subsection{Three-Factor Rule (3F)}

\begin{definition}[3F Learning Rule]
For synaptic conductances:
\begin{equation}
\Delta g_j^{\mathrm{syn}} = \eta \left\langle x_j R_n^{\mathrm{tot}} (E_j - V_n) e_n \right\rangle_B
\label{eq:3f_syn}
\end{equation}
For dendritic conductances:
\begin{equation}
\Delta g_j^{\mathrm{den}} = \eta \left\langle R_n^{\mathrm{tot}} (V_j - V_n) e_n \right\rangle_B
\label{eq:3f_den}
\end{equation}
where $\langle \cdot \rangle_B$ denotes batch average.
\end{definition}

\begin{remark}
The three factors are: (1) presynaptic activity $x_j$ or voltage difference $(V_j - V_n)$, (2) postsynaptic modulation $(E_j - V_n)$ or $R_n^{\mathrm{tot}}$, (3) broadcast error $e_n$.
\\[2pt]
\textbf{Symmetry note.} The same multiplicative factors ($R_n^{\mathrm{tot}}$, $\rho$, $\phi$, $s_j$) apply to both excitatory and inhibitory synapses; the sign difference arises solely from the driving force $(E_j - V_n)$.
\end{remark}

\subsection{Four-Factor Rule (4F): Morphology Correlation}

\begin{definition}[Morphology Factor]
Let $\bar{V}_n = \frac{1}{d_n} \sum_{j=1}^{d_n} V_{n,j}$ be the mean voltage over compartments in layer $n$. Define the correlation with output:
\begin{equation}
\rho_n = \frac{\Cov(\bar{V}_n, \bar{V}_0)}{\sqrt{\Var(\bar{V}_n) \Var(\bar{V}_0)} + \varepsilon}
\label{eq:rho}
\end{equation}
\end{definition}

\begin{proposition}[4F Update Rule]
Multiply 3F updates by $\rho_n$:
\begin{align}
\Delta g_j^{\mathrm{syn}} &= \eta \rho_n \left\langle x_j R_n^{\mathrm{tot}} (E_j - V_n) e_n \right\rangle_B \label{eq:4f_syn} \\
\Delta g_j^{\mathrm{den}} &= \eta \rho_n \left\langle R_n^{\mathrm{tot}} (V_j - V_n) e_n \right\rangle_B \label{eq:4f_den}
\end{align}
\end{proposition}

\begin{proposition}[Approximate Gradient Alignment]
Let $L$ be a smooth loss. If layer $n$ contributes to the output primarily through its mean activity, then
$\mathbb{E}[\frac{\partial L}{\partial \bar{V}_n} \cdot \bar{V}_n] \propto \rho_n \cdot \Var(\bar{V}_n)$.
Thus $\rho_n$ weights updates by the layer's relevance to the task.
\end{proposition}

\paragraph{Estimators (EMA / online).}
For minibatches $B\ge 2$, estimate $\rho_n$ from \eqref{eq:rho} with an EMA over batches.
For $B=1$ (online), maintain means $\mu_x,\mu_y$, variances $\sigma_x^2,\sigma_y^2$, and covariance $C_{xy}$ for
$x_t=\bar V_0^{(t)}$ and $y_t=\bar V_n^{(t)}$ using Welford's numerically stable algorithm \cite{welford1962note}:
\begin{align}
\mu_x^{(t)} &= (1-\alpha)\mu_x^{(t-1)} + \alpha x_t, &
\mu_y^{(t)} &= (1-\alpha)\mu_y^{(t-1)} + \alpha y_t,\nonumber\\
\delta_x &= x_t - \mu_x^{(t-1)}, &
\delta_y &= y_t - \mu_y^{(t-1)},\nonumber\\
\sigma_x^{2(t)} &= (1-\alpha)\sigma_x^{2(t-1)} + \alpha\, \delta_x^2, &
\sigma_y^{2(t)} &= (1-\alpha)\sigma_y^{2(t-1)} + \alpha\, \delta_y^2,\nonumber\\
C_{xy}^{(t)} &= (1-\alpha)C_{xy}^{(t-1)} + \alpha\, \delta_x \delta_y.
\end{align}
Then $\rho_n^{(t)} = C_{xy}^{(t)} / (\sqrt{\sigma_x^{2(t)}\sigma_y^{2(t)}}+\varepsilon)$, where $\alpha$ is the EMA rate.

\subsection{Five-Factor Rule (5F): Conditional Information}

\begin{definition}[Conditional Predictability Factor]
\label{def:phi_fixed}
Let $P_n$ be parent compartment voltage. Define the conditional variance via ridge regression:
\begin{align}
\beta_n &= \frac{\Cov(V_n, P_n)}{\Var(P_n) + \lambda} \label{eq:beta} \\
\sigma_{\mathrm{res}}^2 &= \Var(V_n) - \beta_n \Cov(V_n, P_n) \label{eq:residual_var}
\end{align}
The information proxy is:
\begin{equation}
\phi_n = \frac{\Var(V_n)}{\sigma_{\mathrm{res}}^2 + \varepsilon} = \frac{1}{1 - R_n^2} \geq 1,
\label{eq:phi}
\end{equation}
where $R_n^2=\frac{\beta_n\,\Cov(V_n,P_n)}{\Var(V_n)}$ is the (ridge) coefficient of determination.
\end{definition}

\begin{remark}[Information-Theoretic Interpretation]
$\phi_n$ increases when $V_n$ is \emph{more} predictable from its parent $P_n$ (higher $R^2$), amplifying updates for compartments with strong signal propagation. In practice, $\phi_n$ is clamped to $[0.25, 4.0]$ for stability. An alternative $\phi_n = 1 - R^2$ would instead emphasize compartments with unique information; both are valid depending on whether coherent signal flow or novelty is prioritized.
\end{remark}

\begin{proposition}[5F Update Rule]
\begin{align}
\Delta g_j^{\mathrm{syn}} &= \eta \rho_n \phi_n \left\langle x_j R_n^{\mathrm{tot}} (E_j - V_n) e_n \right\rangle_B \label{eq:5f_syn} \\
\Delta g_j^{\mathrm{den}} &= \eta \rho_n \phi_n \left\langle R_n^{\mathrm{tot}} (V_j - V_n) e_n \right\rangle_B \label{eq:5f_den}
\end{align}
\end{proposition}

\section{Morphology-Aware Extensions}

Standard 4F/5F rules use layer-wise factors $\rho_n$, $\phi_n$ that ignore branch-specific topology. We introduce four extensions that explicitly incorporate dendritic tree structure.

\subsection{Path-Integrated Propagation}

Exact tree backpropagation contains a path-sum of multiplicative edge factors (Eq.~\ref{eq:path_sum}), which induces depth-dependent attenuation. We approximate this attenuation with a per-layer \emph{path factor} $\pi_n$ and modulate the broadcast error as $e_n \leftarrow e_n \cdot \pi_n$:
\begin{equation}
\pi_n = \begin{cases}
1 & n = 0 \\
\pi_{n-1} \cdot R_{n-1}^{\mathrm{tot}} \cdot \bar{g}_{n-1}^{\mathrm{den}} & n \geq 1,
\end{cases}
\label{eq:path_factor}
\end{equation}
where $\bar{g}_{n-1}^{\mathrm{den}}$ is the mean dendritic conductance from layer $n-1$ to $n$. In practice, $\pi_n$ is computed per sample and broadcast within each layer for stability (Appendix~\ref{app:morphology}).

\subsection{Additional Extensions}

Beyond path propagation, we implement three further morphology-aware mechanisms (detailed in Appendix~\ref{app:morphology}):
\begin{itemize}
\item \textbf{Depth modulation:} Per-branch scaling $\rho_j = \rho_{\mathrm{base}} / (d_j + \alpha)$ that mirrors cable attenuation, biasing learning toward proximal synapses.
\item \textbf{Dendritic normalization:} Update normalization by total branch conductance, $\Delta g_j^{\mathrm{den}} \leftarrow \Delta g_j^{\mathrm{den}} / (\sum_k g_k^{\mathrm{den}} + \varepsilon)$, which stabilizes update variance analogous to homeostatic scaling \cite{turrigiano2008homeostatic}.
\item \textbf{Apical/basal differentiation:} Branch-type-specific scaling factors $s_j$ that allow differential plasticity in feedback (apical) vs.\ feedforward (basal) compartments \cite{larkum2013apical}.
\end{itemize}

\paragraph{HSIC auxiliary objectives.}
We optionally add Hilbert-Schmidt Independence Criterion (HSIC) losses \cite{gretton2005hsic} as auxiliary objectives: a self-decorrelation term that encourages diverse representations within each layer, and a target-correlation term that aligns layer activations with labels. Moderate HSIC weights improve performance on context-gating tasks while having negligible effect on MNIST (see Section~\ref{sec:experiments}). Full definitions and gradients are in Appendix~\ref{app:hsic}.

\section{Experiments}
\label{sec:experiments}

\subsection{Setup}
We evaluate local credit assignment in two complementary regimes:
(i)~a \emph{capacity-calibrated} regime where standard backprop achieves high accuracy on the same architectures used for local learning, and
(ii)~a \emph{controlled small-network} sandbox used to isolate mechanisms (decoder locality, inhibition sweeps, and broadcast/path interactions).
We report MNIST and context gating as primary datasets, with CIFAR-10 (flattened) used only as a sanity check for the decoder-locality and shunting-regime claim sweeps (Appendix~\ref{app:extra_results}).
Architectures include point MLP baselines and dendritic cores with either additive dendritic integration or shunting (conductance-based) inhibition. For each setting, we compare standard backprop training to LocalCA training under matched optimization protocols.

\subsection{Results: Three Findings}

\paragraph{Finding 1: Local competence under calibrated capacity.}
In a Phase-1 capacity calibration sweep, standard training achieves high ceilings on MNIST and context gating with dendritic shunting cores ($0.965$ and $0.864$ test, respectively; Appendix~\ref{app:extra_results}).
Within this same capacity regime, Phase-2b local-competence sweeps show that the 5F family is consistently strongest, and that per-soma broadcast is a critical factor (Appendix~\ref{app:extra_results}).
Table~\ref{tab:gap_closing_summary} summarizes the resulting gap closing for the best completed LocalCA configurations.

\begin{table}[t]
\centering
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Dataset} & \textbf{Backprop ceiling (shunting)} & \textbf{Best LocalCA (5F, per-soma)} & \textbf{Gap} \\
\midrule
MNIST & 0.965 & $0.914\pm0.003$ & $-0.051$ \\
Context gating & 0.864 & $0.803\pm0.006$ & $-0.061$ \\
\bottomrule
\end{tabular}
\caption{\textbf{Local competence in a calibrated capacity regime.} Backprop ceilings are computed from Phase-1 capacity sweeps; LocalCA values are the best completed Phase-2b results within the same architecture regime. Errors are across seeds.}
\label{tab:gap_closing_summary}
\end{table}

\paragraph{Finding 2: Regime dependence across inhibition strength.}
Local learning gains are not uniform: they concentrate in inhibition/noise-stress regimes where shunting inhibition is active.
Figure~\ref{fig:claimA_shunting_heatmap} shows the shunting-minus-additive advantage over inhibitory synapse count and broadcast mode; the gap widens as inhibition increases, consistent with shunting-linked divisive gain control stabilizing credit signals.

\begin{figure*}[t]
\centering
\includegraphics[width=0.94\textwidth]{fig_phase3_claimA_shunting_heatmap.pdf}
\caption{\textbf{Shunting advantage across inhibition strength.} Heatmap of shunting minus additive test-accuracy difference over inhibitory synapse count and broadcast modes, showing the regimes where shunting-linked local credit is most beneficial.}
\label{fig:claimA_shunting_heatmap}
\end{figure*}

\paragraph{Finding 3: Shunting improves credit signal fidelity.}
To test whether performance gains correspond to higher-quality credit signals, we compare LocalCA and backprop gradients on the \emph{same batch and same weights} using a component-wise gradient-fidelity diagnostic (Section~\ref{sec:grad_fidelity}).
Shunting architectures show substantially higher directional alignment and lower scale mismatch than additive controls on both MNIST and context gating (Table~\ref{tab:gradient_alignment_summary}, Fig.~\ref{fig:gradient_alignment_components}).

\subsection{Gradient-Fidelity Diagnostic (Local vs Backprop)}
\label{sec:grad_fidelity}

To test whether improved performance corresponds to better credit signals, we compare LocalCA and backprop gradients on the \emph{same batch and same initial weights}, component-wise. For each parameter tensor \(p\), we compute
\begin{equation}
\cos_p = \frac{\langle g_p^{\mathrm{local}}, g_p^{\mathrm{bp}} \rangle}{\|g_p^{\mathrm{local}}\|_2 \, \|g_p^{\mathrm{bp}}\|_2 + \varepsilon},
\end{equation}
and a scale mismatch
\begin{equation}
\Delta^{\mathrm{scale}}_p = \left| \log_{10} \frac{\|g_p^{\mathrm{local}}\|_2}{\|g_p^{\mathrm{bp}}\|_2 + \varepsilon} \right|.
\end{equation}
We then aggregate by parameter count across component groups (excitatory synapses, inhibitory synapses, dendritic conductances, reactivation).

\begin{table}[t]
\centering
\small
\begin{tabular}{@{}llccc@{}}
\toprule
\textbf{Dataset} & \textbf{Core} & \textbf{Weighted cosine} & \textbf{Scale mismatch} & \textbf{Rel.\ L2} \\
\midrule
MNIST & Shunting & \textbf{0.202} & \textbf{0.117} & \textbf{1.130} \\
MNIST & Additive & 0.006 & 1.053 & 13.324 \\
\midrule
Context gating & Shunting & \textbf{0.108} & \textbf{0.036} & \textbf{1.404} \\
Context gating & Additive & -0.007 & 2.154 & 145.965 \\
\bottomrule
\end{tabular}
\caption{\textbf{Gradient-fidelity summary (5F + per-soma broadcast).} Local vs backprop gradients compared on matched weights and batches. ``Weighted cosine'' is parameter-count weighted over component groups. ``Scale mismatch'' is $|\log_{10}(\|g^{\mathrm{local}}\|/\|g^{\mathrm{bp}}\|)|$ (lower is better). Shunting networks show $30\times$ better directional alignment and $10\times$ lower scale distortion than additive controls.}
\label{tab:gradient_alignment_summary}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=0.98\linewidth]{fig_gradient_alignment_phase2b_best.pdf}
\caption{\textbf{Phase-2b best-regime gradient fidelity.} Shunting models show substantially better local-vs-backprop directional alignment (higher weighted cosine) and much lower gradient-scale distortion than additive controls on both MNIST and context-gating best configurations.}
\label{fig:gradient_alignment_components}
\end{figure}

\paragraph{Limitations and future work.}
We view these results as evidence for a mechanism (shunting-linked credit stabilization) rather than a complete biological account. Remaining extensions include scaling to deeper architectures and vision benchmarks beyond flattened inputs, testing on reconstructed morphologies, and connecting the discrete-time rules to event-driven spiking implementations (Appendix~\ref{app:eligibility}).

\section{Related Work}

\paragraph{Dendritic credit assignment.}
Dendritic trees support nonlinear subunit computation and can be interpreted as a multi-stage function approximator \cite{poiarazi2003pyramidal,london2005dendritic}. Several proposals exploit dendritic compartmentalization for biologically plausible learning. Urbanczik \& Senn~\cite{urbanczik2014dendritic} derive a single-neuron learning rule from dendritic prediction of somatic spiking. Guerguiev et al.~\cite{guerguiev2017segregated} propose segregated dendrites where apical compartments carry teaching signals and basal compartments carry feedforward input, enabling approximate deep learning. Sacramento et al.~\cite{sacramento2018dendritic} show that cortical microcircuits with dendritic predictions can approximate backprop if apical and basal inputs converge with appropriate timing. Our work differs in two key ways: (i)~we derive exact gradients for a conductance-based compartment model (not an abstract compartmental surrogate), and (ii)~we identify shunting inhibition as a critical enabler of local gradient quality via quantitative gradient-fidelity analysis.

\paragraph{Feedback alignment and variants.}
Lillicrap et al.~\cite{lillicrap2016random} showed that random fixed feedback weights suffice for learning in deep networks (feedback alignment, FA), and N{\o}kland~\cite{nokland2016dfa} extended this to direct feedback alignment (DFA) where output errors project directly to each layer. Our broadcast error modes (Section~\ref{sec:broadcast}) generalize this idea to the dendritic setting. The key difference is that our local rules also exploit conductance-based local signals (driving force, input resistance) that are unavailable to standard FA/DFA.

\paragraph{Other biologically plausible methods.}
Equilibrium propagation \cite{scellier2017equilibrium} computes exact gradients in energy-based networks by contrasting free and clamped phases. Predictive coding networks \cite{whittington2019theories} perform inference via local prediction errors that converge to backprop gradients at equilibrium. Target propagation methods use local targets rather than error gradients. These approaches solve the weight transport problem through different mechanisms; our contribution is orthogonal, showing that dendritic biophysics provides yet another route to local credit assignment.

\paragraph{Divisive normalization and shunting.}
Shunting inhibition provides divisive normalization of neural responses \cite{carandini2012normalization}, though its effect on firing rates can be subtractive in certain regimes \cite{holt1997shunting}. Inhibitory plasticity and E/I balance are also implicated in stabilizing cortical dynamics \cite{vogels2011inhibitory}. We show that this divisive interaction has a benefit for learning: it creates regimes where local gradient approximations are substantially more faithful to exact gradients. Homeostatic mechanisms including synaptic scaling \cite{turrigiano2008homeostatic} and apical/basal plasticity differences \cite{larkum2013apical} motivate our morphology-aware extensions.

\section{Conclusion}

We have shown that compartmental dendritic networks with shunting inhibition create a favorable regime for local credit assignment. Starting from conductance-based dendritic equations, we derived exact backpropagation gradients for dendritic trees and constructed a principled hierarchy of local approximations (3F/4F/5F) that use only synapse-local quantities plus a broadcast error signal. Our central empirical finding is that \emph{shunting inhibition is the key architectural enabler}: it provides divisive normalization that dramatically improves the directional alignment and scale fidelity of local gradients relative to exact backpropagation. The best local rule (5F with per-soma broadcast) closes much of the gap to backpropagation on standard benchmarks, and the gradient-fidelity diagnostic provides a new tool for understanding \emph{why} certain architectures support local learning better than others.

\begin{thebibliography}{99}

\bibitem{koch1999biophysics}
Koch, C. (1999).
\emph{Biophysics of Computation: Information Processing in Single Neurons}.
Oxford University Press.

\bibitem{dayan2001theoretical}
Dayan, P., \& Abbott, L. F. (2001).
\emph{Theoretical Neuroscience}.
MIT Press.

\bibitem{poiarazi2003pyramidal}
Poirazi, P., Brannon, T., \& Mel, B. W. (2003).
Pyramidal neuron as two-layer neural network.
\emph{Neuron}, 37(6), 989--999.

\bibitem{london2005dendritic}
London, M., \& H{\"a}usser, M. (2005).
Dendritic computation.
\emph{Annual Review of Neuroscience}, 28, 503--532.

\bibitem{urbanczik2014dendritic}
Urbanczik, R., \& Senn, W. (2014).
Learning by the dendritic prediction of somatic spiking.
\emph{Neuron}, 81(3), 521--528.

\bibitem{carandini2012normalization}
Carandini, M., \& Heeger, D. J. (2012).
Normalization as a canonical neural computation.
\emph{Nature Reviews Neuroscience}, 13(1), 51--62.

\bibitem{holt1997shunting}
Holt, G. R., \& Koch, C. (1997).
Shunting inhibition does not have a divisive effect on firing rates.
\emph{Neural Computation}, 9(5), 1001--1013.

\bibitem{vogels2011inhibitory}
Vogels, T. P., Sprekeler, H., Zenke, F., Clopath, C., \& Gerstner, W. (2011).
Inhibitory plasticity balances excitation and inhibition in sensory pathways and memory networks.
\emph{Science}, 334(6062), 1569--1573.

\bibitem{lillicrap2016random}
Lillicrap, T. P., Cownden, D., Tweed, D. B., \& Akerman, C. J. (2016).
Random synaptic feedback weights support error backpropagation for deep learning.
\emph{Nature Communications}, 7, 13276.

\bibitem{nokland2016dfa}
N{\o}kland, A. (2016).
Direct feedback alignment provides learning in deep neural networks.
\emph{Advances in Neural Information Processing Systems}, 29.

\bibitem{guerguiev2017segregated}
Guerguiev, J., Lillicrap, T. P., \& Richards, B. A. (2017).
Towards deep learning with segregated dendrites.
\emph{eLife}, 6, e22901.

\bibitem{sacramento2018dendritic}
Sacramento, J., Costa, R. P., Bengio, Y., \& Senn, W. (2018).
Dendritic cortical microcircuits approximate the backpropagation algorithm.
\emph{Advances in Neural Information Processing Systems}, 31.

\bibitem{whittington2019theories}
Whittington, J. C., \& Bogacz, R. (2019).
Theories of error back-propagation in the brain.
\emph{Trends in Cognitive Sciences}, 23(3), 235--250.

\bibitem{scellier2017equilibrium}
Scellier, B., \& Bengio, Y. (2017).
Equilibrium propagation: Bridging the gap between energy-based models and backpropagation.
\emph{Frontiers in Computational Neuroscience}, 11, 24.

\bibitem{gretton2005hsic}
Gretton, A., Bousquet, O., Smola, A., \& Sch{\"o}lkopf, B. (2005).
Measuring statistical dependence with Hilbert-Schmidt norms.
\emph{International Conference on Algorithmic Learning Theory}, 63--77.

\bibitem{gretton2007hsic}
Gretton, A., Fukumizu, K., Teo, C. H., Song, L., Sch{\"o}lkopf, B., \& Smola, A. J. (2007).
A kernel statistical test of independence.
\emph{Advances in Neural Information Processing Systems}, 20.

\bibitem{fremaux2016three}
Fr{\'e}maux, N., \& Gerstner, W. (2016).
Neuromodulated spike-timing-dependent plasticity, and theory of three-factor learning rules.
\emph{Frontiers in Neural Circuits}, 9, 85.

\bibitem{bellec2020eprop}
Bellec, G., Scherr, F., Subramoney, A., Hajek, E., Salaj, D., Legenstein, R., \& Maass, W. (2020).
A solution to the learning dilemma for recurrent networks of spiking neurons.
\emph{Nature Communications}, 11, 3625.

\bibitem{larkum2013apical}
Larkum, M. (2013).
A cellular mechanism for cortical associations: an organizing principle for the cerebral cortex.
\emph{Trends in Neurosciences}, 36(3), 141--151.

\bibitem{welford1962note}
Welford, B. P. (1962).
Note on a method for calculating corrected sums of squares and products.
\emph{Technometrics}, 4(3), 419--420.

\bibitem{turrigiano2008homeostatic}
Turrigiano, G. G. (2008).
The self-tuning neuron: synaptic scaling of excitatory synapses.
\emph{Cell}, 135(3), 422--435.

\end{thebibliography}

\appendix

\section{Supplementary Results and Figures}
\label{app:extra_results}

\subsection*{Rule-Family Ranking and Broadcast-Mode Dependence}
To make the rule comparison explicit, we aggregate completed Phase-2 and Phase-2b runs on MNIST and context gating and rank 3F/4F/5F under matched local-learning sweeps.

\begin{table}[t]
\centering
\small
\begin{tabular}{@{}llcc@{}}
\toprule
\textbf{Dataset} & \textbf{Rule} & \textbf{Top-10 valid mean} & \textbf{Top-10 test mean} \\
\midrule
MNIST & 3F & 0.611 & 0.622 \\
MNIST & 4F & 0.620 & 0.628 \\
MNIST & 5F & \textbf{0.912} & \textbf{0.916} \\
\midrule
Context gating & 3F & 0.398 & 0.396 \\
Context gating & 4F & 0.411 & 0.411 \\
Context gating & 5F & \textbf{0.807} & \textbf{0.789} \\
\bottomrule
\end{tabular}
\caption{\textbf{Rule-family ranking from completed local-competence sweeps.} Values are averages over the top 10 runs (ranked by validation accuracy) within each dataset and rule family.}
\label{tab:variant_ranking_phase2}
\end{table}

\paragraph{Broadcast-mode interaction.}
Per-soma broadcast strongly outperforms scalar and local-mismatch modes for both datasets (MNIST: $0.918$ vs $0.889$ vs $0.200$ test; context gating: $0.827$ vs $0.699$ vs $0.114$ test), motivating broadcast mode as a primary experimental factor.

\subsection*{Phase-Based Sweep Figures}

\begin{figure*}[t]
\centering
\includegraphics[width=0.94\textwidth]{fig_phase1_capacity_best_core.pdf}
\caption{\textbf{Phase 1 capacity ceilings (standard backprop).} Best standard-test accuracy per dataset and core type.}
\label{fig:phase1_capacity}
\end{figure*}

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{fig_phase2b_gap_closing_hsic_weight.pdf}
\caption{\textbf{HSIC strength and broadcast mode.} For context gating, moderate HSIC weights ($0.01$--$0.1$) improve local learning under per-soma broadcast, while large weights degrade performance. For MNIST, HSIC has negligible effect. Error bars: $\pm 1$ s.d.\ across seeds.}
\label{fig:phase2b_hsic_weight}
\end{figure}

\begin{figure*}[t]
\centering
\includegraphics[width=0.92\textwidth]{fig_decoder_locality.pdf}
\caption{\textbf{Decoder locality.} On both MNIST and CIFAR-10, local decoder updates match backpropagated decoder updates, while frozen decoder weights collapse performance.}
\label{fig:decoder_locality}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=0.94\textwidth]{fig_shunting_regime.pdf}
\caption{\textbf{Regime dependence across inhibition levels (robust sweep).} Shunting networks consistently outperform additive controls across inhibitory synapse counts and broadcast modes.}
\label{fig:shunting_regime}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=0.94\textwidth]{fig_source_interaction.pdf}
\caption{\textbf{Broadcast-path interaction in source analysis (robust sweep).} Within per-soma broadcast, path propagation changes information metrics more than accuracy; within scalar broadcast it improves accuracy more with smaller information gains.}
\label{fig:source_interaction}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=0.94\textwidth]{fig_phase_information_panel.pdf}
\caption{\textbf{Information panel.} Mutual information proxy $I(E,I;C)$ versus test accuracy for shunting (green) and additive (blue) networks, with and without path propagation.}
\label{fig:phase_information_panel}
\end{figure*}

\begin{figure*}[t]
\centering
\begin{subfigure}[t]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{fig_phase3_claimB_morphology_scaling.pdf}
\caption{\textbf{Morphology scaling.} Accuracy as a function of dendritic branching with and without path propagation/modulators.}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{fig_phase3_claimC_error_shaping.pdf}
\caption{\textbf{Error shaping.} Comparison of broadcast modes and decoder update modes on hierarchical/context tasks.}
\end{subfigure}
\caption{\textbf{Phase 3 ablations: morphology and error shaping.}}
\label{fig:phase3_morph_error}
\end{figure*}

\begin{table}[t]
\centering
\small
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Robust Claim / Condition} & \textbf{Metric} & \textbf{Value} \\
\midrule
Claim2 (MNIST, decoder local vs backprop vs none) & test accuracy & 0.3790 vs 0.3788 vs 0.1756 \\
Claim2 (CIFAR-10, decoder local vs backprop vs none) & test accuracy & 0.1669 vs 0.1671 vs 0.1000 \\
Claim3 (MNIST, shunting vs additive; avg matched) & test accuracy delta & +0.210 \\
Claim3 (CIFAR-10, shunting vs additive; avg matched) & test accuracy delta & +0.044 \\
Claim4 (per-soma, path true minus false) & test / \(\mathrm{MI}(E,I;C)\) & -0.0030 / +0.0173 \\
\bottomrule
\end{tabular}
\caption{\textbf{Mechanistic ablation results (controlled architecture).} These experiments use smaller architectures to isolate mechanisms (decoder locality, shunting regime dependence, broadcast/path interaction) rather than maximize absolute performance.}
\label{tab:robust_headline}
\end{table}

\subsection*{Additional Gradient-Fidelity Analyses}

\begin{table}[t]
\centering
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Dataset (shunting)} & \textbf{Cosine (per-soma)} & \textbf{Cosine (scalar)} & \textbf{Scale (per-soma)} & \textbf{Scale (scalar)} \\
\midrule
MNIST & 0.0537 & 0.0536 & 1.6756 & 1.6765 \\
Context gating & 0.1140 & 0.1138 & 0.8303 & 0.8314 \\
\bottomrule
\end{tabular}
\caption{\textbf{Broadcast-mode checkpoint comparison on best Phase-2b runs.} ``Scale'' is \(|\log_{10}(\|g_{\mathrm{local}}\|/\|g_{\mathrm{bp}}\|)|\), parameter-count weighted over component groups.}
\label{tab:gradient_mode_compare_ckpt}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=0.98\linewidth]{fig_gradient_alignment_phase2b_mode_compare.pdf}
\caption{\textbf{Per-soma vs scalar gradient-fidelity on trained Phase-2b checkpoints.} Aggregate directional and scale metrics for shunting and additive cores on MNIST and context gating.}
\label{fig:gradient_alignment_mode_compare}
\end{figure}

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\textwidth]{fig_gradient_fidelity_trajectory.pdf}
\caption{\textbf{Gradient fidelity over training.} (A)~Parameter-count-weighted cosine similarity between local and backprop gradients over training epochs. (B)~Per-component cosine at initialization. (C)~Scale mismatch over training.}
\label{fig:gradient_fidelity_trajectory}
\end{figure*}

\subsection*{Small-Network Sandbox Summary}
\begin{figure*}[t]
\centering
\includegraphics[width=0.92\textwidth]{fig_neurips_combined.pdf}
\caption{\textbf{Controlled small-network sandbox (summary).} A compact view of strategy comparisons, learning dynamics, and gradient-fidelity trends in a matched small-architecture regime used for mechanism isolation.}
\label{fig:neurips_combined}
\end{figure*}

\section{Implementation Details (Appendix)}
\label{app:implementation}

\subsection*{Units and Normalization}
\begin{table}[h]\centering
\begin{tabular}{@{}lll@{}}\toprule
Quantity & Symbol & Typical units (scaled)\\\midrule
Voltage & $V$ & mV (normalized to $[-1,1]$)\\
Synaptic conductance & $g^{\mathrm{syn}}$ & nS (nonnegative)\\
Dendritic conductance & $g^{\mathrm{den}}$ & nS (nonnegative)\\
Leak conductance & $g^{\mathrm{leak}}$ & nS (set to $1$ in normalized units)\\
Input resistance & $R^{\mathrm{tot}}$ & $\mathrm{nS}^{-1}$ (normalized $\le 1$)\\\bottomrule
\end{tabular}
\caption{Units and normalization conventions.}
\label{tab:units}
\end{table}

\subsection*{Positive Parameterization}
To enforce $g \geq 0$, we use a positive parameterization (e.g.\ $g=\exp(\theta)$ or softplus). For $g=\exp(\theta)$, $\frac{\partial L}{\partial \theta} = \frac{\partial L}{\partial g}\cdot g$.

\subsection*{Decoder Update Modes}
Let $W_{\mathrm{dec}}$ map $V_L \to \hat{y}\in\mathbb{R}^{d_{\mathrm{out}}}$. We compare:
\textbf{backprop} ($\nabla_{W_{\mathrm{dec}}}L$ via autograd), \textbf{local} ($\Delta W_{\mathrm{dec}}=\eta\langle \delta_0 V_L^\top\rangle_B$), and \textbf{frozen} ($\Delta W_{\mathrm{dec}}=0$).

\subsection*{Algorithm Sketch}
\begin{algorithm}[H]
\caption{Local Credit Assignment (schematic)}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Model, minibatch $(x,y)$, config $\mathcal{C}$
\STATE Forward pass; compute loss $L$ and output error $\delta^y=\partial L/\partial \hat{y}$
\STATE Compute somatic error $\delta_0 = W_{\mathrm{dec}}^\top \delta^{y}$
\FOR{each layer $n$ (reverse order)}
    \STATE Broadcast error: $e_n = \text{broadcast}(\delta_0, \mathcal{C})$
    \IF{path propagation enabled}
        \STATE Compute $\pi_n$ via Eq.~\ref{eq:path_factor}; set $e_n \leftarrow e_n \cdot \pi_n$
    \ENDIF
    \STATE Compute layer modulators $\rho_n$ (correlation) and $\phi_n$ (conditional predictability)
    \STATE Optionally convert to branch-specific factors (depth modulation, apical/basal scaling)
    \STATE Apply local updates for synaptic and dendritic conductances (3F/4F/5F template)
\ENDFOR
\STATE Clip gradients; optimizer step
\end{algorithmic}
\end{algorithm}

\section{Theoretical Comparison (Appendix)}
\label{app:theory_compare}

\begin{table}[h]
\centering
\begin{tabular}{@{}llcl@{}}
\toprule
\textbf{Method} & \textbf{Factors} & \textbf{Complexity} & \textbf{Best observed regime (current sweeps)} \\
\midrule
3F & $x, (E\!-\!V), e$ & $\mathcal{O}(1)$ & Baseline local plasticity; weak on contextual/hierarchical tasks \\
4F & 3F $+ \rho$ & $\mathcal{O}(1)$ & Better conditioning than 3F; limited performance ceiling \\
5F & 4F $+ \phi$ & $\mathcal{O}(d_n)$ & Strongest overall local competence (MNIST, context gating) \\
\midrule
5F + Path & 5F $+ \pi$ & $\mathcal{O}(L)$ & Strongest impact on representation metrics; selective accuracy gains \\
5F + Depth & 5F, $\rho\!\to\!\rho_j$ & $\mathcal{O}(d_n)$ & Useful in deeper/branched morphologies \\
5F + Norm & 5F + normalization & $\mathcal{O}(d_n)$ & Stabilizes update scale across branches \\
5F + Types & 5F $\times s_j$ & $\mathcal{O}(1)$ & Tests apical/basal specialization hypotheses \\
\bottomrule
\end{tabular}
\caption{Variant taxonomy: computational cost and current empirical regime map ($L$ = depth, $d_n$ = compartments).}
\end{table}

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Component} & \textbf{Biological Analog} & \textbf{Interpretation} \\
\midrule
Conductance scaling $R_n^{\mathrm{tot}}$ & Input resistance & Local sensitivity modulation (Lemma~\ref{lem:convex}) \\
Driving force $(E_j - V_n)$ & Synaptic current & Local gradient factor (Prop.~\ref{prop:grad_gsyn}) \\
Shunting inhibition & Divisive normalization & Sensitivity $\partial V/\partial g_{\mathrm{inh}}\propto -V$ (Sec.~\ref{sec:shunting}) \\
Path factor $\pi_n$ & Cable attenuation & Approximate depth attenuation (Eq.~\ref{eq:path_factor}) \\
Morphology factor $\rho_n$ & Layer relevance & Correlation with output (Eq.~\ref{eq:rho}) \\
Information factor $\phi_n$ & Conditional predictability & Predictability-based amplification (Eq.~\ref{eq:phi}) \\
Dendritic normalization & Homeostasis & Stabilizes branch-scale updates (Appendix~\ref{app:morphology}) \\
Branch-type scaling & Apical vs.\ basal & Differential plasticity across compartments (Appendix~\ref{app:morphology}) \\
Broadcast alignment & Feedback alignment & Descent-direction in expectation (Thm.~\ref{thm:fa_alignment}) \\
\bottomrule
\end{tabular}
\caption{Theoretical components and their biological/algorithmic interpretations.}
\end{table}

\section{Morphology-Aware Extensions (Details)}
\label{app:morphology}

This appendix provides the full definitions for the morphology-aware extensions summarized in the main text.

\paragraph{Path-integrated propagation.}
Exact gradients in dendritic trees involve a path-sum of multiplicative edge factors (Eq.~\ref{eq:path_sum}). Path propagation approximates this depth attenuation by modulating the broadcast error with the recursive path factor $\pi_n$ (Eq.~\ref{eq:path_factor}), implemented as a per-sample scalar broadcast within each layer.

\paragraph{Branch-specific depth modulation.}
Let $d_j$ be the graph distance from the soma to branch $j$. Define per-branch morphology factor $\rho_j = \rho_{\mathrm{base}} / (d_j + \alpha)$, where $\alpha > 0$ prevents singularity. This mirrors cable attenuation: distal synapses receive smaller plasticity updates, with $\|\Delta g_j^{\mathrm{syn}}\| \propto 1/(d_j + \alpha)$.

\paragraph{Dendritic normalization.}
Normalize dendritic updates by total branch conductance: $\Delta g_j^{\mathrm{den}} \leftarrow \Delta g_j^{\mathrm{den}} / (\sum_{k} g_k^{\mathrm{den}} + \varepsilon)$. This reduces update variance when total conductance $G_n$ is large, analogous to homeostatic synaptic scaling \cite{turrigiano2008homeostatic}.

\paragraph{Apical vs basal branch differentiation.}
Assign each branch a type flag $t_j \in \{0, 1\}$ (basal, apical) with type-specific scales $s_j = s_{\mathrm{basal}} + t_j (s_{\mathrm{apical}} - s_{\mathrm{basal}})$. Setting $s_{\mathrm{apical}} > s_{\mathrm{basal}}$ amplifies top-down learning, consistent with distinct plasticity rules in apical vs.\ basal dendrites of pyramidal neurons \cite{larkum2013apical}.

\section{HSIC Auxiliary Objectives (Details)}
\label{app:hsic}

For layer activations $\mathbf{Z} \in \mathbb{R}^{B \times d_n}$ with kernel matrix $\mathbf{K}_Z$ and centering matrix $\mathbf{H} = \mathbf{I} - \frac{1}{B}\mathbf{1}\mathbf{1}^\top$, the HSIC losses are:
\begin{align}
\mathcal{L}_{\mathrm{HSIC}}^{\mathrm{self}} &= \frac{1}{B^2} \tr(\mathbf{K}_Z \mathbf{H} \mathbf{K}_Z \mathbf{H}) & \text{(self-decorrelation)} \\
\mathcal{L}_{\mathrm{HSIC}}^{\mathrm{target}} &= -\frac{1}{B^2} \tr(\mathbf{K}_Z \mathbf{H} \mathbf{K}_Y \mathbf{H}) & \text{(target-correlation)}
\end{align}
For linear kernel $\mathbf{K}_Z = \mathbf{Z}\mathbf{Z}^\top$, the gradients are $\partial \mathcal{L}^{\mathrm{self}} / \partial \mathbf{Z} = \frac{4}{B^2} \mathbf{H} \mathbf{K}_Z \mathbf{H} \mathbf{Z}$ and $\partial \mathcal{L}^{\mathrm{target}} / \partial \mathbf{Z} = -\frac{4}{B^2} \mathbf{H} \mathbf{K}_Y \mathbf{H} \mathbf{Z}$. These are added to synaptic eligibility traces via the chain rule.

\section{Online Variant with Eligibility Traces}
\label{app:eligibility}

Define continuous-time eligibilities per synapse:
$\tau_e \dot{e}_{j}^{\mathrm{syn}}(t) = -e_{j}^{\mathrm{syn}}(t) + x_j(t)\, (E_j - V_n(t))\, R_n^{\mathrm{tot}}(t)$,
and likewise for dendritic connections. With modulatory signal $m_n(t)$:
$\Delta g_j^{\mathrm{syn}} \propto \int e_{j}^{\mathrm{syn}}(t)\, m_n(t)\, \mathrm{d}t$,
which instantiates three-factor learning in continuous time \cite{fremaux2016three,bellec2020eprop}.

\end{document}
