\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}

\geometry{margin=1in}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\tr}{tr}

\title{Local Credit Assignment in Compartmental Dendritic Networks}
\author{}
\date{}

\begin{document}
\maketitle

\begin{abstract}
We present a rigorous mathematical framework for local credit assignment in compartmental dendritic networks. Starting from the passive cable equation and deriving exact backpropagation gradients for general dendritic trees, we introduce local approximations that use only signals available at each synapse. We formulate three classes of learning rules—3-factor, 4-factor, and 5-factor—and extend them with four morphology-aware mechanisms that explicitly incorporate dendritic tree topology: path-integrated propagation, branch-specific depth modulation, dendritic normalization, and apical-basal differentiation. We provide theoretical analysis of shunting inhibition's role in divisive gain control, prove positive expected alignment between local and exact gradients under broadcast error schemes, and establish connections to feedback alignment, predictive coding, and homeostatic plasticity. We prove consistency with the implemented algorithms and provide comprehensive experimental protocols for validation.
\end{abstract}

\section{Compartmental Voltage Model}

\subsection{From the Passive Cable to the Compartment Equation}
\label{sec:cable-derivation}

Starting from the linear passive cable equation for membrane potential $v(x,t)$ relative to rest,
\[
c_m \frac{\partial v}{\partial t} = \frac{1}{r_a}\frac{\partial^2 v}{\partial x^2} - \frac{1}{r_m} v + i_\mathrm{syn}(x,t),
\]
and discretizing a dendritic branch into isopotential compartments with axial conductances, the steady-state ($\partial_t v=0$) yields a nodal balance of conductances and driving forces.\footnote{See Koch's \emph{Biophysics of Computation} and Dayan \& Abbott's \emph{Theoretical Neuroscience} for derivations and assumptions underlying the linear regime.}
Representing synapses as conductances with reversal potentials and siblings as dendritic conductances gives the compartment equation below with unit leak to $E_\mathrm{leak}{=}0$. This clarifies that (i) all inputs contribute via conductances, (ii) total conductance $g_n^{\rm tot}$ controls both input resistance $R_n^{\rm tot}$ and divisive normalization, and (iii) shunting inhibition corresponds to adding conductance with $E_\mathrm{inh} \approx 0$ (Section~\ref{sec:shunting}).

\subsection{Voltage Equation}

Consider compartment $n$ receiving synaptic inputs indexed by $j$ and dendritic inputs from child compartments. Let:
\begin{itemize}
\item $x_j \in \mathbb{R}_+$: presynaptic activity at synapse $j$
\item $E_j \in \mathbb{R}$: reversal potential of synapse $j$ (excitatory: $E_j > 0$; inhibitory: $E_j \leq 0$)
\item $g_j^{\mathrm{syn}} \geq 0$: synaptic conductance (learned parameter)
\item $V_j \in \mathbb{R}$: voltage of child compartment $j$
\item $g_j^{\mathrm{den}} \geq 0$: dendritic conductance from child $j$ (learned parameter)
\end{itemize}

\paragraph{Currents.}
Synaptic current:
\begin{equation}
I_{\mathrm{syn}} = \sum_j (E_j - V_n) x_j g_j^{\mathrm{syn}}
\end{equation}
Dendritic current:
\begin{equation}
I_{\mathrm{den}} = \sum_j (V_j - V_n) g_j^{\mathrm{den}}
\end{equation}

\paragraph{Steady-state voltage.}
With unit leak conductance to reversal potential $0$:
\begin{equation}
V_n = \frac{\sum_j E_j x_j g_j^{\mathrm{syn}} + \sum_j V_j g_j^{\mathrm{den}}}{\sum_j x_j g_j^{\mathrm{syn}} + \sum_j g_j^{\mathrm{den}} + 1}
\label{eq:voltage}
\end{equation}

\paragraph{Total conductance and resistance.}
\begin{equation}
g_n^{\mathrm{tot}} = \sum_j x_j g_j^{\mathrm{syn}} + \sum_j g_j^{\mathrm{den}} + 1, \qquad R_n^{\mathrm{tot}} = \frac{1}{g_n^{\mathrm{tot}}}
\label{eq:conductance}
\end{equation}

\begin{lemma}[Convexity and Bounds]
\label{lem:convex}
Let $\mathcal{S}_n=\{E_j\}_{\text{syn at }n}\cup \{V_j\}_{\text{children}} \cup \{0\}$. Then $V_n$ in \eqref{eq:voltage} is a convex combination of elements of $\mathcal{S}_n$, hence
\[
\min \mathcal{S}_n \ \le\ V_n \ \le\ \max \mathcal{S}_n.
\]
Moreover, $0<R_n^{\mathrm{tot}}\le 1$ and $R_n^{\mathrm{tot}} g_{i}^{\mathrm{den}}<1$ for all $i$.
\end{lemma}
\begin{proof}
Immediate from \eqref{eq:voltage}–\eqref{eq:conductance} since all conductances are nonnegative and leak adds $+1$ to the denominator.
\end{proof}

\subsection{Local Sensitivities}

\begin{proposition}[Synaptic Gradient]
\label{prop:grad_gsyn}
\begin{equation}
\frac{\partial V_n}{\partial g_i^{\mathrm{syn}}} = x_i R_n^{\mathrm{tot}} (E_i - V_n)
\label{eq:grad_gsyn}
\end{equation}
\end{proposition}
\begin{proof}
Apply quotient rule to \eqref{eq:voltage}:
\begin{align*}
\frac{\partial V_n}{\partial g_i^{\mathrm{syn}}} &= \frac{E_i x_i \cdot g_n^{\mathrm{tot}} - \left(\sum_j E_j x_j g_j^{\mathrm{syn}} + \sum_j V_j g_j^{\mathrm{den}}\right) \cdot x_i}{(g_n^{\mathrm{tot}})^2} \\
&= \frac{E_i x_i}{g_n^{\mathrm{tot}}} - \frac{V_n x_i}{g_n^{\mathrm{tot}}} = x_i R_n^{\mathrm{tot}} (E_i - V_n). \qedhere
\end{align*}
\end{proof}

\begin{proposition}[Child Voltage Gradient]
\label{prop:grad_V}
\begin{equation}
\frac{\partial V_n}{\partial V_i} = g_i^{\mathrm{den}} R_n^{\mathrm{tot}}
\label{eq:grad_V}
\end{equation}
\end{proposition}

\begin{proposition}[Dendritic Gradient]
\label{prop:grad_gden}
\begin{equation}
\frac{\partial V_n}{\partial g_i^{\mathrm{den}}} = R_n^{\mathrm{tot}} (V_i - V_n)
\label{eq:grad_gden}
\end{equation}
\end{proposition}

\begin{proposition}[Additional Local Sensitivities]
\label{prop:additional}
\[
\frac{\partial V_n}{\partial x_i} = g_i^{\mathrm{syn}} R_n^{\mathrm{tot}} (E_i - V_n), 
\qquad 
\frac{\partial V_n}{\partial E_i} = x_i g_i^{\mathrm{syn}} R_n^{\mathrm{tot}},
\qquad
\frac{\partial V_n}{\partial g^{\mathrm{leak}}} = - V_n R_n^{\mathrm{tot}}.
\]
\end{proposition}

\begin{remark}
The sensitivity $\partial V_n/\partial g^{\mathrm{leak}} = - V_n R_n^{\mathrm{tot}}$ applies if $g^{\mathrm{leak}}$ is a trainable parameter. In all reported experiments, we fix $g^{\mathrm{leak}}{=}1$ (normalized units).
\end{remark}

\subsection{Shunting Inhibition and Divisive Gain Control}
\label{sec:shunting}

A conductance-based inhibitory synapse with $E_{\mathrm{inh}}\approx E_\mathrm{leak}{=}0$ contributes current $I_{\mathrm{inh}} = (0 - V_n)\, x_j g_j^{\mathrm{syn}}$ and increases $g_n^{\mathrm{tot}}$ in \eqref{eq:conductance}. 

\begin{proposition}[Subthreshold Effect of Shunts]
For a pure shunt ($E_j=0$), the steady-state sensitivity to the inhibitory conductance is
\[
\frac{\partial V_n}{\partial g_j^{\mathrm{syn}}}
= x_j R_n^{\mathrm{tot}} (0 - V_n) = - x_j R_n^{\mathrm{tot}} V_n.
\]
Thus $V_n$ is multiplicatively attenuated (divisive normalization) by increased inhibitory conductance at fixed drives.
\end{proposition}

\begin{remark}[Divisive vs.\ Subtractive at the Firing-Rate Level]
While shunting produces divisive scaling of subthreshold voltages, its net effect on firing rates can be subtractive in many regimes \cite{holt1997shunting}, so we report both voltage- and rate-level analyses in experiments. Normalization via added conductance is consistent with canonical divisive normalization models in cortex \cite{carandini2012normalization}.
\end{remark}

\paragraph{Inhibitory/shunting synapses.}
For an inhibitory synapse with $E_j \approx 0$, the 3F update reduces to
\[
\Delta g_{j,\mathrm{inh}}^{\mathrm{syn}} 
= \eta\, \langle x_j R_n^{\mathrm{tot}} (-V_n)\, e_n \rangle_B,
\]
i.e., anti-Hebbian in $V_n$ and divisive in $g_n^{\mathrm{tot}}$. 
With 4F/5F, multiply by $\rho$ and $\phi$ (Def.~\ref{def:phi_fixed}). Note that the same multiplicative factors are applied to both excitatory and inhibitory synapses in the implementation; the sign difference arises solely from the driving force $(E_j - V_n)$.

\subsection{Loss Propagation}

Let $V_0$ denote the somatic/output compartment. The decoder produces $\hat y = W_{\mathrm{dec}} V_0$ (linear case), and $L$ is the task loss. Define the error gradients:
\begin{equation}
\delta^y := \frac{\partial L}{\partial \hat y}, \qquad
\delta_0 := \frac{\partial L}{\partial V_0} = \left(\frac{\partial \hat y}{\partial V_0}\right)^\top \delta^y
= W_{\mathrm{dec}}^\top \delta^y.
\end{equation}

\begin{theorem}[Backpropagation on a Dendritic Tree]
\label{thm:tree_backprop}
Let the dendritic morphology be a rooted tree with soma/output at node $0$. For any compartment $n$ with parent set $\mathcal{P}(n)$ (typically $|\mathcal{P}(n)|{=}1$), the loss gradient satisfies the recursion
\begin{equation}
\frac{\partial L}{\partial V_n}
=\sum_{p \in \mathcal{P}(n)} \frac{\partial L}{\partial V_p}\, \frac{\partial V_p}{\partial V_n}
=\sum_{p \in \mathcal{P}(n)} \delta_p \, R_p^{\mathrm{tot}}\, g_{n\to p}^{\mathrm{den}},
\qquad \delta_p \equiv \frac{\partial L}{\partial V_p}.
\label{eq:tree_recursion}
\end{equation}
Unrolling the recursion yields a sum over all directed paths $\mathcal{P}: n \leadsto 0$:
\begin{equation}
\frac{\partial L}{\partial V_n} 
= \frac{\partial L}{\partial V_0}
\sum_{\mathcal{P}:n\leadsto 0}
\prod_{(i\to k)\in \mathcal{P}} R_k^{\mathrm{tot}}\, g_{i\to k}^{\mathrm{den}}.
\label{eq:path_sum}
\end{equation}
\end{theorem}
\begin{proof}
Apply the multivariate chain rule on the directed acyclic computation graph defined by the tree; use Proposition~\ref{prop:grad_V}. Each path contributes a product of edge sensitivities. Summing over parents produces \eqref{eq:tree_recursion}; unrolling yields \eqref{eq:path_sum}.
\end{proof}

\begin{corollary}[Chain Case]
\label{cor:chain}
If the morphology is a simple chain $V_0 \leftarrow V_1 \leftarrow \cdots \leftarrow V_n$, \eqref{eq:path_sum} reduces to
\begin{equation}
\frac{\partial L}{\partial V_n} = \frac{\partial L}{\partial V_0} \prod_{i=1}^n R_{i-1}^{\mathrm{tot}} g_i^{\mathrm{den}}.
\label{eq:exact_backprop}
\end{equation}
Defining $g_0^{\mathrm{den}} = 1$ and reindexing:
\begin{equation}
\frac{\partial L}{\partial V_n} = \frac{\partial L}{\partial V_0} \prod_{i=0}^n R_i^{\mathrm{tot}} g_i^{\mathrm{den}}.
\label{eq:exact_backprop_reindex}
\end{equation}
\end{corollary}

\begin{corollary}[Synaptic Parameter Gradient]
\begin{equation}
\frac{\partial L}{\partial g_j^{\mathrm{syn}}} = \frac{\partial L}{\partial V_0} \left(\prod_{i=0}^n R_i^{\mathrm{tot}} g_i^{\mathrm{den}}\right) x_j (E_j - V_n)
\label{eq:exact_gsyn}
\end{equation}
\end{corollary}

\begin{corollary}[Dendritic Parameter Gradient]
\begin{equation}
\frac{\partial L}{\partial g_j^{\mathrm{den}}} = \frac{\partial L}{\partial V_0} \left(\prod_{i=0}^n R_i^{\mathrm{tot}} g_i^{\mathrm{den}}\right) (V_j - V_n)
\label{eq:exact_gden}
\end{equation}
\end{corollary}

\section{Local Learning Approximations}

\subsection{Broadcast Error Approximation}

\begin{definition}[Local Approximation]
Replace the exact gradient $\frac{\partial L}{\partial V_n}$ with a broadcast error signal $e_n$ derived from the output error $\delta_0 = \frac{\partial L}{\partial V_0}$:
\begin{equation}
\frac{\partial L}{\partial V_n} \approx e_n, \qquad \prod_{i=0}^n R_i^{\mathrm{tot}} g_i^{\mathrm{den}} \approx 1
\label{eq:local_approx}
\end{equation}
\end{definition}

Three broadcast modes are implemented (config: \texttt{error\_broadcast\_mode}):

\paragraph{(A) Scalar broadcast.}
For minibatch index $b$:
\begin{equation}
\bar{\delta}(b) = \frac{1}{d_{\mathrm{out}}} \sum_{k=1}^{d_{\mathrm{out}}} \delta_k(b), \qquad e_n(b) = \bar{\delta}(b) \mathbf{1}_{d_n}
\end{equation}

\paragraph{(B) Per-compartment mapping.}
If $d_n = d_{\mathrm{out}}$: $e_n(b) = \delta(b)$. Otherwise, \emph{fallback to scalar broadcast}. An optional DFA-style mode uses a fixed random feedback matrix $B_n \in \mathbb{R}^{d_n \times d_{\mathrm{out}}}$ sampled once at initialization: $e_n(b) = B_n \delta(b)$. This supports testing Theorem~\ref{thm:fa_alignment}.

\paragraph{(C) Local mismatch modulation.}
Let $P_n(b)$ be parent compartment drive (e.g., blocklinear output). Define centered mismatch:
\begin{equation}
\varepsilon_n(b) = \left(P_n(b) - V_n(b)\right) - \frac{1}{B} \sum_{t=1}^B \left(P_n(t) - V_n(t)\right)
\end{equation}
Then:
\begin{equation}
e_n(b) = \bar{\delta}(b) \varepsilon_n(b)
\end{equation}

\subsection{Gradient Alignment with Broadcast Errors}
\label{sec:alignment}

Define the exact synaptic gradient at layer $n$ by $g^{\mathrm{exact}} = \delta_0 \cdot \Xi_n$, where $\Xi_n$ collects local factors and the exact path-sum \eqref{eq:path_sum}. The local 3F gradient with broadcast error $e_n=B_n \delta_0$ is $g^{\mathrm{local}} = e_n \cdot \widehat{\Xi}_n$, where $\widehat{\Xi}_n$ omits the path-sum.

\begin{theorem}[Positive Expected Alignment under Random Broadcast]
\label{thm:fa_alignment}
Let $B_n\in\mathbb{R}^{d_n\times d_{\mathrm{out}}}$ have i.i.d.\ zero-mean entries with $\mathbb{E}[B_n^\top B_n]=\alpha I$. If the decoder aligns with the forward pathway (standard during training), then
\[
\mathbb{E}\big[\cos\angle(g^{\mathrm{local}},g^{\mathrm{exact}})\big] \ \ge\ c_n>0,
\]
where $c_n$ depends on $\alpha$ and the average correlation between $\widehat{\Xi}_n$ and $\Xi_n$. Thus $g^{\mathrm{local}}$ provides a descent direction in expectation.
\end{theorem}
\begin{proof}[Sketch]
Adapt the feedback-alignment argument \cite{lillicrap2016random,nokland2016dfa}: fixed random feedback suffices for alignment as forward weights adapt. Here, $\widehat{\Xi}_n$ is proportional to $\Xi_n$ up to the missing path factor; Jensen bounds on \eqref{eq:path_sum} yield $c_n>0$.
\end{proof}

\subsection{Three-Factor Rule (3F)}

\begin{definition}[3F Learning Rule]
For synaptic conductances:
\begin{equation}
\Delta g_j^{\mathrm{syn}} = \eta \left\langle x_j R_n^{\mathrm{tot}} (E_j - V_n) e_n \right\rangle_B
\label{eq:3f_syn}
\end{equation}
For dendritic conductances:
\begin{equation}
\Delta g_j^{\mathrm{den}} = \eta \left\langle R_n^{\mathrm{tot}} (V_j - V_n) e_n \right\rangle_B
\label{eq:3f_den}
\end{equation}
where $\langle \cdot \rangle_B$ denotes batch average.
\end{definition}

\begin{remark}
The three factors are: (1) presynaptic activity $x_j$ or voltage difference $(V_j - V_n)$, (2) postsynaptic modulation $(E_j - V_n)$ or $R_n^{\mathrm{tot}}$, (3) broadcast error $e_n$.
\\[2pt]
\textbf{Symmetry note.} In implementation, the same multiplicative factors (conductance scaling $R_n^{\mathrm{tot}}$, morphology $\rho$, information $\phi$, and branch scaling $s_j$) are applied consistently to both excitatory and inhibitory synapses; inhibitory only differs in the driving force sign (shunting) via $-(V_n)$ when using driving-force mode.
\end{remark}

\subsection{Four-Factor Rule (4F): Morphology Correlation}

\begin{definition}[Morphology Factor]
Let $\bar{V}_n = \frac{1}{d_n} \sum_{j=1}^{d_n} V_{n,j}$ be the mean voltage over compartments in layer $n$. Define the correlation with output:
\begin{equation}
\rho_n = \frac{\Cov(\bar{V}_n, \bar{V}_0)}{\sqrt{\Var(\bar{V}_n) \Var(\bar{V}_0)} + \varepsilon}
\label{eq:rho}
\end{equation}
\end{definition}

\begin{proposition}[4F Update Rule]
Multiply 3F updates by $\rho_n$:
\begin{align}
\Delta g_j^{\mathrm{syn}} &= \eta \rho_n \left\langle x_j R_n^{\mathrm{tot}} (E_j - V_n) e_n \right\rangle_B \label{eq:4f_syn} \\
\Delta g_j^{\mathrm{den}} &= \eta \rho_n \left\langle R_n^{\mathrm{tot}} (V_j - V_n) e_n \right\rangle_B \label{eq:4f_den}
\end{align}
\end{proposition}

\begin{theorem}[Theoretical Justification]
Let $L$ be a smooth loss. Under the assumption that layer $n$ contributes to the output primarily through its mean activity, the correlation $\rho_n$ approximates the alignment between local voltage fluctuations and output gradients:
\begin{equation}
\mathbb{E}\left[\frac{\partial L}{\partial \bar{V}_n} \cdot \bar{V}_n\right] \propto \rho_n \cdot \Var(\bar{V}_n)
\end{equation}
Thus $\rho_n$ weights updates by the layer's relevance to the task.
\end{theorem}

\paragraph{Estimators (EMA / online).}
For minibatches $B\ge 2$, estimate $\rho_n$ from \eqref{eq:rho} with an EMA over batches.
For $B=1$ (online), maintain means $\mu_x,\mu_y$, variances $\sigma_x^2,\sigma_y^2$, and covariance $C_{xy}$ for
$x_t=\bar V_0^{(t)}$ and $y_t=\bar V_n^{(t)}$ using Welford's numerically stable algorithm \cite{welford1962note}:
\begin{align}
\mu_x^{(t)} &= (1-\alpha)\mu_x^{(t-1)} + \alpha x_t, &
\mu_y^{(t)} &= (1-\alpha)\mu_y^{(t-1)} + \alpha y_t,\nonumber\\
\delta_x &= x_t - \mu_x^{(t-1)}, &
\delta_y &= y_t - \mu_y^{(t-1)},\nonumber\\
\sigma_x^{2(t)} &= (1-\alpha)\sigma_x^{2(t-1)} + \alpha\, \delta_x^2, &
\sigma_y^{2(t)} &= (1-\alpha)\sigma_y^{2(t-1)} + \alpha\, \delta_y^2,\nonumber\\
C_{xy}^{(t)} &= (1-\alpha)C_{xy}^{(t-1)} + \alpha\, \delta_x \delta_y.
\end{align}
Then $\rho_n^{(t)} = \frac{C_{xy}^{(t)}}{\sqrt{\sigma_x^{2(t)}\sigma_y^{2(t)}}+\varepsilon}$, where $\alpha$ is the EMA rate (\texttt{ema\_alpha}).

\subsection{Five-Factor Rule (5F): Conditional Information}

\begin{definition}[Conditional Predictability Factor]
\label{def:phi_fixed}
Let $P_n$ be parent compartment voltage. Define the conditional variance via ridge regression:
\begin{align}
\beta_n &= \frac{\Cov(V_n, P_n)}{\Var(P_n) + \lambda} \label{eq:beta} \\
\sigma_{\mathrm{res}}^2 &= \Var(V_n) - \beta_n \Cov(V_n, P_n) \label{eq:residual_var}
\end{align}
The information proxy is:
\begin{equation}
\phi_n = \frac{\Var(V_n)}{\sigma_{\mathrm{res}}^2 + \varepsilon} = \frac{1}{1 - R_n^2} \geq 1,
\label{eq:phi}
\end{equation}
where $R_n^2=\frac{\beta_n\,\Cov(V_n,P_n)}{\Var(V_n)}$ is the (ridge) coefficient of determination.
\end{definition}

\begin{remark}[Information-Theoretic Interpretation and Implementation]
$\phi_n$ increases when $V_n$ is \emph{more} predictable from its parent $P_n$ (higher $R^2$), providing an amplification factor. This formulation $\phi_n = 1/(1-R^2)$ is used in the current implementation (\texttt{\_compute\_layer\_phi\_conditional}, line 1261) and is clamped to $[0.25, 4.0]$ for stability. 

\emph{Alternative formulation}: To emphasize \emph{unique} variance (information beyond the parent), one could instead use $\phi_n = 1 - R^2 \in (0,1]$, which decreases when $V_n$ is predictable from $P_n$. The current implementation uses the inverse formulation, treating high predictability as indicative of strong signal propagation through the parent pathway. Both interpretations are valid depending on the desired emphasis: the current form amplifies well-predicted compartments (coherent signal flow), while the alternative would amplify compartments with unique information.
\end{remark}

\begin{proposition}[5F Update Rule]
\begin{align}
\Delta g_j^{\mathrm{syn}} &= \eta \rho_n \phi_n \left\langle x_j R_n^{\mathrm{tot}} (E_j - V_n) e_n \right\rangle_B \label{eq:5f_syn} \\
\Delta g_j^{\mathrm{den}} &= \eta \rho_n \phi_n \left\langle R_n^{\mathrm{tot}} (V_j - V_n) e_n \right\rangle_B \label{eq:5f_den}
\end{align}
\end{proposition}

\section{Morphology-Aware Extensions}

Standard 4F/5F rules use layer-wise factors $\rho_n$, $\phi_n$ that ignore branch-specific topology. We introduce four extensions that explicitly incorporate dendritic tree structure.

\subsection{Path-Integrated Propagation}

\begin{definition}[Path Factor]
Define recursively:
\begin{equation}
\pi_n = \begin{cases}
1 & n = 0 \\
\pi_{n-1} \cdot R_{n-1}^{\mathrm{tot}} \cdot \bar{g}_{n-1}^{\mathrm{den}} & n \geq 1
\end{cases}
\label{eq:path_factor}
\end{equation}
where $\bar{g}_{n-1}^{\mathrm{den}}$ is the mean dendritic conductance from layer $n-1$ to $n$.
\end{definition}

\begin{proposition}[Exact for Chains]
\label{prop:path_exact_chain}
For a chain morphology (single path), the path factor \eqref{eq:path_factor} satisfies
\begin{equation}
\pi_n = \prod_{i=0}^{n-1} R_i^{\mathrm{tot}} g_i^{\mathrm{den}}
\end{equation}
and thus exactly matches \eqref{eq:path_sum}.
\end{proposition}

\begin{remark}[Sandwich bounds for trees]
\label{rem:path_tree_bounds}
For a tree, let $\mathcal{P}$ be the set of directed paths from $n$ to $0$ and define, at each depth $d$,
$m_d := \min_{\mathcal{P}} a_{d,\mathcal{P}}$ and $M_d := \max_{\mathcal{P}} a_{d,\mathcal{P}}$, where
$a_{d,\mathcal{P}}$ is the edge factor $R_k^{\mathrm{tot}} g_{i\to k}^{\mathrm{den}}$ at depth $d$ along path $\mathcal{P}$.
Then
\[
|\mathcal{P}| \prod_{d} m_d
\ \le\ 
\sum_{\mathcal{P}} \prod_{d} a_{d,\mathcal{P}}
\ \le\
|\mathcal{P}| \prod_{d} M_d.
\]
If per-depth factors are narrowly concentrated ($m_d\!\approx\!M_d$), replacing the sum by a product of per-depth means (our $\pi_n$) is accurate. Modulating the error by $\pi_n$ yields $e_n^{\mathrm{path}} = e_n \cdot \pi_n$, which better approximates exact backpropagation.
\end{remark}

\begin{corollary}[Depth Attenuation]
From Lemma~\ref{lem:convex}, $R_k^{\mathrm{tot}} g^{\mathrm{den}}_{i\to k} < 1$. Therefore any product $\prod Rg$ in \eqref{eq:path_sum} decays exponentially with depth, motivating path-based error attenuation.
\end{corollary}

\begin{remark}[Implementation]
Computed via \texttt{\_compute\_path\_propagation\_factor()} and implemented as a \emph{per-sample scalar} path factor $\pi_n(b) \in \mathbb{R}$ (broadcast to all compartments in layer $n$). The factor $\bar{g}_{n-1}^{\mathrm{den}}$ is the \emph{arithmetic mean} over outgoing dendritic connections at depth $n-1$. This stabilizes shapes across layers and matches the code behavior when applying $e_n \leftarrow e_n \cdot \pi_n$.
\end{remark}

\paragraph{Theoretical effect.}
Path propagation introduces depth-dependent attenuation: deeper compartments receive exponentially smaller error signals $e_n \sim \prod R_i g_i$. This encourages specialization: shallow layers learn direct input-output mappings, while deep layers integrate over longer paths. In practice, we use a \emph{per-sample scalar} path factor $\pi_n(b)$ for stability and consistent broadcasting across layers with different widths.

\subsection{Branch-Specific Depth Modulation}

\begin{definition}[Depth-Modulated Morphology Factor]
Let $d_j$ be the graph distance (number of edges) from the soma to branch $j$. Define per-branch morphology:
\begin{equation}
\rho_j = \frac{\rho_{\mathrm{base}}}{d_j + \alpha}
\label{eq:rho_depth}
\end{equation}
where $\alpha > 0$ (\texttt{morphology\_depth\_offset}) prevents singularity.
\end{definition}

\begin{proposition}[Depth-Modulated Updates]
Replace scalar $\rho_n$ with tensor $\bm{\rho}_n \in \mathbb{R}^{d_n}$ in updates:
\begin{equation}
\Delta g_j^{\mathrm{syn}} = \eta \rho_j \phi_n \left\langle x_j R_n^{\mathrm{tot}} (E_j - V_n) e_n \right\rangle_B
\end{equation}
\end{proposition}

\begin{theorem}[Biological Motivation]
In real dendrites, distal synapses (large $d_j$) contribute less to somatic depolarization due to cable attenuation. The scaling $\rho_j \propto 1/d_j$ mirrors this: deeper branches receive smaller plasticity updates.
\end{theorem}

\paragraph{Theoretical effect.}
Depth modulation biases learning toward proximal synapses. For fixed error $e_n$, the gradient magnitude is:
\begin{equation}
\left\|\Delta g_j^{\mathrm{syn}}\right\| \propto \frac{1}{d_j + \alpha}
\end{equation}
Tuning $\alpha$ controls the depth penalty: small $\alpha$ → strong penalty, large $\alpha$ → mild penalty.

\subsection{Dendritic Normalization}

\begin{definition}[Conductance Normalization]
For dendritic updates, normalize by total branch conductance:
\begin{equation}
\Delta g_j^{\mathrm{den}} \leftarrow \frac{\Delta g_j^{\mathrm{den}}}{\sum_{k=1}^{K_n} g_k^{\mathrm{den}} + \varepsilon}
\label{eq:dendritic_norm}
\end{equation}
where $K_n$ is the number of dendritic inputs to compartment $n$.
\end{definition}

\begin{theorem}[Variance Stabilization]
Let $G_n = \sum_k g_k^{\mathrm{den}}$ and assume $\Delta g_k \sim \mathcal{N}(0, \sigma^2)$. Without normalization:
\begin{equation}
\Var\left(\sum_k \Delta g_k\right) = K_n \sigma^2
\end{equation}
With normalization:
\begin{equation}
\Var\left(\sum_k \frac{\Delta g_k}{G_n}\right) = \frac{K_n \sigma^2}{G_n^2}
\end{equation}
Thus normalization reduces variance when $G_n$ is large, preventing dominant branches from accumulating unbounded updates.
\end{theorem}

\paragraph{Theoretical effect.}
Analogous to batch normalization, dendritic normalization balances contributions across branches. In sparse connectivity (e.g., TopK synapses), some branches may have much higher $G_n$ than others. Normalization equalizes their influence on the compartment voltage. This mechanism is consistent with homeostatic synaptic scaling observed in biology \cite{turrigiano2008homeostatic}, where neurons adjust synaptic strengths to maintain stable activity levels, and relates to Oja-style stability rules that prevent unbounded weight growth.

\subsection{Apical vs Basal Branch Differentiation}

\begin{definition}[Branch Type Scaling]
Assign each branch $j$ a type flag $t_j \in \{0, 1\}$ (0 = basal, 1 = apical). Define type-specific scales:
\begin{equation}
s_j = s_{\mathrm{basal}} + t_j (s_{\mathrm{apical}} - s_{\mathrm{basal}})
\label{eq:branch_scale}
\end{equation}
Apply to all update factors:
\begin{equation}
\Delta g_j^{\mathrm{syn}} = \eta s_j \rho_j \phi_n \left\langle x_j R_n^{\mathrm{tot}} (E_j - V_n) e_n \right\rangle_B
\end{equation}
\end{definition}

\begin{theorem}[Compartmental Specialization]
Pyramidal neurons exhibit distinct plasticity rules in apical (layer 1, feedback) vs basal (layer 5, feedforward) dendrites. Setting $s_{\mathrm{apical}} > s_{\mathrm{basal}}$ amplifies top-down learning, while $s_{\mathrm{apical}} < s_{\mathrm{basal}}$ emphasizes bottom-up processing.
\end{theorem}

\paragraph{Theoretical effect.}
For hierarchical tasks, apical amplification ($s_{\mathrm{apical}} = 1.5$, $s_{\mathrm{basal}} = 1.0$) allows the network to prioritize contextual modulation. The gradient ratio is:
\begin{equation}
\frac{\|\Delta g_{\mathrm{apical}}\|}{\|\Delta g_{\mathrm{basal}}\|} = \frac{s_{\mathrm{apical}}}{s_{\mathrm{basal}}}
\end{equation}

\section{Auxiliary Objectives: HSIC}

\subsection{Hilbert-Schmidt Independence Criterion}

For layer activations $\mathbf{Z} \in \mathbb{R}^{B \times d_n}$, define kernel matrix $\mathbf{K}_Z$ (linear, RBF, or polynomial) and centering matrix $\mathbf{H} = \mathbf{I} - \frac{1}{B}\mathbf{1}\mathbf{1}^\top$.

\begin{definition}[HSIC Loss]
Self-decorrelation (maximize independence within layer):
\begin{equation}
\mathcal{L}_{\mathrm{HSIC}}^{\mathrm{self}} = \frac{1}{B^2} \tr(\mathbf{K}_Z \mathbf{H} \mathbf{K}_Z \mathbf{H})
\end{equation}
Target-correlation (align with labels $\mathbf{Y}$):
\begin{equation}
\mathcal{L}_{\mathrm{HSIC}}^{\mathrm{target}} = -\frac{1}{B^2} \tr(\mathbf{K}_Z \mathbf{H} \mathbf{K}_Y \mathbf{H})
\end{equation}
\end{definition}

\begin{proposition}[Linear Kernel Gradients]
For $\mathbf{K}_Z = \mathbf{Z}\mathbf{Z}^\top$:
\begin{align}
\frac{\partial \mathcal{L}_{\mathrm{HSIC}}^{\mathrm{self}}}{\partial \mathbf{Z}} &= \frac{4}{B^2} \mathbf{H} \mathbf{K}_Z \mathbf{H} \mathbf{Z} \\
\frac{\partial \mathcal{L}_{\mathrm{HSIC}}^{\mathrm{target}}}{\partial \mathbf{Z}} &= -\frac{4}{B^2} \mathbf{H} \mathbf{K}_Y \mathbf{H} \mathbf{Z}
\end{align}
\end{proposition}

Gradients are added to synaptic eligibility traces via chain rule through $\mathbf{Z} = f(\mathbf{g}^{\mathrm{syn}})$.

\begin{remark}
The expression for $\partial \mathcal{L}_{\mathrm{HSIC}}^{\mathrm{target}}/\partial \mathbf{Z}$ assumes $\mathbf{K}_Y$ is symmetric (true for standard kernels).
\end{remark}

\section{Implementation Details}

\subsection{Units and Normalization}

\begin{table}[h]\centering
\begin{tabular}{@{}lll@{}}\toprule
Quantity & Symbol & Typical units (scaled)\\\midrule
Voltage & $V$ & mV (normalized to $[-1,1]$)\\
Synaptic conductance & $g^{\mathrm{syn}}$ & nS (nonnegative)\\
Dendritic conductance & $g^{\mathrm{den}}$ & nS (nonnegative)\\
Leak conductance & $g^{\mathrm{leak}}$ & nS (set to $1$ in normalized units)\\
Input resistance & $R^{\mathrm{tot}}$ & $\mathrm{nS}^{-1}$ (normalized $\le 1$)\\\bottomrule
\end{tabular}
\caption{Units and normalization conventions.}
\label{tab:units}
\end{table}

\subsection{Positive Weight Parameterization}

To enforce $g \geq 0$, we can use exponential:
\begin{equation}
g = \exp(\theta), \quad \theta \in \mathbb{R}
\end{equation}
Chain rule for gradients:
\begin{equation}
\frac{\partial L}{\partial \theta} = \frac{\partial L}{\partial g} \cdot g
\end{equation}
Alternatively, use softplus $g = \log(1+\exp(\theta))$ to avoid extreme gradients.

\subsection{Online Variant with Eligibility Traces}
Define continuous-time eligibilities per synapse:
\[
\tau_e \dot{e}_{j}^{\mathrm{syn}}(t) = -e_{j}^{\mathrm{syn}}(t) + x_j(t)\, (E_j - V_n(t))\, R_n^{\mathrm{tot}}(t),
\]
and likewise for dendritic connections with $(V_j(t)-V_n(t))\,R_n^{\mathrm{tot}}(t)$. Let the modulatory/error signal be $m_n(t)$ (e.g., broadcast from output or neuromodulatory). Then
\[
\Delta g_j^{\mathrm{syn}} \propto \int e_{j}^{\mathrm{syn}}(t)\, m_n(t)\, \mathrm{d}t, 
\qquad 
\Delta g_j^{\mathrm{den}} \propto \int e_{j}^{\mathrm{den}}(t)\, m_n(t)\, \mathrm{d}t,
\]
which instantiates three-factor learning in continuous time \cite{fremaux2016three,bellec2020eprop}.

\subsection{Decoder Update Modes}

Let $W_{\mathrm{dec}}$ map $V_L \to y \in \mathbb{R}^{d_{\mathrm{out}}}$. Three modes:
\begin{enumerate}
\item \textbf{Backprop}: $\nabla_{W_{\mathrm{dec}}} L$ via autograd.
\item \textbf{Local}: $\Delta W_{\mathrm{dec}} = \eta \langle \delta_0 V_L^\top \rangle_B$ (3-factor).
\item \textbf{Frozen}: $\Delta W_{\mathrm{dec}} = 0$.
\end{enumerate}

\subsection{Algorithm Summary}

\begin{algorithm}[H]
\caption{Local Credit Assignment with Morphology-Aware Extensions}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Model, minibatch $(x, y)$, config $\mathcal{C}$
\STATE Forward pass: $\hat{y} = f(x; \mathbf{g}^{\mathrm{syn}}, \mathbf{g}^{\mathrm{den}})$
\STATE Compute loss $L$ and output error $\delta^{y} = \frac{\partial L}{\partial \hat{y}}$
\STATE Compute somatic error $\delta_0 = W_{\mathrm{dec}}^\top \delta^{y}$
\FOR{each layer $n$ (reverse order)}
    \STATE Broadcast error: $e_n = \text{broadcast}(\delta_0, \mathcal{C})$
    \IF{path propagation enabled}
        \STATE Compute $\pi_n$ via \eqref{eq:path_factor}; $e_n \leftarrow e_n \cdot \pi_n$
    \ENDIF
    \STATE Compute $\rho_n$ via \eqref{eq:rho}
    \IF{depth modulation enabled}
        \STATE $\rho_n \leftarrow [\rho_1, \ldots, \rho_{d_n}]$ via \eqref{eq:rho_depth}
    \ENDIF
    \STATE Compute $\phi_n$ via \eqref{eq:phi}
    \STATE Compute branch scales $s_j$ via \eqref{eq:branch_scale}
    \STATE Synaptic updates: $\Delta g_j^{\mathrm{syn}} = \eta s_j \rho_j \phi_n \langle x_j R_n^{\mathrm{tot}} (E_j - V_n) e_n \rangle_B$
    \STATE Dendritic updates: $\Delta g_j^{\mathrm{den}} = \eta s_j \rho_j \phi_n \langle R_n^{\mathrm{tot}} (V_j - V_n) e_n \rangle_B$
    \IF{dendritic normalization enabled}
        \STATE $\Delta g_j^{\mathrm{den}} \leftarrow \Delta g_j^{\mathrm{den}} / (G_n + \varepsilon)$ via \eqref{eq:dendritic_norm}
    \ENDIF
    \IF{HSIC enabled}
        \STATE Add HSIC gradients to $\Delta g_j^{\mathrm{syn}}$
    \ENDIF
\ENDFOR
\STATE Clip gradients; optimizer step
\end{algorithmic}
\end{algorithm}

\section{Configuration Reference}

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Section} & \textbf{Key Parameters} \\
\midrule
Core & \texttt{rule\_variant}, \texttt{error\_mode}, \texttt{error\_broadcast\_mode} \\
3F (\texttt{three\_factor}) & \texttt{use\_conductance\_scaling}, \texttt{use\_driving\_force}, $\theta$, $E^{\mathrm{rev}}$ \\
4F (\texttt{four\_factor}) & \texttt{rho\_mode}, \texttt{rho\_estimator}, \texttt{ema\_alpha}, \texttt{layer\_wise\_rho\_scale} \\
5F (\texttt{five\_factor}) & \texttt{phi\_mode}, \texttt{phi\_estimator}, \texttt{phi\_ridge\_lambda}, \texttt{layer\_wise\_phi\_scale} \\
Morphology-aware & \texttt{use\_path\_propagation}, \texttt{morphology\_modulator\_mode}, \texttt{morphology\_depth\_offset}, \texttt{use\_dendritic\_normalization}, \texttt{use\_branch\_type\_rules} \\
HSIC (\texttt{hsic}) & \texttt{enabled}, \texttt{weight}, \texttt{self\_weight}, \texttt{target\_weight}, \texttt{kernel}, \texttt{sigma} \\
\bottomrule
\end{tabular}
\caption{Configuration grouped by learning rule sections (see \texttt{LocalRuleConfig}).}
\end{table}

\section{Theoretical Comparison}

\begin{table}[h]
\centering
\begin{tabular}{@{}llcc@{}}
\toprule
\textbf{Method} & \textbf{Factors} & \textbf{Topology} & \textbf{Complexity} \\
\midrule
3F & $x, (E-V), e$ & Layer-wise & $\mathcal{O}(1)$ \\
4F & 3F $+ \rho$ & Layer-wise & $\mathcal{O}(1)$ \\
5F & 4F $+ \phi$ & Layer-wise & $\mathcal{O}(d_n)$ \\
\midrule
5F + Path & 5F $+ \pi$ & Path-aware & $\mathcal{O}(L)$ \\
5F + Depth & 5F, $\rho \to \rho_j$ & Branch-aware & $\mathcal{O}(d_n)$ \\
5F + Norm & 5F + normalization & Branch-aware & $\mathcal{O}(d_n)$ \\
5F + Types & 5F $\times s_j$ & Compartment-aware & $\mathcal{O}(1)$ \\
\bottomrule
\end{tabular}
\caption{Computational complexity per update ($L$ = depth, $d_n$ = compartments).}
\end{table}

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Component} & \textbf{Biological Analog} & \textbf{Key Result} \\
\midrule
Conductance scaling $R_n^{\mathrm{tot}}$ & Input resistance & Lemma~\ref{lem:convex}: $0 < R_n^{\mathrm{tot}} \le 1$ \\
Driving force $(E_j - V_n)$ & Synaptic current & Prop.~\ref{prop:grad_gsyn}: Local sensitivity \\
Shunting inhibition & Divisive normalization & Sec.~\ref{sec:shunting}: $\partial V/\partial g_{\text{inh}} \propto -V$ \\
Path factor $\pi_n$ & Cable attenuation & Prop.~\ref{prop:path_exact_chain}: Depth decay \\
Morphology factor $\rho_n$ & Layer correlation & Eq.~\eqref{eq:rho}: Task relevance \\
Information factor $\phi_n$ & Conditional predictability & Eq.~\eqref{eq:phi}: $1/(1-R^2)$ \\
Dendritic normalization & Homeostatic scaling & Sec.~4.3: Variance stabilization \\
Branch-type scaling & Apical vs.\ basal & Sec.~4.4: Compartment specialization \\
Broadcast alignment & Feedback alignment & Thm.~\ref{thm:fa_alignment}: $\mathbb{E}[\cos \angle] > 0$ \\
\bottomrule
\end{tabular}
\caption{Summary of theoretical components and their biological/algorithmic interpretations.}
\end{table}

\section{Experimental Validation}

\subsection{Model and Task Suite}

We propose evaluating three model classes across multiple morphologies and tasks:

\paragraph{Models.}
\begin{enumerate}
\item \textbf{Rate-based conductance model}: Current implementation with steady-state voltage equation \eqref{eq:voltage}.
\item \textbf{Conductance-based LIF}: Identical subthreshold dynamics; use surrogate gradients for spikes.
\item \textbf{Multi-compartment biophysical}: 2--4 compartments per neuron (soma, basal, apical); test shunting inhibition placement.
\end{enumerate}

\paragraph{Morphologies.}
\begin{itemize}
\item Chain (single path)
\item Balanced binary tree
\item Random tree
\item Reconstructed morphology from NeuroMorpho (optional)
\end{itemize}

\paragraph{Tasks.}
MNIST, CIFAR-10 (classification); synthetic regression tasks with known optimal solutions.

\paragraph{Baselines.}
Backpropagation (BP), Feedback Alignment (FA), Direct Feedback Alignment (DFA), Target Propagation, Equilibrium Propagation (scaled-down ConvNets if needed).

\subsection{Metrics}

\begin{itemize}
\item \textbf{Performance}: Test accuracy, convergence rate (5 seeds, 95\% CIs)
\item \textbf{Gradient fidelity}: $\|\nabla_{\mathrm{local}} - \nabla_{\mathrm{exact}}\|_2$
\item \textbf{Cosine alignment}: $\cos\angle(\nabla_{\mathrm{local}}, \nabla_{\mathrm{exact}})$ per layer and epoch
\item \textbf{Factor magnitudes}: $\rho_n$, $\phi_n$, $\pi_n$ across layers/epochs (time-series with EMA windows)
\item \textbf{Morphology alignment}: Correlation between learned $\mathbf{g}^{\mathrm{den}}$ and anatomical connectivity
\end{itemize}

\subsection{Ablation Protocol}

\begin{enumerate}
\item Baseline: Standard 5F
\item +Path: Enable \texttt{use\_path\_propagation}
\item +Depth: Enable \texttt{morphology\_modulator\_mode = "depth"}
\item +Norm: Enable \texttt{use\_dendritic\_normalization}
\item +Types: Enable \texttt{use\_branch\_type\_rules}
\item Full: All enabled
\end{enumerate}

\subsection{Shunting Inhibition Ablations}

\begin{itemize}
\item Vary $E_\mathrm{inh} \in \{0, -0.2, -0.4\}$
\item Plot total conductance $g_n^{\mathrm{tot}}$, input resistance $R_n^{\mathrm{tot}}$, and gain curves
\item Report voltage-level divisive effect and rate-level shift
\item Compare proximal vs.\ distal inhibitory placement in multi-compartment models
\end{itemize}

\subsection{Broadcast Modes}

Compare scalar broadcast (A), per-compartment mapping (B), and optional DFA-style random feedback matrix $B_n$ to test Theorem~\ref{thm:fa_alignment}.

\subsection{HSIC Auxiliary Losses}

\begin{itemize}
\item Compare RBF vs.\ linear kernels
\item Report runtime overhead and effect on representation diversity (measured via effective rank or singular value spectrum)
\item Ablate self-decorrelation vs.\ target-correlation weights
\end{itemize}

\section{Future Extensions and Open Questions}

Several directions remain for strengthening this framework:

\paragraph{Information factor variants.}
The current implementation uses $\phi_n = 1/(1-R^2)$, amplifying well-predicted compartments. An alternative $\phi_n = 1 - R^2$ would emphasize unique information. A conditional HSIC formulation could provide:
\[
\phi_n^{\mathrm{cond}} = \frac{\mathrm{HSIC}(V_n,y) - \kappa\, \mathrm{HSIC}(P_n,y)}{\mathrm{HSIC}(V_n,y)+\varepsilon},
\]
with $\kappa\!\in\![0,1]$ controlling parent discount. Empirical comparison of these variants on tasks requiring novelty detection vs. hierarchical consistency would clarify when each is advantageous.

\paragraph{Spiking neural networks.}
Extending to conductance-based LIF neurons with surrogate gradients would demonstrate biological plausibility. The eligibility trace formulation (Section~\ref{sec:implementation}) provides a natural bridge to event-driven learning.

\paragraph{Reconstructed morphologies.}
Testing on realistic dendritic trees from NeuroMorpho would validate the morphology-aware factors on biologically constrained topologies, particularly the depth modulation and branch-type differentiation.

\paragraph{Convergence analysis.}
For linear decoders and quadratic loss, the scalar broadcast (mode A) yields an unbiased descent direction up to a positive scalar (Theorem~\ref{thm:fa_alignment}). Formal convergence rates under Robbins-Monro conditions for diminishing step sizes remain to be established.

\section{Related Work}

Our rules connect to dendritic credit-assignment via apical errors \cite{guerguiev2017segregated,sacramento2018dendritic} and predictive coding or equilibrium propagation as local-gradient mechanisms \cite{whittington2019theories,scellier2017equilibrium}. Broadcast-error variants relate to feedback alignment and direct feedback alignment (DFA) \cite{lillicrap2016random,nokland2016dfa}. Our HSIC-based auxiliary losses follow kernel independence measures \cite{gretton2005hsic,gretton2007hsic}. Shunting inhibition connects to divisive normalization \cite{carandini2012normalization} and the nuanced rate-level consequences of shunts \cite{holt1997shunting}. The compartmental specialization between apical and basal branches relates to empirical findings on layer-specific plasticity rules \cite{larkum2013apical}. The dendritic normalization mechanism parallels homeostatic synaptic scaling \cite{turrigiano2008homeostatic}.

\section{Conclusion}

We have presented a rigorous mathematical framework for local credit assignment in compartmental dendritic networks. Starting from the passive cable equation and deriving exact backpropagation gradients for dendritic trees \eqref{eq:path_sum}, we introduced three classes of local approximations (3F/4F/5F) and extended them with four morphology-aware mechanisms that explicitly exploit dendritic tree topology. Theoretical analysis reveals that each component—path propagation, depth modulation, dendritic normalization, and branch type differentiation—addresses specific limitations of layer-wise approximations. We clarified the role of shunting inhibition in divisive gain control and proved positive expected alignment between local and exact gradients under broadcast error schemes. All methods are implemented in \texttt{local\_learning.py} with full configurability and consistency with the mathematical derivations presented here.

\appendix

\section{Codebase Mapping}

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Equation/Concept} & \textbf{Implementation} \\
\midrule
\eqref{eq:voltage} & \texttt{DendriticBranchLayer.forward()} \\
\eqref{eq:grad_gsyn}, \eqref{eq:grad_gden} & Lines 488--639 (eligibility traces) \\
\eqref{eq:3f_syn}, \eqref{eq:3f_den} & Lines 535--584 (3F updates) \\
\eqref{eq:rho} & \texttt{\_compute\_layer\_rho()} (lines 857--1018) \\
\eqref{eq:phi} & \texttt{\_compute\_layer\_phi\_conditional()} (lines 1039--1146) \\
\eqref{eq:path_factor} & \texttt{\_compute\_path\_propagation\_factor()} (lines 1153--1209) \\
\eqref{eq:rho_depth} & \texttt{\_compute\_branch\_depth\_modulator()} (lines 1211--1236) \\
\eqref{eq:dendritic_norm} & \texttt{\_compute\_dendritic\_normalization()} (lines 1238--1266) \\
\eqref{eq:branch_scale} & \texttt{\_get\_branch\_type\_scale()} (lines 1268--1290) \\
HSIC & Lines 723--854 \\
\bottomrule
\end{tabular}
\end{table}

\section{Synapse Count Optima from Current Optima}

We summarize how to translate an optimal current ratio $r^* = (I_E/I_I)^*$ into an optimal synapse count ratio $(N_e/N_i)^*$ under two commonly used biological/engineering constraints.

\paragraph{(A) Fixed weight ratio $\gamma = w_e/w_i$.}
Current balance constraint $N_e w_e = r^* N_i w_i$ yields
\begin{equation}
\boxed{\left( \frac{N_e}{N_i} \right)^* = \frac{r^*}{\gamma}}.
\end{equation}
Examples: balance ($r^*=1$) $\Rightarrow (N_e/N_i)^*{=}1/\gamma$; Fisher-optimal ($r^*{=}(\sigma_I/\sigma_E)^2$) $\Rightarrow (N_e/N_i)^*{=} (\sigma_I/\sigma_E)^2/\gamma$.

\paragraph{(B) Mean-field scaling $w \propto 1/\sqrt{N}$ with equal constants.}
With $w_e = c/\sqrt{N_e}$ and $w_i = c/\sqrt{N_i}$ (to maintain $O(1)$ variances), currents are $I_E = c\sqrt{N_e}\bar{\mu}$ and $I_I = c\sqrt{N_i}\bar{\mu}$. Enforcing $I_E/I_I = r^*$ gives
\begin{equation}
\boxed{\left( \frac{N_e}{N_i} \right)^* = (r^*)^2}.
\end{equation}
Example: Fisher-optimal $r^*{=}(\sigma_I/\sigma_E)^2 \Rightarrow (N_e/N_i)^*{=} (\sigma_I/\sigma_E)^4$.

These formulas apply uniformly to all cases discussed (balance, noise asymmetry, signal asymmetry, correlation corrections), by substituting the corresponding $r^*$.

\section{Example Configuration}

\begin{verbatim}
local_ca:
  rule_variant: "5f"
  error_broadcast_mode: "scalar"
  
  # Morphology factor
  rho_mode: "pearson"
  rho_estimator: "ema"
  ema_alpha: 0.05
  
  # Information factor
  phi_mode: "conditional"
  phi_estimator: "conditional_ema"
  phi_ridge_lambda: 0.001
  
  # Morphology-aware extensions
  use_path_propagation: true
  morphology_modulator_mode: "depth"
  morphology_depth_offset: 2.0
  use_dendritic_normalization: true
  use_branch_type_rules: true
  apical_branch_scale: 1.5
  basal_branch_scale: 1.0
  
  # Compartmental
  use_conductance_scaling: true
  use_driving_force: true
  e_rev_exc: 1.0
  
  # Optimization
  clip_grad_value: 5.0
  normalize_by_batch: true
\end{verbatim}

\begin{thebibliography}{99}

\bibitem{koch1999biophysics}
Koch, C. (1999).
\emph{Biophysics of Computation: Information Processing in Single Neurons}.
Oxford University Press.

\bibitem{dayan2001theoretical}
Dayan, P., \& Abbott, L. F. (2001).
\emph{Theoretical Neuroscience}.
MIT Press.

\bibitem{carandini2012normalization}
Carandini, M., \& Heeger, D. J. (2012).
Normalization as a canonical neural computation.
\emph{Nature Reviews Neuroscience}, 13(1), 51--62.

\bibitem{holt1997shunting}
Holt, G. R., \& Koch, C. (1997).
Shunting inhibition does not have a divisive effect on firing rates.
\emph{Neural Computation}, 9(5), 1001--1013.

\bibitem{lillicrap2016random}
Lillicrap, T. P., Cownden, D., Tweed, D. B., \& Akerman, C. J. (2016).
Random synaptic feedback weights support error backpropagation for deep learning.
\emph{Nature Communications}, 7, 13276.

\bibitem{nokland2016dfa}
N{\o}kland, A. (2016).
Direct feedback alignment provides learning in deep neural networks.
\emph{Advances in Neural Information Processing Systems}, 29.

\bibitem{guerguiev2017segregated}
Guerguiev, J., Lillicrap, T. P., \& Richards, B. A. (2017).
Towards deep learning with segregated dendrites.
\emph{eLife}, 6, e22901.

\bibitem{sacramento2018dendritic}
Sacramento, J., Costa, R. P., Bengio, Y., \& Senn, W. (2018).
Dendritic cortical microcircuits approximate the backpropagation algorithm.
\emph{Advances in Neural Information Processing Systems}, 31.

\bibitem{whittington2019theories}
Whittington, J. C., \& Bogacz, R. (2019).
Theories of error back-propagation in the brain.
\emph{Trends in Cognitive Sciences}, 23(3), 235--250.

\bibitem{scellier2017equilibrium}
Scellier, B., \& Bengio, Y. (2017).
Equilibrium propagation: Bridging the gap between energy-based models and backpropagation.
\emph{Frontiers in Computational Neuroscience}, 11, 24.

\bibitem{gretton2005hsic}
Gretton, A., Bousquet, O., Smola, A., \& Sch{\"o}lkopf, B. (2005).
Measuring statistical dependence with Hilbert-Schmidt norms.
\emph{International Conference on Algorithmic Learning Theory}, 63--77.

\bibitem{gretton2007hsic}
Gretton, A., Fukumizu, K., Teo, C. H., Song, L., Sch{\"o}lkopf, B., \& Smola, A. J. (2007).
A kernel statistical test of independence.
\emph{Advances in Neural Information Processing Systems}, 20.

\bibitem{fremaux2016three}
Fr{\'e}maux, N., \& Gerstner, W. (2016).
Neuromodulated spike-timing-dependent plasticity, and theory of three-factor learning rules.
\emph{Frontiers in Neural Circuits}, 9, 85.

\bibitem{bellec2020eprop}
Bellec, G., Scherr, F., Subramoney, A., Hajek, E., Salaj, D., Legenstein, R., \& Maass, W. (2020).
A solution to the learning dilemma for recurrent networks of spiking neurons.
\emph{Nature Communications}, 11, 3625.

\bibitem{larkum2013apical}
Larkum, M. (2013).
A cellular mechanism for cortical associations: an organizing principle for the cerebral cortex.
\emph{Trends in Neurosciences}, 36(3), 141--151.

\bibitem{welford1962note}
Welford, B. P. (1962).
Note on a method for calculating corrected sums of squares and products.
\emph{Technometrics}, 4(3), 419--420.

\bibitem{turrigiano2008homeostatic}
Turrigiano, G. G. (2008).
The self-tuning neuron: synaptic scaling of excitatory synapses.
\emph{Cell}, 135(3), 422--435.

\end{thebibliography}

\end{document}
