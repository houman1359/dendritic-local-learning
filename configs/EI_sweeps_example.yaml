# ============================================================================
# UNIFIED PARAMETER SWEEP CONFIGURATION
# ============================================================================
# This configuration file enables comprehensive parameter sweeps across multiple
# dimensions of dendritic models. It supports EI ratios, branch architecture,
# and general hyperparameter sweeps in a unified framework.
#
# USAGE:
#   Generate sweep configurations only (no job submission):
#   python src/dendritic_modeling/scripts/sweeps/sweep_manager.py --config configs/unified_all_sweeps_example.yaml --generate-only
#
#   Generate sweep configurations and create SLURM job scripts:
#   python src/dendritic_modeling/scripts/sweeps/sweep_manager.py --config configs/unified_all_sweeps_example.yaml
#
#   Analyze completed sweep results:
#   python src/dendritic_modeling/scripts/sweeps/result_analyzer.py /path/to/sweep/results/
#
# SWEEP TYPES SUPPORTED:
#   - EI sweeps: Excitatory/Inhibitory synapse ratios and connectivity
#   - Branch sweeps: Dendritic architecture patterns and branching factors
#   - General sweeps: Any model/training hyperparameters
#   - Mixed sweeps: Combine multiple sweep types simultaneously
#   - Noise sweeps: Robustness testing with various noise types
#
# NOTE: Individual sweep configurations are trained using:
#   python src/dendritic_modeling/scripts/training/train_experiments.py <config_path>

# Output and SLURM configuration
slurm_config:
  account: ""     # Your SLURM account
  partition: ""   # Your SLURM partition
  time: "12:00:00"
  nodes: 1
  ntasks_per_node: 1
  gpus_per_node: 1
  cpus_per_task: 12
  mem: 200GB
  max_concurrent_jobs: 32  # Maximum number of array jobs to run simultaneously
  exclude_nodes: ["holygpu8a10101", "holygpu8a10102"]  # Nodes to exclude from job scheduling (e.g., ["node1", "node2"])
  modules_to_load:
    - python/3.12.5-fasrc01
    - cuda/12.4.1-fasrc01
    - cudnn/8.9.2.26_cuda12-fasrc01
  conda_env_path: ""
  training_script: "src/dendritic_modeling/scripts/training/train_experiments.py"

# Experiment settings
experiment_settings:
  seeds_per_condition: 20   # Number of seeds per parameter combination
  base_seed: 0

# ============================================================================
# PARAMETER SWEEPS - Choose your parameters!
# ============================================================================
sweep_config:
  # -------------------------
  # EI CONNECTIVITY SWEEPS
  # -------------------------
  model.core.connectivity.ee_synapses_per_branch_per_layer: [[1],[2],[3],[4],[5],[6],[7],[8],[9],[10],[11],[12],[13],[14],[15],[16],[17],[18],[19],[20],[21],[22],[23],[24],[25],[26],[27],[28],[29],[30],[31],[32],[33],[34],[35],[36],[37],[38],[39],[40],[41],[42],[43],[44],[45],[46],[47],[48],[49],[50]]
  model.core.connectivity.ie_synapses_per_branch_per_layer: [[1],[2],[3],[4],[5],[6],[7],[8],[9],[10],[11],[12],[13],[14],[15],[16],[17],[18],[19],[20],[21],[22],[23],[24],[25],[26],[27],[28],[29],[30],[31],[32],[33],[34],[35],[36],[37],[38],[39],[40],[41],[42],[43],[44],[45],[46],[47],[48],[49],[50]]
  # Architecture variants (replaces legacy `model.core.morphology.use_shunting`)
  model.core.type: ["dendritic_shunting", "dendritic_additive", "dendritic_mlp", "point_mlp"]


  # model.core.architecture.excitatory_layer_sizes: [[10], [20], [30]]
  # model.encoder.params.encoder_hidden_dims: [[32], [64], [128]]

  # Use [start, end, step] to generate ranges:
  # some_parameter: [1, 10, 2]    # Creates [1, 3, 5, 7, 9]
  # float_parameter: [0.1, 1.0, 0.1]  # Creates [0.1, 0.2, ..., 1.0]

# ============================================================================
# FILTER CONFIGURATION
# ============================================================================
filter_config:
  # EI Filtering (optional)
  mode: "ei-sum"  # Options: "all", "ei-sum", "e-const", "i-const", "ei-ratio"
  value: 51      # ei-sum: E+I=25, e-const: E=25, i-const: I=25, ei-ratio: E/I=25
  # tolerance: 0   # exact match (default: 0 for exact matching)

  # Branch Sweep (optional - can combine with EI filtering above)
  # branch_sweep_mode: "uniform_sweep"         # Options: uniform_sweep, depth_sweep, width_sweep, pattern_sweep, custom_configs
  # dendrite_tree_depth: [1, 4, 1]            # depths 1,2,3,4
  # dendrite_branches_per_layer: [2, 8, 2]    # branches 2,4,6,8

  # See examples section at end of file for all options and combinations

# ============================================================================
# BASE TRAINING CONFIGURATION (Shared by all sweep variants)
# ============================================================================
base_config:
  # ============================================================================
  # EXPERIMENT SETUP
  # ============================================================================
  experiment:
    seed: 42                              # Random seed for reproducibility (any integer)
    train_valid_split: 0.8                # Train/validation split ratio (0.0-1.0)
    enable_amp: false                     # Enable automatic mixed precision (true/false)
    enable_profiling: false               # Enable PyTorch profiler (true/false)
    profiling_output_dir: "results/profiling/"  # Directory for profiling output
    enable_hooks: true                    # Enable gradient hooks for analysis (true/false)

    checkpointing:
      enabled: false                      # Enable model checkpointing (true/false)
      save_every_n_epochs: 10             # Checkpoint frequency in epochs (any positive integer)
      checkpoint_dir: "results/checkpoints/"  # Directory for checkpoints

  # ============================================================================
  # DATA
  # ============================================================================
  data:
    dataset_name: "cifar10"               # Dataset name
    # Options: "mnist", "cifar10", "imagenet", "mnist_modulo10", "random_flip_mnist",
    #          "poisson_generator", "multixor", "orientation_bars", "orthonet"

    base_dir: "/n/netscratch/bsabatini_lab/Everyone/mrichards/HS/paper_results/"                          # Base directory for datasets (empty = auto-detect)

    processing:
      flatten: true                       # Flatten input images to 1D (true/false)
      normalize: false                    # Normalize input data (true/false)

    dataset_params:
      mnist_modulo10:
        shuffle_iterations: 2             # Number of shuffle iterations (any positive integer)

      poisson_generator:
        base_dataset: "mnist"             # Base dataset for Poisson generation
        stimulus_duration: 2.5              # Rate multiplier for Poisson process
        max_gain_factor: 10              # Maximum gain factor for Poisson process
        max_gain_tau_ratio: null
        uniform_gain: false              # Uniform gain factor for Poisson process

      multixor:
        n_bits: 8                         # Number of bits for multi-XOR problem

      orientation_bars:
        n_orient_classes: 8               # Number of orientation classes

      orthonet:
        input_dim: 784                    # Input dimension
        n_layers: 5                       # Number of layers
        nonlinearity: "sigmoid"           # Nonlinearity: "sigmoid", "tanh", null
        repeat_layers: false              # Repeat layer structure (true/false)
        noise_std: 0.1                    # Noise standard deviation

      random_flip_mnist:
        flip_probability: 0.5             # Probability of flipping labels (0.0-1.0)

  # ============================================================================
  # MODEL
  # ============================================================================
  model:
    task: "classification"                # Task type: "classification", "regression"

    encoder:
      type: "identity"                    # Encoder type
      # Options: "identity", "mlpautoencoder", "cnnautoencoder",
      #          "mlpvariationalautoencoder", "cnnvariationalautoencoder"

      load_save_root: "./.trained_encoder_networks/"  # Directory for encoder checkpoints

      params:
        input_dim: null                   # Input dimension (auto-detected from data)
        input_shape: [3, 32, 32]          # Input shape for CNN encoders [channels, height, width]
        encoder_hidden_dims: [128, 64]    # Hidden layer dimensions for MLP encoders
        encoder_kernel_sizes: [3, 3]      # Kernel sizes for CNN encoders
        encoder_strides: [2, 2]           # Strides for CNN encoders
        latent_dim: null                  # Latent dimension for autoencoders
        latent_shape: null                # Latent shape for CNN autoencoders
        decoder_hidden_dims: [64, 128]    # Decoder hidden dimensions
        activation: "relu"                # Activation function: "relu", "tanh", "sigmoid", "gelu"

    core:
      type: "EINet"                       # Core network type
      # Options: "EINet", "MLP", "MatchedParamMLP", "EffectiveMatchedParamMLP"

      architecture:
        excitatory_layer_sizes: [20]      # Number of excitatory neurons per layer
        inhibitory_layer_sizes: []        # Number of inhibitory neurons per layer
        excitatory_branch_factors: [3, 3, 3, 3]  # Branching factors for excitatory dendrites
        inhibitory_branch_factors: [1]    # Branching factors for inhibitory dendrites

        # Inhibitory network configuration
        inhibitory_network_type: "dendritic"  # Options: "dendritic", "mlp"
        mlp_inhibitory_network_params:     # Parameters for MLP inhibitory network (used when type="mlp")
          hidden_dims: [200]               # Hidden layer dimensions for MLP inhibitory network
          activation: "relu"               # Activation function for MLP inhibitory network

      connectivity:
        ee_synapses_per_branch_per_layer: [40]  # Excitatory-to-excitatory synapses per branch
        ei_synapses_per_branch_per_layer: [0]   # Excitatory-to-inhibitory synapses per branch
        ie_synapses_per_branch_per_layer: [10]  # Inhibitory-to-excitatory synapses per branch
        ii_synapses_per_branch_per_layer: [0]   # Inhibitory-to-inhibitory synapses per branch

      transfer:
        input_mode: 1                     # Input mode: 0 (combined), 1 (separate E/I pathways)
        independent_pathways: false       # Independent E/I pathways (true/false)
        excitatory_dim: null              # Excitatory pathway dimension (for input_mode 1)
        inhibitory_dim: null              # Inhibitory pathway dimension (for input_mode 1)
        output_activation: null           # Output activation: "sigmoid", "relu", "tanh", "softplus", null

      morphology:
        somatic_synapses: false           # Include somatic synapses (true/false)
        use_shunting: true                # Use shunting inhibition (true/false)
        weight_transform: "softplus"      # Weight transformation: "exp", "relu", "softplus"
        dbl_init_method: "analytical_expectation"  # DBL initialization: "analytical_expectation", "default", "ei_equivalence", "naive"

      sparsity:
        init_method: "xavier_normal"      # Weight initialization: "xavier_normal", "xavier_uniform", "kaiming_normal"
        noise_level: 0.0                  # Noise level for stochastic sparsity (0.0-1.0)
        type: "standard"                  # Sparsity type: "standard", "stochastic", "variance", "deepst"
        temperature: 0.0                  # Temperature for stochastic selection (0.1-1.0)
        gradient_scaling: "none"          # Gradient scaling strategy: "none", "conductance_dynamic"

        deepst:                           # DeepST-specific parameters (only used when type: "deepst")
          excitatory_target_density: 0.1  # Target density for excitatory connections (0.0-1.0)
          inhibitory_target_density: 0.1  # Target density for inhibitory connections (0.0-1.0)
          use_noise: true                 # Apply Gaussian noise during rewiring (true/false)
          sigma: 0.05                     # Standard deviation of Gaussian noise
          rewiring_mode: "global"         # Rewiring mode: "global", "local", "constant_branch"
          rewire_frequency: 1             # Rewiring frequency in epochs
          synapses_per_branch: null       # Synapses per branch (for constant_branch mode)
          freeze_excitatory_connectivity: false  # Freeze excitatory connectivity (true/false)
          freeze_inhibitory_connectivity: false  # Freeze inhibitory connectivity (true/false)
          init_method: "xavier_normal"    # Weight initialization method
          weight_threshold: 1e-6          # Pruning threshold

      reactivation:
        enabled: true                     # Enable reactivation functions (true/false)
        type: "param_tanh"                # Reactivation type: "param_tanh", "tanh", "sigmoid"
        init_m: 1.5                       # Reactivation parameter m (slope)
        init_b: 0.5                       # Reactivation parameter b (bias)
        gradient_scaling: "none"          # Gradient scaling: "none", "inverse", "conductance_dynamic"

      blocklinear:
        gradient_scaling: "none"          # BlockLinear strategy: "none", "conductance_dynamic"
        efficient: false                  # Use memory-efficient implementation (true/false)

      implementation:
        adaptive_initialization: false    # Network-aware weight scaling (true/false)
        print_hooks: false                # Print gradient statistics (true/false)

    decoder:
      type: "MLP"                         # Decoder type: "MLP", "Identity"
      params:
        input_dim: null                   # Input dimension (auto-set from core network)
        hidden_dims: [32, 16]             # Hidden layer dimensions
        activation: "relu"                # Activation: "relu", "tanh", "sigmoid", "gelu"
        output_dim: 10                    # Output dimension (auto-set from dataset)

  # ============================================================================
  # TRAINING
  # ============================================================================
  training:
    # ==========================================
    # ENCODER TRAINING
    # ==========================================
    encoder:
      strategy: "standard"                # Training strategy: "standard", "voltage_stabilization",
                                         # "homeostatic_control", "freeze_layers", "freeze_branches"

      common:
        lr: 0.001                         # Learning rate (any positive float)
        loss_function: "log_mse"          # Loss function: "mse", "log_mse", "ce", "bce"
        epochs: 500                       # Number of training epochs
        early_stopping: true              # Enable early stopping (true/false)
        patience: 100                     # Early stopping patience in epochs
        batch_size: 512                   # Training batch size
        shuffle: true                     # Shuffle training data (true/false)
        grad_clip_value: 5.0              # Gradient clipping value (any positive float)
        load_best_state_dict: true        # Load best model after training (true/false)
        plot_losses: false                # Plot training curves (true/false)
        suppress_prints: false            # Suppress training output (true/false)
        print_every: 10                   # Print frequency in epochs
        use_amp: false                    # Use automatic mixed precision (true/false)
        enable_nan_checking: false        # Enable NaN checking (true/false)
        enable_adaptive_clipping: false   # Enable adaptive gradient clipping (true/false)

    # ==========================================
    # MAIN TRAINING
    # ==========================================
    main:
      strategy: "standard"                # Training strategy
      # Options: "standard", "voltage_stabilization", "homeostatic_control",
      #          "freeze_layers", "freeze_branches", "local_ca", "multi_stage"

      common:
        epochs: 10000                        # Number of training epochs
        batch_size: 128                   # Training batch size
        shuffle: true                     # Shuffle training data (true/false)
        grad_clip_value: 5.0              # Gradient clipping value
        loss_function: "cat_nll"          # Loss function: "ce", "cat_nll", "mse", "bce"
        early_stopping: false             # Enable early stopping (true/false)
        patience: 500                     # Early stopping patience in epochs
        load_best_state_dict: true        # Load best model after training (true/false)
        save_path: null                   # Custom save path (null = auto)
        plot_losses: true                 # Plot training curves (true/false)
        suppress_prints: false            # Suppress training output (true/false)
        print_every: 10                   # Print frequency in epochs
        use_amp: false                    # Use automatic mixed precision (true/false)
        enable_nan_checking: false        # Enable NaN checking (true/false)
        enable_adaptive_clipping: false   # Enable adaptive gradient clipping (true/false)
        weight_decay_rate: 0.0            # Weight decay rate (0.0-0.1)
        weight_boosting: false            # Enable weight boosting during decay (true/false)

        param_groups:                     # Parameter-specific learning rates
          lr: 0.001                       # Base learning rate
          split_params: false             # Split parameters into groups (true/false)
          topk_lr: 0.001                  # Learning rate for sparsity parameters
          blocklinear_lr: 0.0001          # Learning rate for BlockLinear parameters
          reactivation_lr: 0.0001         # Learning rate for reactivation parameters
          decoder_lr: 0.01                # Learning rate for decoder parameters

  # ==========================================
      # REGULARIZATION
  # ==========================================
      regularization:
        l1_weight: 0.0                    # L1 regularization strength (0.0-0.1)
        l2_weight: 0.0                    # L2 regularization strength (0.0-0.1)

        selective:                        # Selective regularization application
          layers: []                      # Layer indices to regularize (empty = all)
          branches: []                    # Branch names to regularize (empty = all)
          weight_types: ["all"]           # Weight types: "all", "excitatory", "inhibitory", "recurrent"

        split_params: false               # Use parameter-specific regularization (true/false)
        param_group_weights:              # Parameter group regularization weights
          topk:
            l1_weight: 0.0
            l2_weight: 0.0
          blocklinear:
            l1_weight: 0.0
            l2_weight: 0.0
          reactivation:
            l1_weight: 0.0
            l2_weight: 0.0
          decoder:
            l1_weight: 0.0
            l2_weight: 0.0

        enforce_ei_weight_ratio: false    # Enforce E/I weight ratio (true/false)
        target_ei_weight_ratio: 0.5       # Target E/I weight ratio (any positive float)
        ei_ratio_loss_weight: 0.5         # E/I ratio loss weight (0.0-1.0)
        ei_ratio_scope: "per_branch"      # E/I ratio scope: "per_branch", "per_layer", "global"
        ei_ratio_metric: "mean"           # E/I ratio metric: "mean", "median", "rms"

  # ==========================================
      # PRUNING
  # ==========================================
      pruning:
        enabled: false                    # Enable weight pruning (true/false)
        threshold: 2                      # Pruning threshold (any positive float)
        selective:
          layers: []                      # Layer indices to prune (empty = all)
          branches: []                    # Branch names to prune (empty = all)
          weight_types: ["excitatory", "inhibitory"]  # Weight types to prune
        re_evaluate_after: false          # Re-evaluate model after pruning (true/false)
        dynamic_pruning: false            # Prune during training (true/false)
        pruning_frequency: 10             # Dynamic pruning frequency in epochs

        report_non_pruned: true           # Report non-pruned synapse count (true/false)
        save_pruning_stats: false         # Save pruning statistics (true/false)
        detailed_branch_report: true      # Detailed per-branch pruning report (true/false)
        redo_analysis_after_pruning: false # Redo analysis after pruning (true/false)

      # ==========================================
      # TRAINING STRATEGIES
      # ==========================================
      strategies:
        standard: {}                      # Standard backpropagation (no additional params)

        multi_stage:                      # Multi-stage training configuration
          analyze_between_stages: true    # Run analysis between stages (true/false)
          use_best_from_each_stage: true  # Use best checkpoint from each stage (true/false)
          continue_on_failure: false      # Continue if a stage fails (true/false)

          stages:                         # List of training stages
            - learning_strategy: "standard"
              epochs: 100
              batch_size: 512
              save_checkpoint: false
              param_groups:
                lr: 0.01
                split_params: true
                topk_lr: 0.001
                blocklinear_lr: 0.0001
                reactivation_lr: 0.0001
                decoder_lr: 0.001
              trainer_config:
                loss_function: "ce"
                early_stopping: true
                patience: 500

            - learning_strategy: "local_ca"
              epochs: 100
              batch_size: 512
              reset_optimizer: false
              save_checkpoint: false
              param_groups:
                lr: 0.0001
                split_params: true
                topk_lr: 0.0001
                blocklinear_lr: 0.00001
                reactivation_lr: 0.00001
                decoder_lr: 0.001
              trainer_config:
                loss_function: "ce"
                early_stopping: false

        local_ca:                         # Local credit assignment configuration
          # Core configuration
          rule_variant: "5f"              # Rule variant: "3f", "4f", "5f"
          error_mode: "auto"              # Error mode: "auto", "mse", "ce", "bce"
          error_broadcast_mode: "scalar"  # Error broadcast: "scalar", "per_soma", "local_mismatch"

          # 3-factor rule parameters (base)
          three_factor:
            use_conductance_scaling: true # Scale by R_tot = 1/g_tot
            use_driving_force: true       # Use (E_rev - V_n) driving force
            theta: 0.0                    # Leak reversal potential
            e_rev_exc: 1.0                # Excitatory reversal potential
            e_rev_inh: 0.0                # Inhibitory reversal (shunting)

          # 4-factor rule parameters (morphology correlation ρ)
          four_factor:
            rho_mode: "dot"               # "pearson", "dot", "none"
            rho_estimator: "ema"          # "ema", "augment", "ema+augment"
            ema_alpha: 0.1                # EMA smoothing rate
            layer_wise_rho_scale: 0.0     # Layer-wise scaling (0.0 = disabled)
            augment_k: 8                  # Augmented samples for micro-ensemble
            augment_noise_sigma: 0.01     # Noise level for augmentation

          # 5-factor rule parameters (information proxy φ)
          five_factor:
            phi_mode: "conditional"       # "conditional", "variance"
            phi_estimator: "conditional_ema" # "conditional_ema", "variance_ema"
            phi_ridge_lambda: 0.001       # Ridge regularization
            layer_wise_phi_scale: 0.0     # Layer-wise scaling (0.0 = disabled)
            rls_forgetting: 0.99          # RLS forgetting factor

          # Morphology-aware extensions
          morphology_aware:
            use_path_propagation: false   # Path-integrated propagation
            morphology_modulator_mode: "none" # "none", "depth", "centrality"
            morphology_depth_offset: 1.0  # Depth modulation offset
            morphology_centrality_metric: "betweenness" # Centrality metric
            use_dendritic_normalization: false # Normalize by branch conductance
            use_branch_type_rules: false  # Apical vs basal differentiation
            apical_branch_scale: 1.0      # Apical branch scaling
            basal_branch_scale: 1.0       # Basal branch scaling
            use_branch_length_modulation: false

          # HSIC auxiliary loss
          hsic:
            enabled: false                # Enable HSIC objective
            weight: 0.01                  # HSIC strength
            self_weight: 0.3              # Self-decorrelation weight
            target_weight: 0.3            # Target-correlation weight
            target_source: "labels"       # "labels" or "logits"
            kernel: "rbf"                 # "linear", "rbf", "polynomial"
            sigma: 1.0                    # RBF bandwidth
            degree: 2                     # Polynomial degree
            coef0: 1.0                    # Polynomial coefficient
            grad_clip_value: 0.1          # HSIC gradient clipping
            warmup_epochs: 5              # HSIC warmup epochs
            apply_last_layer_only: false  # Apply only to last layer

          # Parameter update control
          update_inactive_weights: false  # Update masked weights (true/false)
          update_reactivation: true       # Update reactivation parameters
          decoder_update_mode: "backprop" # "backprop", "local", "none"

          # Training schedule
          freeze_reactivation_epochs: 20  # Epochs to freeze reactivation
          freeze_decoder_epochs: 0        # Epochs to freeze decoder
          training_schedule: []            # Custom per-epoch schedule

          # Optimization
          clip_grad_value: 5.0            # Gradient clipping value
          normalize_by_batch: false       # Normalize updates by batch

        voltage_stabilization:
          pretrain_epochs: 50
          stabilize_mode: "vout"          # Stabilization mode: "vout", "vinf"
          freeze_reactivation: true
          target_voltage: 0.5
          loss_metric: "mse"

        homeostatic_control:
          pretrain_epochs: 50
          stabilize_mode: "vinf"
          freeze_reactivation: false
          target_voltage: 0.5
          loss_metric: "mse"

        freeze_layers:
          pretrain_epochs: 10
          reverse_training: false
          epochs_per_layer: 50
          final_tune_epochs: 10

        freeze_branches:
          pretrain_epochs: 10
          reverse_training: false
          epochs_per_branch: 50
          final_tune_epochs: 10

        freeze_branch_kl:
          pretrain_epochs: 10
          reverse_training: false
          epochs_per_branch: 50
          final_tune_epochs: 10
          kl_weight: 0.1

# ============================================================================
  # ANALYSIS
# ============================================================================
  analysis:

    performance_analysis:
      enabled: true                       # Enable performance analysis (true/false)
      training: true                      # Run during training (true/false)
      params:
        accuracy: true                    # Compute accuracy (true/false)
        auc: true                         # Compute AUC (true/false)
        categorical_loglikelihood: true   # Compute categorical log-likelihood (true/false)
        mse: false                        # Compute MSE (true/false)
        cosine_similarity: false          # Compute cosine similarity (true/false)

    information_analysis:
      enabled: true                       # Enable information analysis (true/false)
      training: false                     # Run during training (true/false)
      params:
        selection:
          components: ["basic", "pairwise", "conditional", "ablation_aligned", "layer_proxies"]  # Components: "basic", "pairwise", "conditional", "gaussian_fisher", "pid", "ablation_aligned", "layer_proxies", "upstream_unique_cmi"
          scopes: []                         # Scopes: "layer", "einet", "neuron" (alias: views)
          signal_variants: ["network", "lda"] # Variants: "network", "lda", "random" (random requires baselines.random_weights.n_shuffles > 0)

        compute:
          max_samples: 1000                 # Max samples (null = all)
          include_vinf: true                 # Include soma activation: true|false
          normalize_mi: false                # Normalize by entropy: true|false
          verbose: false                     # Verbose: true|false
          computation_level: "single_branch" # Level: "single_branch", "layer_branch", "all_branch"
          branch_aggregation: "mean"         # Aggregation: "mean", "multivariate", "sample"
          per_layer_analysis: false          # Extra breakdown/plots: per-layer (overridden by selection.scopes if non-empty)
          per_einet_analysis: false          # Extra breakdown/plots: per-E/I-net (overridden by selection.scopes if non-empty)
          per_neuron_analysis: false         # Extra breakdown/plots: per-neuron (expensive; overridden by selection.scopes if non-empty)
          layer_total_topk: 10               # K for *_topK_sum proxies (int >= 1)
          gaussian_fisher:
            aggregation: "sum"  # Aggregate per-feature F: "sum"|"mean"|"max"
            mi_transform: "half_log1p"  # Transform F→MI proxy: "half_log1p"|"log1p"|"none"
            eps: 1.0e-8  # Denominator stabilizer (alias: ridge) (float > 0)

        pid:
          binarize: true  # Binarize continuous vars: true|false
          binarize_method: "median"  # Binarization: "median"|"mean"|"quantile"
          binarize_threshold: 0.5  # Used only if binarize_method="quantile": q∈[0,1]

        estimator:
          method: "kraskov"                 # MI estimation: "kraskov", "decoder", "binned", "sklearn", "copula", "auto"
          kraskov:
            n_neighbors: 10                 # Kraskov k-NN (int >= 1)
          binned:
            n_bins: 20                      # Binned/sklearn histogram bins (int >= 2)
            binning_strategy: "quantile"    # binned: "quantile" or "uniform"
            bias_correction: "none"         # binned: "none" or "miller_madow"
          decoder:
            cv_folds: 5                     # decoder: CV folds (int >= 2)
            C: 1.0                          # decoder: LogisticRegression C (float > 0)
            standardize: true               # decoder: standardize X (true|false)
            seed: 0                         # decoder: CV shuffle seed (int)
            continuous:
              strategy: "none"              # decoder continuous: "none", "binned", "gaussian"
              max_total_dim: 2              # decoder continuous: max total dim (x+y [+z]); used only if strategy != "none"
              gaussian_ridge: 1e-6          # decoder continuous: gaussian ridge (float > 0)
          copula:
            type: "gaussian"                # Copula: "gaussian", "kernel", "dvc"
          sklearn:
            n_bins: 20                      # sklearn histogram bins (int >= 2)
            bandwidth: null                 # sklearn KDE bandwidth (null = auto)

        baselines:
          random_weights:
            n_shuffles: 0                   # Random-weight baseline shuffles (int >= 0; produces *_sh metrics)
            seed: 42                        # Random-weight seed (int)
          label_shuffle_null:
            n_shuffles: 0                   # Null (label-shuffle) baseline shuffles (int >= 0; produces *_null metrics)
            seed: 0                         # Null baseline seed (int)
            components: ["basic"]           # Null components: "basic", "conditional", "ablation_aligned", "upstream_unique_cmi", "all"
            conditional_shuffle: "auto"     # Conditional null shuffle: "auto", "global", "bins"
            conditional_bins: 10            # If using bins: number of Z-bins (int >= 2)

    correlation_analysis:
      enabled: false                      # Enable correlation analysis (true/false)
      training: false
      params:
        selection:
          components: ["total", "noise", "signal", "tuning"]  # Components: "total", "noise", "signal", "tuning"
          pairs: ["EE", "II", "EI", "E_output", "I_output", "output_output"]  # Pairs: "EE", "II", "EI", "E_output", "I_output", "output_output"
          scopes: ["layer", "einet"]        # Scopes: "layer", "einet", "neuron"

        compute:
          computation_level: "layer_branch" # Level: "synaptic", "single_branch", "layer_branch", "all_branch"
          max_samples: 2000                 # Max samples (null = all)
          batch_process: true               # Batch processing: true|false
          per_layer_analysis: true          # Extra breakdown/plots: per-layer (overridden by selection.scopes if non-empty)
          per_einet_analysis: true          # Extra breakdown/plots: per-E/I-net (overridden by selection.scopes if non-empty)
          per_neuron_analysis: false        # Extra breakdown/plots: per-neuron (expensive; overridden by selection.scopes if non-empty)

        sampling:
          n_synapse_pairs: 1000             # Synaptic-only: number of synapse pairs (int >= 1)
          sampling_strategy: "stratified"   # Sampling: "random", "stratified", "importance"
          stratified_categories: ["within_branch", "within_layer", "across_layer"]  # Stratified categories: "within_branch", "within_layer", "across_layer"
          max_pairs_per_category: 500       # Stratified-only: max pairs per category (int >= 1)

    ablation_analysis:
      enabled: true                       # Enable ablation analysis (true/false)
      training: false
      params:
        selection:
          levels: ["layer"]                 # Levels: "layer", "compartment" (compartment is lesion-only)
          targets: ["all_synapses", "excitation", "inhibition", "upstream"]  # Targets: "all_synapses", "excitation", "inhibition", "upstream"
          metrics: ["accuracy", "auc", "categorical_loglikelihood"]  # Metrics: "accuracy", "auc", "categorical_loglikelihood", "mse", "cosine_similarity", "pred_label_mi_bits"

        methods:
          ablation_methods: ["lesion"]      # Methods: "lesion", "shuffle", "mean_clamp"
          shuffle_seed: 0                   # Shuffle seed (int)
          clamp_statistic: "mean"           # mean_clamp statistic: "mean"

    synapse_turnover_analysis:
      enabled: true                       # Enable synapse turnover analysis (true/false)
      training: true                      # Run during training (true/false)
      params:
        snapshot_interval: 1              # Snapshot frequency in epochs
        synaptic_analysis: true           # Enable synaptic connectivity analysis (true/false)
        save_synaptic_report: true        # Save synaptic analysis report (true/false)

    noise_perturbation_analysis:
      enabled: true                       # Enable noise perturbation analysis (true/false)
      training: false
      params:
        uniform_noise: true               # Test uniform noise (true/false)
        uniform_magnitudes: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]  # Noise magnitudes
        gaussian_noise: true              # Test Gaussian noise (true/false)
        gaussian_sdevs: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]      # Noise standard deviations
        n_samples: 10                     # Number of noise samples per magnitude
        accuracy: true                    # Compute accuracy (true/false)
        auc: true                         # Compute AUC (true/false)
        categorical_loglikelihood: true   # Compute categorical log-likelihood (true/false)
        mse: false                        # Compute MSE (true/false)
        cosine_similarity: false          # Compute cosine similarity (true/false)

    weight_analysis:
      enabled: true                       # Enable weight analysis (true/false)
      training: false
      params:
        synapse_weights: true             # Analyze synapse weights (true/false)
        branch_weights: true              # Analyze branch weights (true/false)
        computation_level: "single_branch" # Computation level: "single_branch", "layer_branch", "all_branch"
        branch_aggregation: "mean"        # Aggregation method: "mean", "multivariate", "sample"
        per_layer_analysis: true          # Per-layer analysis (true/false)
        per_einet_analysis: true          # Per-EI-network analysis (true/false)
        per_neuron_analysis: false        # Per-neuron analysis (true/false)
        analyze_excitatory: true          # Analyze excitatory weights (true/false)
        analyze_inhibitory: true          # Analyze inhibitory weights (true/false)
        analyze_branch_output: false      # Analyze branch-output weights (true/false)
        compute_mean: true                # Compute mean weights (true/false)
        compute_variance: true            # Compute weight variance (true/false)
        compute_percentiles: false        # Compute percentiles (true/false)
        compute_min_max: true             # Compute min/max (true/false)
        compute_dendritic_strength: true  # Compute dendritic strength (true/false)
        weight_threshold: 1e-6            # Weight threshold for "active" synapses

    single_layer_contribution:
      enabled: false                      # Enable single layer contribution analysis (true/false)
      training: false
      params:
        excitation_contribution: true     # Analyze excitation contribution (true/false)
        inhibition_contribution: true     # Analyze inhibition contribution (true/false)
        both_contribution: true           # Analyze combined contribution (true/false)
        accuracy: true                    # Compute accuracy (true/false)
        auc: true                         # Compute AUC (true/false)
        categorical_loglikelihood: true   # Compute categorical log-likelihood (true/false)
        mse: true                         # Compute MSE (true/false)
        cosine_similarity: true           # Compute cosine similarity (true/false)

    synaptic_activation_analysis:
      enabled: false                      # Enable synaptic activation analysis (true/false)
      training: false
      params:
        network_summary: true             # Network-level summary (true/false)
        layer_summary: true               # Layer-level summary (true/false)
        branch_summary: true              # Branch-level summary (true/false)
        raw_summary: true                 # Raw activation summary (true/false)
        logspace: false                   # Use log space (true/false)
        downsample: true                  # Downsample data (true/false)
        samples: 5                        # Number of samples to analyze

    branch_activation_analysis:
      enabled: false                      # Enable branch activation analysis (true/false)
      training: false
      params:
        synapse_activations: true         # Analyze synapse activations (true/false)
        branch_activations: true          # Analyze branch activations (true/false)
        network_summary: true             # Network-level summary (true/false)
        layer_summary: true               # Layer-level summary (true/false)
        branch_summary: true              # Branch-level summary (true/false)
        raw_summary: true                 # Raw activation summary (true/false)
        logspace: false                   # Use log space (true/false)
        downsample: true                  # Downsample data (true/false)
        samples: 5                        # Number of samples to analyze

    compartment_statistics_analysis:
      enabled: true                       # Enable compartment statistics analysis (true/false)
      training: false                     # Run during training (true/false)
      params:
        synapse_weights: true             # Analyze excitatory and inhibitory synapse weights (true/false)
        branch_weights: true              # Analyze branch-to-output connection weights (true/false)
        inputs: true                      # Analyze inputs (true/false)
        activations: true                 # Analyze activations (true/false)
        global_analysis: true             # Analyze global statistics (true/false)
        layer_analysis: true              # Analyze layer statistics (true/false)
        branch_analysis: true             # Analyze branch statistics (true/false)
        compute_mean: true                # Compute mean weights per branch (true/false)
        compute_variance: true            # Compute variance of weights per branch (true/false)
        compute_percentiles: true         # Compute percentiles (median, quartiles) (true/false)
        compute_min_max: true             # Compute min/max per layer (true/false)
        compute_entropy: true             # Compute entropy per layer (true/false)
        per_class_analysis: false         # Analyze statistics separately for each class (true/false)
        n_samples: 1000                   # Number of samples to analyze (null = all)

    compartment_snr_analysis:
      enabled: true
      training: true
      params:
        global_analysis: true
        layer_analysis: true
        branch_analysis: true

  # ============================================================================
  # LOGGING
  # ============================================================================
  wandb:
    use_wandb: false                      # Enable Weights & Biases logging (true/false)
    entity: ""                            # W&B entity name (string)
    project: "dendritic-sweeps"           # W&B project name (string)
    group: "unified_sweep"                # W&B group name (string)
    tags: ["debug"]                       # W&B tags (list of strings)
    enabled: true                         # Enable W&B features (true/false)
    log_freq: 100                         # Logging frequency in steps
    log_gradients: false                  # Log gradients (true/false)
    log_parameters: false                 # Log parameters (true/false)
    log_activations: false                # Log activations (true/false)

  # ============================================================================
  # OUTPUTS
  # ============================================================================
  outputs:
    run_name: "ei_sweep"                  # Experiment run name (string)
    results_dir: "outputs"                # Results directory path (string)

  # ============================================================================
  # DISTRIBUTED/FSDP
  # ============================================================================
  fsdp:
    use_fsdp: false                       # Enable Fully Sharded Data Parallel (true/false)
    sharding_strategy: "FULL_SHARD"      # Sharding strategy: "FULL_SHARD", "SHARD_GRAD_OP", "NO_SHARD", "HYBRID_SHARD"
    mixed_precision: true                 # Use mixed precision (true/false)
    cpu_offload: false                    # Offload to CPU (true/false)
    min_num_params: 1000000               # Minimum parameters for FSDP wrapping
    reduce_communication_overhead: true   # Reduce communication overhead (true/false)
    limit_all_gathers: true               # Limit all-gather operations (true/false)
    forward_prefetch: true                # Prefetch in forward pass (true/false)
    sync_module_states: true              # Synchronize module states (true/false)
    use_orig_params: true                 # Use original parameters (true/false)
    backward_prefetch: "BACKWARD_PRE"     # Backward prefetch: "BACKWARD_PRE", "BACKWARD_POST", null
    gradient_checkpointing: false         # Enable gradient checkpointing (true/false)


# ============================================================================
# EXAMPLES - Complete sweep configurations
# ============================================================================

# EI Filtering Only
# filter_config:
#   mode: "ei-sum"
#   value: 10
#   tolerance: 0
# Result: Only EI combinations where E+I=10

# Branch Sweep Only
# filter_config:
#   branch_sweep_mode: "pattern_sweep"
#   dendrite_tree_depth: [2, 4, 1]
#   base_branches: 3
#   branch_patterns: ["uniform", "decreasing"]
# Result: 3 depths × 2 patterns = 6 configurations

# EI Filtering + Branch Sweep (COMBINED!)
# filter_config:
#   mode: "ei-sum"                        # Filter EI: E+I=10
#   value: 10
#   tolerance: 0
#   branch_sweep_mode: "pattern_sweep"    # Plus branch patterns
#   dendrite_tree_depth: [2, 3, 1]
#   base_branches: 3
#   branch_patterns: ["uniform", "decreasing"]
# Result: 9 EI combos × 4 branch patterns = 36 configurations

# Full Mixed Sweep
# sweep_config:
  #   model.core.connectivity.ee_synapses_per_branch_per_layer: [[1],[2],[3],[4],[5]]
  #   model.core.connectivity.ie_synapses_per_branch_per_layer: [[1],[2],[3],[4],[5]]
  #   training.main.common.lr: [0.001, 0.01]
# filter_config:
#   mode: "ei-sum"
#   value: 6
#   tolerance: 0
#   branch_sweep_mode: "depth_sweep"
#   dendrite_tree_depth: [1, 3, 1]
#   uniform_branches_per_layer: 4
# Result: 5 EI combos × 2 LR × 3 depths = 30 configurations
# = 30 configurations
