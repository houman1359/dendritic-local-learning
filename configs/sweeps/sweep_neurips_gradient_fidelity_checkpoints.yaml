## Gradient fidelity over training: backprop training with frequent checkpoints
## After training completes, run gradient_fidelity_over_training.py on checkpoints
## to measure how local-vs-backprop alignment evolves during learning.
output_dir: "/n/holylfs06/LABS/kempner_project_b/Lab/dendritic/HS/LOCAL_LEARNING/sweep_runs"

slurm_config:
  account: "kempner_dev"
  partition: "kempner_eng"
  time: "02:00:00"
  nodes: 1
  ntasks_per_node: 1
  gpus_per_node: 1
  cpus_per_task: 4
  mem: 32GB
  max_concurrent_jobs: 6
  modules_to_load: []
  conda_env_path: ""
  training_script: "src/dendritic_modeling/scripts/training/train_experiments.py"

experiment_settings:
  seeds_per_condition: 3
  base_seed: 42

sweep_config:
  model.core.type: ["dendritic_shunting", "dendritic_additive"]

base_config:
  experiment:
    seed: 42
    train_valid_split: 0.1
    enable_amp: false
    enable_profiling: false
    enable_hooks: true
  data:
    dataset_name: "mnist"
    base_dir: ""
    processing:
      flatten: true
      normalize: false
  model:
    task: "classification"
    encoder:
      type: "identity"
      params:
        input_dim: null
    core:
      type: "dendritic_shunting"
      architecture:
        excitatory_layer_sizes: [20]
        inhibitory_layer_sizes: []
        excitatory_branch_factors: [3, 3]
        inhibitory_branch_factors: [1]
      connectivity:
        ee_synapses_per_branch_per_layer: [40]
        ei_synapses_per_branch_per_layer: [0]
        ie_synapses_per_branch_per_layer: [10]
        ii_synapses_per_branch_per_layer: [0]
      transfer:
        input_mode: 1
        independent_pathways: false
        output_activation: null
      morphology:
        somatic_synapses: false
        weight_transform: "softplus"
      reactivation:
        enabled: true
        type: "param_tanh"
        init_m: 1.5
        init_b: 0.5
    decoder:
      type: "MLP"
      params:
        hidden_dims: []
        activation: "relu"
        output_dim: 10
  training:
    main:
      strategy: "standard"
      common:
        epochs: 50
        batch_size: 256
        shuffle: true
        loss_function: "cat_nll"
        early_stopping: false
        load_best_state_dict: false
        plot_losses: false
        suppress_prints: false
        print_every: 5
        use_amp: false
        enable_nan_checking: false
        enable_adaptive_clipping: false
        weight_decay_rate: 0.0
        weight_boosting: false
        # Frequent checkpoints for gradient fidelity analysis
        checkpointing: true
        checkpoint_interval: 2
        param_groups:
          lr: 0.002
          split_params: true
          topk_lr: 0.002
          blocklinear_lr: 0.001
          reactivation_lr: 0.001
          decoder_lr: 0.002
  analysis:
    performance_analysis:
      enabled: true
      training: true
      params:
        accuracy: true
        auc: false
        categorical_loglikelihood: true
        mse: false
        cosine_similarity: false
    information_analysis:
      enabled: false
    correlation_analysis:
      enabled: false
  wandb:
    use_wandb: false
  outputs:
    run_name: "sweep_neurips_gradient_fidelity_checkpoints"
    results_dir: "outputs"
