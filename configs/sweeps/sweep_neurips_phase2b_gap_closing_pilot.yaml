output_dir: "/n/holylfs06/LABS/kempner_project_b/Lab/dendritic/HS/LOCAL_LEARNING/sweep_runs"

slurm_config:
  account: "kempner_dev"
  partition: "kempner_eng"
  time: "10:00:00"
  nodes: 1
  ntasks_per_node: 1
  gpus_per_node: 1
  cpus_per_task: 8
  mem: 64GB
  # Hard cap requested: do not exceed 16 GPUs concurrently on kempner_eng.
  max_concurrent_jobs: 16
  modules_to_load: []
  conda_env_path: ""
  training_script: "src/dendritic_modeling/scripts/training/train_experiments.py"

experiment_settings:
  # Pilot: increase robustness relative to earlier phase2 (2 seeds),
  # but keep job count manageable.
  seeds_per_condition: 5
  base_seed: 42

sweep_config:
  # Focus on the MNIST-derived tasks aligned with dendritic motifs.
  data.dataset_name: ["mnist", "context_gating"]

  # Broadcast and HSIC are the two highest-upside knobs from phase2.
  training.main.learning_strategy_config.error_broadcast_mode: ["scalar", "per_soma"]
  training.main.learning_strategy_config.hsic.enabled: [false, true]
  # NOTE: HSIC semantics were fixed so `hsic.weight` gates both self/target terms.
  training.main.learning_strategy_config.hsic.weight: [0.0, 0.01, 0.1, 1.0]

base_config:
  experiment:
    seed: 42
    train_valid_split: 0.8
    enable_amp: false
    enable_profiling: false
    enable_hooks: true
  data:
    dataset_name: "mnist"
    base_dir: ""
    processing:
      flatten: true
      normalize: false
  model:
    task: "classification"
    encoder:
      type: "identity"
      params:
        input_dim: null
    core:
      type: "dendritic_shunting"
      architecture:
        # Match a strong phase1 ceiling regime (kept fixed in this pilot).
        excitatory_layer_sizes: [128]
        inhibitory_layer_sizes: []
        excitatory_branch_factors: [3, 3]
        inhibitory_branch_factors: [1]
      connectivity:
        ee_synapses_per_branch_per_layer: [40]
        ei_synapses_per_branch_per_layer: [0]
        ie_synapses_per_branch_per_layer: [20]
        ii_synapses_per_branch_per_layer: [0]
      transfer:
        input_mode: 1
        independent_pathways: false
        output_activation: null
      morphology:
        somatic_synapses: false
        weight_transform: "softplus"
      reactivation:
        enabled: true
        type: "param_tanh"
        init_m: 1.5
        init_b: 0.5
    decoder:
      type: "MLP"
      params:
        hidden_dims: []
        activation: "relu"
        output_dim: 10
  training:
    main:
      strategy: "local_ca"
      common:
        # Give local learning enough time; early stopping can mask slow-but-steady learning.
        epochs: 200
        batch_size: 256
        shuffle: true
        loss_function: "cat_nll"
        early_stopping: false
        patience: 20
        load_best_state_dict: true
        plot_losses: false
        suppress_prints: false
        print_every: 1
        use_amp: false
        enable_nan_checking: false
        enable_adaptive_clipping: false
        weight_decay_rate: 0.0
        weight_boosting: false
        param_groups:
          lr: 0.0015
          split_params: true
          topk_lr: 0.0015
          blocklinear_lr: 0.0007
          reactivation_lr: 0.0005
          decoder_lr: 0.0015
      learning_strategy_config:
        rule_variant: "5f"
        error_mode: "auto"
        error_broadcast_mode: "per_soma"
        decoder_update_mode: "local"
        update_reactivation: true
        update_inactive_weights: false
        normalize_by_batch: true
        clip_grad_value: 5.0
        three_factor:
          # Fair additive/shunting comparison:
          # auto selects additive-consistent derivatives for additive cores.
          dynamics_mode: "auto"
          use_conductance_scaling: true
          use_driving_force: true
          theta: 0.0
          e_rev_exc: 1.0
          e_rev_inh: 0.0
        four_factor:
          rho_mode: "dot"
          rho_estimator: "ema"
          ema_alpha: 0.1
          layer_wise_rho_scale: 0.0
          augment_k: 8
          augment_noise_sigma: 0.01
        five_factor:
          phi_mode: "conditional"
          phi_estimator: "conditional_ema"
          phi_ridge_lambda: 0.001
          layer_wise_phi_scale: 0.0
          rls_forgetting: 0.99
        morphology_aware:
          use_path_propagation: false
          morphology_modulator_mode: "none"
          morphology_depth_offset: 1.0
          morphology_centrality_metric: "betweenness"
          use_dendritic_normalization: false
          use_branch_type_rules: false
          apical_branch_scale: 1.0
          basal_branch_scale: 1.0
          use_branch_length_modulation: false
        hsic:
          enabled: false
          weight: 0.0
          self_weight: 0.3
          target_weight: 0.3
          target_source: "labels"
          kernel: "rbf"
          sigma: 1.0
          degree: 2
          coef0: 1.0
          grad_clip_value: 0.1
          warmup_epochs: 5
          apply_last_layer_only: false
  analysis:
    performance_analysis:
      enabled: true
      training: true
      params:
        accuracy: true
        auc: false
        categorical_loglikelihood: true
        mse: false
        cosine_similarity: false
    # Keep analysis off for pilots to reduce runtime/IO.
    information_analysis:
      enabled: false
      training: false
    correlation_analysis:
      enabled: false
      training: false
    ablation_analysis:
      enabled: false
      training: false
    synapse_turnover_analysis:
      enabled: false
      training: false
    noise_perturbation_analysis:
      enabled: false
      training: false
    weight_analysis:
      enabled: false
      training: false
    single_layer_contribution:
      enabled: false
      training: false
    synaptic_activation_analysis:
      enabled: false
      training: false
    branch_activation_analysis:
      enabled: false
      training: false
    compartment_statistics_analysis:
      enabled: false
      training: false
    compartment_snr_analysis:
      enabled: false
      training: false
  wandb:
    use_wandb: false
  outputs:
    run_name: "sweep_neurips_phase2b_gap_closing_pilot"
    results_dir: "outputs"
